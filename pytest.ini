[tool:pytest]
# Unified pytest configuration for GrandModel aligned with src/grandmodel structure
# Supports local development, CI, and production environments

# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Minimum version
minversion = 7.0

# Base configuration
addopts = 
    -v
    --strict-markers
    --strict-config
    --tb=short
    --asyncio-mode=auto
    --disable-warnings
    --ignore=venv
    --ignore=.venv
    --ignore=build
    --ignore=dist
    --ignore=.git
    --ignore=__pycache__
    --ignore=.pytest_cache
    --ignore=reorganize_tests.py
    --ignore=fix_test_imports.py
    
    # Coverage for grandmodel package
    --cov=src.grandmodel
    --cov-report=html:htmlcov
    --cov-report=term-missing
    --cov-report=xml:coverage.xml
    --cov-branch
    --cov-fail-under=65

# Comprehensive test markers aligned with new structure
markers =
    # Core system markers
    unit: Unit tests (fast, isolated)
    integration: Integration tests (system components)
    performance: Performance and benchmark tests
    slow: Slow running tests
    
    # Component markers aligned with src/grandmodel structure
    agents: Agent system tests
    algorithms: Algorithm tests
    analytics: Analytics system tests
    api: API tests
    backtesting: Backtesting framework tests
    certification: Certification tests
    compliance: Compliance tests
    components: Component integration tests
    consensus: Consensus mechanism tests
    core: Core system tests
    dashboard: Dashboard tests
    data: Data handling tests
    data_pipeline: Data pipeline tests
    database: Database tests
    deployment: Deployment tests
    disaster_recovery: Disaster recovery tests
    execution: Execution system tests
    failover: Failover tests
    governance: Governance tests
    human_interface: Human interface tests
    indicators: Technical indicator tests
    infrastructure: Infrastructure tests
    intelligence: Intelligence system tests
    llm: LLM integration tests
    logging: Logging system tests
    matrix: Matrix assembler tests
    model_risk: Model risk tests
    models: Model tests
    monitoring: Monitoring system tests
    operations: Operations tests
    pnl: PnL calculation tests
    risk: Risk management system tests
    safety: Safety mechanism tests
    security: Security and adversarial tests
    synergy: Synergy pattern tests
    tactical: Tactical MARL system tests
    testing: Testing framework tests
    trading: Trading system tests
    utils: Utility tests
    validation: Validation tests
    xai: XAI explainability tests
    
    # Legacy markers for backward compatibility
    strategic: Strategic MARL system tests
    marl: MARL system tests
    
    # Special markers
    smoke: Smoke tests
    regression: Regression tests
    acceptance: Acceptance tests
    local: Local development tests
    ci: CI environment tests
    production: Production environment tests
    
    # Resource markers
    memory_intensive: Memory intensive tests
    cpu_intensive: CPU intensive tests
    gpu_intensive: GPU intensive tests
    network_intensive: Network intensive tests
    
    # Dependency markers
    requires_docker: Docker required tests
    requires_gpu: GPU required tests
    requires_data: External data required tests
    requires_redis: Redis required tests
    requires_network: Network required tests
    requires_vault: Vault required tests
    requires_prometheus: Prometheus required tests
    
# Async configuration
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Logging configuration
log_cli = false
log_cli_level = WARNING
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Performance benchmarking
benchmark_min_time = 0.001
benchmark_max_time = 2.0
benchmark_min_rounds = 3
benchmark_warmup = true
benchmark_warmup_iterations = 1
benchmark_sort = mean
benchmark_group_by = group

# Test reliability
xfail_strict = true
junit_family = xunit2
junit_suite_name = GrandModel
junit_duration_report = total
junit_log_passing_tests = false

# Collection configuration
collect_ignore = 
    setup.py
    venv
    .venv
    build
    dist
    .git
    __pycache__
    .pytest_cache
    colab/notebooks/archive_backup
    adversarial_tests/extreme_data
    adversarial_tests/malicious_configs
    models/artifacts
    logs
    htmlcov
    reorganize_tests.py
    fix_test_imports.py
    
# Timeout configuration
timeout = 300
timeout_method = thread

# Warnings filter
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:torch.*
    ignore::UserWarning:tensorflow.*
    ignore::FutureWarning:pandas.*
    ignore::FutureWarning:numpy.*
    ignore::ResourceWarning
    ignore::RuntimeWarning:numpy.*
    ignore::UserWarning:transformers.*
    ignore::UserWarning:numba.*
    
# Test execution order
test_order = 
    unit
    integration
    performance
    slow