[tool:pytest]
# Performance Testing pytest configuration for GrandModel
# Optimized for performance benchmarking and resource monitoring

# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Minimum version
minversion = 7.0

# Performance testing optimizations
addopts = 
    -v
    --strict-markers
    --strict-config
    --tb=short
    --timeout=600
    --timeout-method=thread
    --durations=50
    --durations-min=0.001
    --maxfail=1
    --ignore=venv
    --ignore=.venv
    --ignore=build
    --ignore=dist
    --ignore=.git
    --ignore=__pycache__
    --ignore=.pytest_cache
    --benchmark-only
    --benchmark-sort=mean
    --benchmark-group-by=group
    --benchmark-warmup=on
    --benchmark-warmup-iterations=3
    --benchmark-min-rounds=5
    --benchmark-max-time=10.0
    --benchmark-min-time=0.001
    --benchmark-save=performance_results
    --benchmark-save-data
    --benchmark-compare
    --benchmark-histogram
    
    --no-cov
    
# Sequential execution for consistent performance measurements
    -n 0
    --dist=no

# Performance-specific markers
markers =
    performance: Performance benchmark tests
    latency: Latency measurement tests
    throughput: Throughput measurement tests
    memory: Memory usage tests
    cpu: CPU usage tests
    stress: Stress testing
    load: Load testing
    endurance: Endurance testing
    scalability: Scalability testing
    ultra_low_latency: Ultra-low latency tests
    
# Performance configuration
asyncio_mode = auto

# Minimal logging for performance tests
log_cli = false
log_cli_level = ERROR

# Memory and resource monitoring
memory_profiler = true
memory_profiler_backend = tracemalloc

# Strict warnings for performance tests
filterwarnings =
    error::PerformanceWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:torch.*
    ignore::UserWarning:tensorflow.*

# Performance test reliability
xfail_strict = true

# Collection configuration (performance-focused)
collect_ignore = 
    setup.py
    venv
    .venv
    build
    dist
    .git
    __pycache__
    .pytest_cache
    colab/notebooks/archive_backup
    adversarial_tests/extreme_data
    adversarial_tests/malicious_configs
    models/artifacts
    logs
    htmlcov
    tests/unit
    tests/integration
    tests/chaos_engineering
    tests/formal_verification

# Performance test settings
session_timeout = 7200  # 2 hours for performance tests
test_timeout = 600      # 10 minutes per performance test
cache_dir = .pytest_cache

# Benchmark configuration
benchmark_min_time = 0.001
benchmark_max_time = 10.0
benchmark_min_rounds = 5
benchmark_warmup = true
benchmark_warmup_iterations = 3
benchmark_sort = mean
benchmark_group_by = group
benchmark_save = performance_results
benchmark_save_data = true
benchmark_compare = true
benchmark_histogram = true

# Test reporting for performance
junit_family = xunit2
junit_suite_name = GrandModel-Performance
junit_duration_report = total
junit_log_passing_tests = false

# Resource thresholds
memory_limit_mb = 2048
cpu_limit_percent = 95
execution_time_limit_seconds = 600

# Performance test execution order
test_order = 
    latency
    throughput
    memory
    cpu
    stress
    load
    endurance
    scalability