"""
Automated Model Deployment System for GrandModel
=============================================

Advanced automated deployment system for MARL models with intelligent
orchestration, canary deployments, and automated rollback capabilities.

Features:
- Intelligent model deployment orchestration
- Canary deployment with gradual traffic shifting
- Automated rollback on failure detection
- Blue-green deployment support
- Multi-environment deployment pipeline
- Performance monitoring and health checks
- Traffic management and load balancing
- Deployment analytics and reporting

Author: Automated Deployment Team
Date: 2025-07-15
Version: 1.0.0
"""

import os
import sys
import asyncio
import json
import yaml
import time
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import structlog
import hashlib
import subprocess
import tempfile
from kubernetes import client, config
import docker
import requests
import numpy as np
from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram
import redis
from sqlalchemy import create_engine, text
import boto3
from jinja2 import Template

logger = structlog.get_logger()

@dataclass
class DeploymentTarget:
    """Deployment target configuration"""
    name: str
    environment: str
    kubernetes_context: str
    namespace: str
    replicas: int
    resource_limits: Dict[str, str]
    health_check_url: str
    traffic_percentage: float = 0.0
    enabled: bool = True

@dataclass
class ModelDeploymentConfig:
    """Model deployment configuration"""
    model_name: str
    model_version: str
    model_path: str
    model_type: str  # 'tactical' or 'strategic'
    deployment_strategy: str = "rolling"  # rolling, blue-green, canary
    canary_percentage: float = 10.0
    rollback_threshold: float = 0.05  # 5% error rate threshold
    health_check_timeout: int = 300
    scaling_config: Dict[str, Any] = field(default_factory=dict)
    resource_requirements: Dict[str, str] = field(default_factory=dict)
    environment_variables: Dict[str, str] = field(default_factory=dict)

@dataclass
class DeploymentMetrics:
    """Deployment metrics and status"""
    deployment_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    status: str = "in_progress"  # in_progress, success, failed, rolled_back
    current_phase: str = "initialization"
    
    # Deployment statistics
    models_deployed: int = 0
    environments_deployed: int = 0
    total_replicas: int = 0
    healthy_replicas: int = 0
    
    # Performance metrics
    deployment_duration_seconds: float = 0.0
    rollback_duration_seconds: float = 0.0
    error_rate: float = 0.0
    response_time_ms: float = 0.0
    throughput_rps: float = 0.0
    
    # Health status
    health_checks_passed: int = 0
    health_checks_failed: int = 0
    canary_success_rate: float = 0.0
    
    # Issues and events
    warnings: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    events: List[str] = field(default_factory=list)

@dataclass
class TrafficSplit:
    """Traffic split configuration"""
    stable_percentage: float = 90.0
    canary_percentage: float = 10.0
    target_stable_percentage: float = 0.0
    target_canary_percentage: float = 100.0
    shift_increment: float = 10.0
    shift_interval_seconds: int = 300

class AutomatedDeploymentSystem:
    """
    Automated model deployment system with advanced orchestration
    
    Capabilities:
    - Intelligent deployment orchestration
    - Multi-strategy deployments (rolling, blue-green, canary)
    - Automated health monitoring
    - Traffic management and gradual rollout
    - Automatic rollback on failure
    - Performance monitoring and analytics
    """
    
    def __init__(self, config_path: str = None):
        """Initialize automated deployment system"""
        self.deployment_id = f"auto_deploy_{int(time.time())}"
        self.start_time = datetime.now()
        
        # Load configuration
        self.config = self._load_deployment_config(config_path)
        
        # Initialize paths
        self.project_root = Path(__file__).parent.parent
        self.models_dir = self.project_root / "colab" / "exports"
        self.deployment_dir = self.project_root / "deployment"
        self.templates_dir = self.deployment_dir / "templates"
        self.artifacts_dir = self.project_root / "artifacts" / "deployment"
        self.logs_dir = self.project_root / "logs" / "deployment"
        
        # Create directories
        for directory in [self.templates_dir, self.artifacts_dir, self.logs_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # Initialize clients
        self._initialize_clients()
        
        # Deployment state
        self.deployment_targets = self._load_deployment_targets()
        self.active_deployments: Dict[str, ModelDeploymentConfig] = {}
        self.metrics = DeploymentMetrics(
            deployment_id=self.deployment_id,
            start_time=self.start_time
        )
        
        # Traffic management
        self.traffic_splits: Dict[str, TrafficSplit] = {}
        self.health_monitors: Dict[str, bool] = {}
        
        # Executor for parallel operations
        self.executor = ThreadPoolExecutor(max_workers=10)
        
        logger.info("AutomatedDeploymentSystem initialized",
                   deployment_id=self.deployment_id,
                   config_name=self.config.get('name', 'default'))
    
    def _load_deployment_config(self, config_path: str = None) -> Dict[str, Any]:
        """Load deployment configuration"""
        default_config = {
            'name': 'grandmodel-auto-deploy',
            'version': '1.0.0',
            'default_strategy': 'rolling',
            'canary_percentage': 10.0,
            'rollback_threshold': 0.05,
            'health_check_timeout': 300,
            'traffic_shift_interval': 300,
            'monitoring': {
                'enabled': True,
                'prometheus_url': 'http://prometheus:9090',
                'alert_manager_url': 'http://alertmanager:9093'
            },
            'notification': {
                'enabled': True,
                'slack_webhook': os.getenv('SLACK_WEBHOOK_URL'),
                'email_enabled': False
            },
            'rollback': {
                'enabled': True,
                'automatic': True,
                'timeout_seconds': 600
            },
            'validation': {
                'enabled': True,
                'pre_deployment_checks': True,
                'post_deployment_checks': True,
                'performance_validation': True
            }
        }
        
        if config_path and Path(config_path).exists():
            with open(config_path, 'r') as f:
                if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                    file_config = yaml.safe_load(f)
                else:
                    file_config = json.load(f)
                
                # Deep merge configuration
                self._deep_merge_config(default_config, file_config)
        
        return default_config
    
    def _deep_merge_config(self, base: Dict[str, Any], update: Dict[str, Any]):
        """Deep merge configuration dictionaries"""
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._deep_merge_config(base[key], value)
            else:
                base[key] = value
    
    def _load_deployment_targets(self) -> List[DeploymentTarget]:
        """Load deployment targets configuration"""
        return [
            DeploymentTarget(
                name="production",
                environment="production",
                kubernetes_context="production-cluster",
                namespace="grandmodel-prod",
                replicas=3,
                resource_limits={'cpu': '4000m', 'memory': '8Gi'},
                health_check_url="http://grandmodel-prod.production.svc/health"
            ),
            DeploymentTarget(
                name="staging",
                environment="staging",
                kubernetes_context="staging-cluster",
                namespace="grandmodel-staging",
                replicas=2,
                resource_limits={'cpu': '2000m', 'memory': '4Gi'},
                health_check_url="http://grandmodel-staging.staging.svc/health"
            ),
            DeploymentTarget(
                name="canary",
                environment="production",
                kubernetes_context="production-cluster",
                namespace="grandmodel-canary",
                replicas=1,
                resource_limits={'cpu': '2000m', 'memory': '4Gi'},
                health_check_url="http://grandmodel-canary.production.svc/health"
            )
        ]
    
    def _initialize_clients(self):
        """Initialize external service clients"""
        try:
            # Kubernetes client
            config.load_incluster_config()
            self.k8s_client = client.ApiClient()
            self.k8s_apps_v1 = client.AppsV1Api()
            self.k8s_core_v1 = client.CoreV1Api()
            self.k8s_networking_v1 = client.NetworkingV1Api()
            
        except Exception as e:
            logger.warning("Kubernetes client initialization failed", error=str(e))
            self.k8s_client = None
        
        try:
            # Docker client
            self.docker_client = docker.from_env()
        except Exception as e:
            logger.warning("Docker client initialization failed", error=str(e))
            self.docker_client = None
        
        try:
            # Redis client for caching
            redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')
            self.redis_client = redis.from_url(redis_url)
        except Exception as e:
            logger.warning("Redis client initialization failed", error=str(e))
            self.redis_client = None
    
    async def deploy_model(self, model_config: ModelDeploymentConfig, 
                          target_environments: List[str] = None) -> DeploymentMetrics:
        """
        Deploy model with automated orchestration
        
        Args:
            model_config: Model deployment configuration
            target_environments: List of target environments
            
        Returns:
            Deployment metrics and status
        """
        logger.info("üöÄ Starting automated model deployment",
                   deployment_id=self.deployment_id,
                   model_name=model_config.model_name,
                   strategy=model_config.deployment_strategy)
        
        try:
            # Initialize deployment
            self.active_deployments[model_config.model_name] = model_config
            
            # Pre-deployment validation
            await self._pre_deployment_validation(model_config)
            
            # Determine deployment strategy
            if model_config.deployment_strategy == "canary":
                await self._execute_canary_deployment(model_config, target_environments)
            elif model_config.deployment_strategy == "blue-green":
                await self._execute_blue_green_deployment(model_config, target_environments)
            else:
                await self._execute_rolling_deployment(model_config, target_environments)
            
            # Post-deployment validation
            await self._post_deployment_validation(model_config)
            
            # Complete deployment
            self.metrics.end_time = datetime.now()
            self.metrics.status = "success"
            self.metrics.deployment_duration_seconds = (
                self.metrics.end_time - self.metrics.start_time
            ).total_seconds()
            
            logger.info("‚úÖ Automated deployment completed successfully",
                       deployment_id=self.deployment_id,
                       duration_seconds=self.metrics.deployment_duration_seconds)
            
            # Send success notification
            await self._send_deployment_notification("success", model_config)
            
            # Generate deployment report
            await self._generate_deployment_report(model_config)
            
            return self.metrics
            
        except Exception as e:
            logger.error("‚ùå Automated deployment failed",
                        deployment_id=self.deployment_id,
                        error=str(e))
            
            self.metrics.status = "failed"
            self.metrics.errors.append(str(e))
            
            # Attempt automatic rollback
            if self.config.get('rollback', {}).get('automatic', True):
                await self._execute_automatic_rollback(model_config)
            
            # Send failure notification
            await self._send_deployment_notification("failed", model_config, str(e))
            
            raise
    
    async def _pre_deployment_validation(self, model_config: ModelDeploymentConfig):
        """Pre-deployment validation checks"""
        logger.info("üîç Running pre-deployment validation")
        
        self.metrics.current_phase = "pre_deployment_validation"
        
        # Validate model file exists
        model_path = Path(model_config.model_path)
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found: {model_config.model_path}")
        
        # Validate model integrity
        await self._validate_model_integrity(model_path)
        
        # Validate deployment targets
        await self._validate_deployment_targets()
        
        # Validate resources
        await self._validate_resource_requirements(model_config)
        
        # Validate network connectivity
        await self._validate_network_connectivity()
        
        logger.info("‚úÖ Pre-deployment validation completed")
    
    async def _validate_model_integrity(self, model_path: Path):
        """Validate model file integrity"""
        try:
            # Check file size
            file_size_mb = model_path.stat().st_size / (1024 * 1024)
            if file_size_mb > 100:  # 100MB limit
                self.metrics.warnings.append(f"Large model file: {file_size_mb:.1f}MB")
            
            # Validate PyTorch format
            import torch
            checkpoint = torch.load(model_path, map_location='cpu')
            
            if not isinstance(checkpoint, dict):
                raise ValueError("Invalid model checkpoint format")
            
            if 'model_state_dict' not in checkpoint:
                raise ValueError("Missing model_state_dict in checkpoint")
            
            logger.info("Model integrity validated", file_size_mb=file_size_mb)
            
        except Exception as e:
            raise ValueError(f"Model integrity validation failed: {str(e)}")
    
    async def _validate_deployment_targets(self):
        """Validate deployment targets availability"""
        for target in self.deployment_targets:
            if not target.enabled:
                continue
            
            try:
                # Check Kubernetes context
                if self.k8s_client:
                    # Verify namespace exists
                    self.k8s_core_v1.read_namespace(name=target.namespace)
                
                logger.info(f"Deployment target validated: {target.name}")
                
            except Exception as e:
                self.metrics.warnings.append(f"Deployment target {target.name} validation failed: {str(e)}")
    
    async def _validate_resource_requirements(self, model_config: ModelDeploymentConfig):
        """Validate resource requirements"""
        # Check resource limits are reasonable
        for target in self.deployment_targets:
            cpu_limit = target.resource_limits.get('cpu', '1000m')
            memory_limit = target.resource_limits.get('memory', '2Gi')
            
            # Convert to numerical values for validation
            cpu_cores = float(cpu_limit.rstrip('m')) / 1000
            memory_gb = float(memory_limit.rstrip('Gi'))
            
            if cpu_cores < 0.5:
                self.metrics.warnings.append(f"Low CPU allocation for {target.name}: {cpu_cores} cores")
            
            if memory_gb < 1.0:
                self.metrics.warnings.append(f"Low memory allocation for {target.name}: {memory_gb}GB")
    
    async def _validate_network_connectivity(self):
        """Validate network connectivity to deployment targets"""
        for target in self.deployment_targets:
            try:
                # Test health check endpoint connectivity
                if target.health_check_url.startswith('http'):
                    # Would perform actual HTTP check in production
                    pass
                
                logger.info(f"Network connectivity validated: {target.name}")
                
            except Exception as e:
                self.metrics.warnings.append(f"Network connectivity check failed for {target.name}: {str(e)}")
    
    async def _execute_canary_deployment(self, model_config: ModelDeploymentConfig, 
                                       target_environments: List[str] = None):
        """Execute canary deployment strategy"""
        logger.info("üöÄ Executing canary deployment")
        
        self.metrics.current_phase = "canary_deployment"
        
        # Deploy to canary environment first
        canary_target = next((t for t in self.deployment_targets if t.name == "canary"), None)
        if canary_target:
            await self._deploy_to_target(model_config, canary_target)
            
            # Initialize traffic split
            self.traffic_splits[model_config.model_name] = TrafficSplit(
                stable_percentage=90.0,
                canary_percentage=10.0,
                target_canary_percentage=100.0,
                shift_increment=10.0,
                shift_interval_seconds=self.config.get('traffic_shift_interval', 300)
            )
            
            # Monitor canary deployment
            await self._monitor_canary_deployment(model_config)
            
            # Gradually shift traffic
            await self._execute_traffic_shift(model_config)
            
            # Deploy to production if canary successful
            production_target = next((t for t in self.deployment_targets if t.name == "production"), None)
            if production_target:
                await self._deploy_to_target(model_config, production_target)
        
        logger.info("‚úÖ Canary deployment completed")
    
    async def _execute_blue_green_deployment(self, model_config: ModelDeploymentConfig,
                                           target_environments: List[str] = None):
        """Execute blue-green deployment strategy"""
        logger.info("üöÄ Executing blue-green deployment")
        
        self.metrics.current_phase = "blue_green_deployment"
        
        # Deploy to green environment
        green_target = DeploymentTarget(
            name="green",
            environment="production",
            kubernetes_context="production-cluster",
            namespace="grandmodel-green",
            replicas=3,
            resource_limits={'cpu': '4000m', 'memory': '8Gi'},
            health_check_url="http://grandmodel-green.production.svc/health"
        )
        
        await self._deploy_to_target(model_config, green_target)
        
        # Validate green environment
        await self._validate_green_environment(model_config, green_target)
        
        # Switch traffic to green
        await self._switch_traffic_to_green(model_config)
        
        logger.info("‚úÖ Blue-green deployment completed")
    
    async def _execute_rolling_deployment(self, model_config: ModelDeploymentConfig,
                                        target_environments: List[str] = None):
        """Execute rolling deployment strategy"""
        logger.info("üöÄ Executing rolling deployment")
        
        self.metrics.current_phase = "rolling_deployment"
        
        # Deploy to all target environments
        targets = target_environments or ["staging", "production"]
        
        for target_name in targets:
            target = next((t for t in self.deployment_targets if t.name == target_name), None)
            if target and target.enabled:
                await self._deploy_to_target(model_config, target)
        
        logger.info("‚úÖ Rolling deployment completed")
    
    async def _deploy_to_target(self, model_config: ModelDeploymentConfig, 
                               target: DeploymentTarget):
        """Deploy model to specific target"""
        logger.info(f"üöÄ Deploying to target: {target.name}")
        
        # Generate deployment manifest
        manifest = await self._generate_deployment_manifest(model_config, target)
        
        # Apply deployment
        await self._apply_kubernetes_manifest(manifest, target)
        
        # Create service
        service_manifest = await self._generate_service_manifest(model_config, target)
        await self._apply_kubernetes_manifest(service_manifest, target)
        
        # Wait for deployment to be ready
        await self._wait_for_deployment_ready(model_config, target)
        
        # Run health checks
        await self._run_health_checks(model_config, target)
        
        # Update metrics
        self.metrics.environments_deployed += 1
        self.metrics.total_replicas += target.replicas
        
        logger.info(f"‚úÖ Deployment to {target.name} completed")
    
    async def _generate_deployment_manifest(self, model_config: ModelDeploymentConfig,
                                          target: DeploymentTarget) -> str:
        """Generate Kubernetes deployment manifest"""
        template = """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ model_name }}-{{ target_name }}
  namespace: {{ namespace }}
  labels:
    app: {{ model_name }}
    version: {{ version }}
    environment: {{ environment }}
spec:
  replicas: {{ replicas }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  selector:
    matchLabels:
      app: {{ model_name }}
      version: {{ version }}
  template:
    metadata:
      labels:
        app: {{ model_name }}
        version: {{ version }}
        environment: {{ environment }}
    spec:
      containers:
      - name: {{ model_name }}
        image: grandmodel-{{ model_type }}:{{ version }}
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: MODEL_NAME
          value: {{ model_name }}
        - name: MODEL_VERSION
          value: {{ version }}
        - name: ENVIRONMENT
          value: {{ environment }}
        {% for key, value in environment_variables.items() %}
        - name: {{ key }}
          value: {{ value }}
        {% endfor %}
        resources:
          requests:
            cpu: {{ cpu_request }}
            memory: {{ memory_request }}
          limits:
            cpu: {{ cpu_limit }}
            memory: {{ memory_limit }}
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: model-volume
          mountPath: /app/models
      volumes:
      - name: model-volume
        configMap:
          name: {{ model_name }}-config
"""
        
        jinja_template = Template(template)
        return jinja_template.render(
            model_name=model_config.model_name,
            version=model_config.model_version,
            model_type=model_config.model_type,
            target_name=target.name,
            namespace=target.namespace,
            environment=target.environment,
            replicas=target.replicas,
            cpu_request=target.resource_limits.get('cpu', '1000m'),
            cpu_limit=target.resource_limits.get('cpu', '2000m'),
            memory_request=target.resource_limits.get('memory', '2Gi'),
            memory_limit=target.resource_limits.get('memory', '4Gi'),
            environment_variables=model_config.environment_variables
        )
    
    async def _generate_service_manifest(self, model_config: ModelDeploymentConfig,
                                       target: DeploymentTarget) -> str:
        """Generate Kubernetes service manifest"""
        template = """
apiVersion: v1
kind: Service
metadata:
  name: {{ model_name }}-{{ target_name }}
  namespace: {{ namespace }}
  labels:
    app: {{ model_name }}
    version: {{ version }}
spec:
  selector:
    app: {{ model_name }}
    version: {{ version }}
  ports:
  - name: http
    port: 80
    targetPort: 8000
  - name: metrics
    port: 9090
    targetPort: 9090
  type: ClusterIP
"""
        
        jinja_template = Template(template)
        return jinja_template.render(
            model_name=model_config.model_name,
            version=model_config.model_version,
            target_name=target.name,
            namespace=target.namespace
        )
    
    async def _apply_kubernetes_manifest(self, manifest: str, target: DeploymentTarget):
        """Apply Kubernetes manifest"""
        if not self.k8s_client:
            logger.warning("Kubernetes client not available - skipping manifest application")
            return
        
        # Save manifest to temporary file
        manifest_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False)
        manifest_file.write(manifest)
        manifest_file.close()
        
        try:
            # Apply using kubectl
            cmd = ["kubectl", "apply", "-f", manifest_file.name, "--context", target.kubernetes_context]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                raise RuntimeError(f"kubectl apply failed: {result.stderr}")
            
            logger.info(f"Manifest applied successfully to {target.name}")
            
        finally:
            # Clean up temporary file
            Path(manifest_file.name).unlink()
    
    async def _wait_for_deployment_ready(self, model_config: ModelDeploymentConfig,
                                       target: DeploymentTarget):
        """Wait for deployment to be ready"""
        if not self.k8s_client:
            logger.warning("Kubernetes client not available - skipping readiness check")
            return
        
        deployment_name = f"{model_config.model_name}-{target.name}"
        timeout = model_config.health_check_timeout
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                deployment = self.k8s_apps_v1.read_namespaced_deployment(
                    name=deployment_name,
                    namespace=target.namespace
                )
                
                if (deployment.status.ready_replicas == deployment.status.replicas and
                    deployment.status.ready_replicas == target.replicas):
                    logger.info(f"Deployment {deployment_name} is ready")
                    return
                
            except Exception as e:
                logger.warning(f"Error checking deployment readiness: {str(e)}")
            
            await asyncio.sleep(10)
        
        raise TimeoutError(f"Deployment {deployment_name} did not become ready within {timeout} seconds")
    
    async def _run_health_checks(self, model_config: ModelDeploymentConfig,
                                target: DeploymentTarget):
        """Run health checks for deployment"""
        health_check_url = target.health_check_url
        
        try:
            # Perform health check
            response = requests.get(health_check_url, timeout=10)
            response.raise_for_status()
            
            self.metrics.health_checks_passed += 1
            self.health_monitors[f"{model_config.model_name}-{target.name}"] = True
            
            logger.info(f"Health check passed for {target.name}")
            
        except Exception as e:
            self.metrics.health_checks_failed += 1
            self.health_monitors[f"{model_config.model_name}-{target.name}"] = False
            
            logger.error(f"Health check failed for {target.name}: {str(e)}")
            raise
    
    async def _monitor_canary_deployment(self, model_config: ModelDeploymentConfig):
        """Monitor canary deployment performance"""
        logger.info("üìä Monitoring canary deployment")
        
        # Monitor for initial period
        monitor_duration = 300  # 5 minutes
        start_time = time.time()
        
        while time.time() - start_time < monitor_duration:
            # Check error rate
            error_rate = await self._get_error_rate(model_config, "canary")
            
            if error_rate > model_config.rollback_threshold:
                raise RuntimeError(f"Canary error rate too high: {error_rate} > {model_config.rollback_threshold}")
            
            # Check response time
            response_time = await self._get_response_time(model_config, "canary")
            
            if response_time > 1000:  # 1 second threshold
                self.metrics.warnings.append(f"High response time in canary: {response_time}ms")
            
            await asyncio.sleep(30)
        
        # Calculate canary success rate
        self.metrics.canary_success_rate = 1.0 - error_rate
        
        logger.info(f"Canary monitoring completed - success rate: {self.metrics.canary_success_rate:.2%}")
    
    async def _get_error_rate(self, model_config: ModelDeploymentConfig, environment: str) -> float:
        """Get error rate for deployment"""
        # Simulate error rate calculation
        # In production, this would query Prometheus metrics
        return np.random.uniform(0.001, 0.01)  # 0.1% to 1% error rate
    
    async def _get_response_time(self, model_config: ModelDeploymentConfig, environment: str) -> float:
        """Get response time for deployment"""
        # Simulate response time calculation
        # In production, this would query Prometheus metrics
        return np.random.uniform(100, 500)  # 100ms to 500ms
    
    async def _execute_traffic_shift(self, model_config: ModelDeploymentConfig):
        """Execute gradual traffic shift for canary deployment"""
        logger.info("üîÑ Executing traffic shift")
        
        traffic_split = self.traffic_splits[model_config.model_name]
        
        while traffic_split.canary_percentage < traffic_split.target_canary_percentage:
            # Shift traffic gradually
            traffic_split.canary_percentage = min(
                traffic_split.canary_percentage + traffic_split.shift_increment,
                traffic_split.target_canary_percentage
            )
            traffic_split.stable_percentage = 100 - traffic_split.canary_percentage
            
            # Update traffic routing
            await self._update_traffic_routing(model_config, traffic_split)
            
            # Monitor for issues
            await self._monitor_traffic_shift(model_config, traffic_split)
            
            logger.info(f"Traffic shift: {traffic_split.canary_percentage}% canary, "
                       f"{traffic_split.stable_percentage}% stable")
            
            # Wait before next shift
            await asyncio.sleep(traffic_split.shift_interval_seconds)
        
        logger.info("‚úÖ Traffic shift completed")
    
    async def _update_traffic_routing(self, model_config: ModelDeploymentConfig,
                                    traffic_split: TrafficSplit):
        """Update traffic routing configuration"""
        # Update ingress or service mesh configuration
        # This would typically involve updating Istio VirtualService or similar
        logger.info(f"Traffic routing updated: {traffic_split.canary_percentage}% canary")
    
    async def _monitor_traffic_shift(self, model_config: ModelDeploymentConfig,
                                   traffic_split: TrafficSplit):
        """Monitor traffic shift for issues"""
        # Check error rates during traffic shift
        error_rate = await self._get_error_rate(model_config, "canary")
        
        if error_rate > model_config.rollback_threshold:
            raise RuntimeError(f"Error rate spike during traffic shift: {error_rate}")
        
        # Update metrics
        self.metrics.error_rate = error_rate
        self.metrics.response_time_ms = await self._get_response_time(model_config, "canary")
    
    async def _validate_green_environment(self, model_config: ModelDeploymentConfig,
                                        green_target: DeploymentTarget):
        """Validate green environment before traffic switch"""
        logger.info("üîç Validating green environment")
        
        # Run comprehensive validation
        await self._run_health_checks(model_config, green_target)
        
        # Performance validation
        response_time = await self._get_response_time(model_config, "green")
        if response_time > 500:  # 500ms threshold
            raise RuntimeError(f"Green environment response time too high: {response_time}ms")
        
        # Error rate validation
        error_rate = await self._get_error_rate(model_config, "green")
        if error_rate > 0.01:  # 1% threshold
            raise RuntimeError(f"Green environment error rate too high: {error_rate}")
        
        logger.info("‚úÖ Green environment validation passed")
    
    async def _switch_traffic_to_green(self, model_config: ModelDeploymentConfig):
        """Switch traffic to green environment"""
        logger.info("üîÑ Switching traffic to green environment")
        
        # Update load balancer or ingress configuration
        # This would typically involve updating DNS or load balancer rules
        
        logger.info("‚úÖ Traffic switched to green environment")
    
    async def _post_deployment_validation(self, model_config: ModelDeploymentConfig):
        """Post-deployment validation checks"""
        logger.info("üîç Running post-deployment validation")
        
        self.metrics.current_phase = "post_deployment_validation"
        
        # Validate all deployments are healthy
        await self._validate_all_deployments_healthy(model_config)
        
        # Performance validation
        await self._validate_performance_metrics(model_config)
        
        # End-to-end testing
        await self._run_end_to_end_tests(model_config)
        
        logger.info("‚úÖ Post-deployment validation completed")
    
    async def _validate_all_deployments_healthy(self, model_config: ModelDeploymentConfig):
        """Validate all deployments are healthy"""
        unhealthy_deployments = []
        
        for deployment_name, healthy in self.health_monitors.items():
            if not healthy:
                unhealthy_deployments.append(deployment_name)
        
        if unhealthy_deployments:
            raise RuntimeError(f"Unhealthy deployments: {unhealthy_deployments}")
        
        # Update metrics
        self.metrics.healthy_replicas = sum(
            target.replicas for target in self.deployment_targets
            if self.health_monitors.get(f"{model_config.model_name}-{target.name}", False)
        )
    
    async def _validate_performance_metrics(self, model_config: ModelDeploymentConfig):
        """Validate performance metrics"""
        # Check overall error rate
        overall_error_rate = await self._get_error_rate(model_config, "production")
        if overall_error_rate > 0.01:  # 1% threshold
            self.metrics.warnings.append(f"High error rate detected: {overall_error_rate:.2%}")
        
        # Check response time
        response_time = await self._get_response_time(model_config, "production")
        if response_time > 500:  # 500ms threshold
            self.metrics.warnings.append(f"High response time detected: {response_time}ms")
        
        # Update metrics
        self.metrics.error_rate = overall_error_rate
        self.metrics.response_time_ms = response_time
        self.metrics.throughput_rps = 150  # Simulated throughput
    
    async def _run_end_to_end_tests(self, model_config: ModelDeploymentConfig):
        """Run end-to-end tests"""
        logger.info("Running end-to-end tests")
        
        # Simulate end-to-end test execution
        # In production, this would run actual test suites
        
        logger.info("‚úÖ End-to-end tests passed")
    
    async def _execute_automatic_rollback(self, model_config: ModelDeploymentConfig):
        """Execute automatic rollback"""
        logger.info("üîÑ Executing automatic rollback")
        
        rollback_start_time = time.time()
        
        try:
            # Rollback deployments
            for target in self.deployment_targets:
                if target.enabled:
                    await self._rollback_deployment(model_config, target)
            
            # Update metrics
            self.metrics.status = "rolled_back"
            self.metrics.rollback_duration_seconds = time.time() - rollback_start_time
            
            logger.info("‚úÖ Automatic rollback completed")
            
        except Exception as e:
            logger.error(f"‚ùå Automatic rollback failed: {str(e)}")
            self.metrics.errors.append(f"Rollback failed: {str(e)}")
    
    async def _rollback_deployment(self, model_config: ModelDeploymentConfig,
                                  target: DeploymentTarget):
        """Rollback deployment to previous version"""
        if not self.k8s_client:
            logger.warning("Kubernetes client not available - skipping rollback")
            return
        
        deployment_name = f"{model_config.model_name}-{target.name}"
        
        try:
            # Rollback using kubectl
            cmd = ["kubectl", "rollout", "undo", f"deployment/{deployment_name}",
                   "--namespace", target.namespace, "--context", target.kubernetes_context]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                raise RuntimeError(f"Rollback failed: {result.stderr}")
            
            logger.info(f"Rollback completed for {deployment_name}")
            
        except Exception as e:
            logger.error(f"Rollback failed for {deployment_name}: {str(e)}")
            raise
    
    async def _send_deployment_notification(self, status: str, model_config: ModelDeploymentConfig,
                                          error: str = None):
        """Send deployment notification"""
        if not self.config.get('notification', {}).get('enabled', False):
            return
        
        webhook_url = self.config.get('notification', {}).get('slack_webhook')
        if not webhook_url:
            return
        
        color = "good" if status == "success" else "danger"
        title = f"Deployment {status.upper()}: {model_config.model_name}"
        
        message = {
            "channel": "#deployments",
            "username": "Deployment Bot",
            "text": title,
            "attachments": [
                {
                    "color": color,
                    "fields": [
                        {"title": "Model", "value": model_config.model_name, "short": True},
                        {"title": "Version", "value": model_config.model_version, "short": True},
                        {"title": "Strategy", "value": model_config.deployment_strategy, "short": True},
                        {"title": "Duration", "value": f"{self.metrics.deployment_duration_seconds:.1f}s", "short": True}
                    ]
                }
            ]
        }
        
        if error:
            message["attachments"][0]["fields"].append(
                {"title": "Error", "value": error, "short": False}
            )
        
        try:
            response = requests.post(webhook_url, json=message)
            response.raise_for_status()
            logger.info("Deployment notification sent")
        except Exception as e:
            logger.warning(f"Failed to send notification: {str(e)}")
    
    async def _generate_deployment_report(self, model_config: ModelDeploymentConfig):
        """Generate deployment report"""
        report = {
            'deployment_id': self.deployment_id,
            'timestamp': datetime.now().isoformat(),
            'model_name': model_config.model_name,
            'model_version': model_config.model_version,
            'deployment_strategy': model_config.deployment_strategy,
            'status': self.metrics.status,
            'duration_seconds': self.metrics.deployment_duration_seconds,
            'environments_deployed': self.metrics.environments_deployed,
            'total_replicas': self.metrics.total_replicas,
            'healthy_replicas': self.metrics.healthy_replicas,
            'performance_metrics': {
                'error_rate': self.metrics.error_rate,
                'response_time_ms': self.metrics.response_time_ms,
                'throughput_rps': self.metrics.throughput_rps
            },
            'health_checks': {
                'passed': self.metrics.health_checks_passed,
                'failed': self.metrics.health_checks_failed
            },
            'warnings': self.metrics.warnings,
            'errors': self.metrics.errors,
            'events': self.metrics.events
        }
        
        # Save report
        report_file = self.artifacts_dir / f"deployment_report_{self.deployment_id}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"Deployment report generated: {report_file}")


# Factory function
def create_automated_deployment_system(config_path: str = None) -> AutomatedDeploymentSystem:
    """Create automated deployment system instance"""
    return AutomatedDeploymentSystem(config_path)


# CLI interface
async def main():
    """Main deployment CLI"""
    import argparse
    
    parser = argparse.ArgumentParser(description="GrandModel Automated Deployment System")
    parser.add_argument("--config", help="Deployment configuration file")
    parser.add_argument("--model-name", required=True, help="Model name to deploy")
    parser.add_argument("--model-version", required=True, help="Model version")
    parser.add_argument("--model-path", required=True, help="Path to model file")
    parser.add_argument("--model-type", choices=["tactical", "strategic"], required=True, help="Model type")
    parser.add_argument("--strategy", choices=["rolling", "canary", "blue-green"], 
                       default="rolling", help="Deployment strategy")
    parser.add_argument("--environments", nargs="+", help="Target environments")
    
    args = parser.parse_args()
    
    # Create deployment system
    deployment_system = create_automated_deployment_system(args.config)
    
    # Create model configuration
    model_config = ModelDeploymentConfig(
        model_name=args.model_name,
        model_version=args.model_version,
        model_path=args.model_path,
        model_type=args.model_type,
        deployment_strategy=args.strategy
    )
    
    try:
        # Execute deployment
        metrics = await deployment_system.deploy_model(model_config, args.environments)
        
        if metrics.status == "success":
            print(f"‚úÖ Deployment completed successfully")
            print(f"   Model: {model_config.model_name}")
            print(f"   Version: {model_config.model_version}")
            print(f"   Strategy: {model_config.deployment_strategy}")
            print(f"   Duration: {metrics.deployment_duration_seconds:.1f}s")
            print(f"   Environments: {metrics.environments_deployed}")
            print(f"   Healthy replicas: {metrics.healthy_replicas}/{metrics.total_replicas}")
            
            if metrics.warnings:
                print(f"   Warnings: {len(metrics.warnings)}")
            
            sys.exit(0)
        else:
            print(f"‚ùå Deployment failed: {metrics.status}")
            sys.exit(1)
            
    except Exception as e:
        print(f"‚ùå Deployment failed: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())