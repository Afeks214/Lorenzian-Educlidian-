"""
Risk Metrics Calculation for Superpositions

This module provides comprehensive risk metrics calculation for sequential risk
superpositions generated by the MARL system. It calculates:

1. Risk attribution metrics across sequential decisions
2. Superposition quality and consistency metrics
3. Performance metrics for real-time constraints
4. Correlation-aware risk measures
5. Emergency protocol effectiveness metrics

Key Features:
- Real-time risk metric calculation
- Sequential decision impact analysis
- VaR correlation system integration
- Performance constraint validation
- Emergency protocol assessment
"""

import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict, deque
import structlog

from src.environment.sequential_risk_env import RiskSuperposition, SequentialContext
from src.risk.core.correlation_tracker import CorrelationTracker, CorrelationRegime
from src.risk.core.var_calculator import VaRCalculator, VaRResult
from src.integration.var_superposition_integration import RiskSuperpositionValidation
from src.core.events import EventBus, Event, EventType

logger = structlog.get_logger()


@dataclass
class SuperpositionRiskMetrics:
    """Comprehensive risk metrics for a superposition"""
    # Basic metrics
    timestamp: datetime
    superposition_id: str
    total_risk_score: float
    quality_score: float
    consistency_score: float
    performance_score: float
    
    # Sequential metrics
    sequential_coordination_score: float
    decision_coherence_score: float
    context_preservation_score: float
    
    # Risk attribution
    position_sizing_risk_contribution: float
    stop_target_risk_contribution: float
    risk_monitor_risk_contribution: float
    portfolio_optimizer_risk_contribution: float
    
    # Performance metrics
    calculation_time_ms: float
    var_calculation_time_ms: float
    correlation_update_time_ms: float
    performance_target_met: bool
    
    # VaR metrics
    portfolio_var: float
    component_vars: Dict[str, float]
    var_utilization: float
    var_efficiency: float
    
    # Correlation metrics
    correlation_regime: str
    avg_correlation: float
    max_correlation: float
    correlation_stability: float
    
    # Emergency metrics
    emergency_protocols_active: int
    emergency_severity_score: float
    emergency_response_time_ms: float
    
    # Quality metrics
    completeness_score: float
    validation_score: float
    constraint_satisfaction_score: float
    
    # Confidence metrics
    overall_confidence: float
    component_confidences: Dict[str, float]
    uncertainty_score: float


@dataclass
class SequentialRiskAttribution:
    """Risk attribution across sequential decisions"""
    timestamp: datetime
    
    # Position sizing attribution
    position_sizing_var_contribution: float
    position_sizing_correlation_impact: float
    position_sizing_leverage_impact: float
    
    # Stop/target attribution
    stop_target_risk_reduction: float
    stop_target_volatility_impact: float
    stop_target_drawdown_protection: float
    
    # Risk monitor attribution
    risk_monitor_alert_effectiveness: float
    risk_monitor_emergency_trigger_accuracy: float
    risk_monitor_correlation_detection: float
    
    # Portfolio optimizer attribution
    portfolio_optimizer_diversification_benefit: float
    portfolio_optimizer_risk_parity_score: float
    portfolio_optimizer_constraint_satisfaction: float
    
    # Interaction effects
    position_stop_interaction: float
    stop_monitor_interaction: float
    monitor_optimizer_interaction: float
    
    # Overall attribution
    total_risk_reduction: float
    sequential_synergy_score: float


@dataclass
class PerformanceRiskMetrics:
    """Performance-related risk metrics"""
    timestamp: datetime
    
    # Timing metrics
    total_processing_time_ms: float
    var_calculation_time_ms: float
    correlation_update_time_ms: float
    decision_making_time_ms: float
    
    # Performance targets
    var_target_ms: float
    var_target_met: bool
    correlation_target_ms: float
    correlation_target_met: bool
    overall_target_met: bool
    
    # Violation metrics
    performance_violation_severity: float
    violation_count: int
    violation_duration_ms: float
    
    # Degradation metrics
    performance_degradation_rate: float
    bottleneck_identification: Dict[str, float]
    optimization_opportunities: List[str]
    
    # Recovery metrics
    recovery_time_ms: float
    fallback_usage_rate: float
    system_resilience_score: float


class SuperpositionRiskMetricsCalculator:
    """
    Calculator for comprehensive risk metrics on superpositions
    
    This calculator provides detailed risk analysis for sequential risk
    superpositions, including attribution, performance, and quality metrics.
    """
    
    def __init__(self, 
                 correlation_tracker: CorrelationTracker,
                 var_calculator: VaRCalculator,
                 event_bus: EventBus,
                 config: Dict[str, Any]):
        
        self.correlation_tracker = correlation_tracker
        self.var_calculator = var_calculator
        self.event_bus = event_bus
        self.config = config
        
        # Configuration
        self.var_target_ms = config.get('var_target_ms', 5.0)
        self.correlation_target_ms = config.get('correlation_target_ms', 2.0)
        self.risk_tolerance = config.get('risk_tolerance', 0.05)
        
        # Metrics history
        self.metrics_history: deque = deque(maxlen=1000)
        self.attribution_history: deque = deque(maxlen=1000)
        self.performance_history: deque = deque(maxlen=1000)
        
        # Benchmark values
        self.benchmark_metrics = self._initialize_benchmarks()
        
        # Risk component weights
        self.component_weights = {
            'position_sizing': 0.3,
            'stop_target': 0.25,
            'risk_monitor': 0.25,
            'portfolio_optimizer': 0.2
        }
        
        logger.info("SuperpositionRiskMetricsCalculator initialized",
                   var_target_ms=self.var_target_ms,
                   correlation_target_ms=self.correlation_target_ms)
    
    def _initialize_benchmarks(self) -> Dict[str, float]:
        """Initialize benchmark values for comparison"""
        return {
            'baseline_var': 0.02,  # 2% VaR
            'baseline_correlation': 0.3,  # 30% correlation
            'baseline_quality': 0.8,  # 80% quality
            'baseline_performance': 5.0,  # 5ms performance
            'baseline_consistency': 0.85,  # 85% consistency
            'baseline_completeness': 0.9  # 90% completeness
        }
    
    def calculate_superposition_metrics(self, 
                                      superposition: RiskSuperposition,
                                      sequential_context: SequentialContext,
                                      validation: RiskSuperpositionValidation,
                                      performance_data: Dict[str, Any]) -> SuperpositionRiskMetrics:
        """
        Calculate comprehensive risk metrics for a superposition
        
        Args:
            superposition: Risk superposition to analyze
            sequential_context: Sequential decision context
            validation: Validation results
            performance_data: Performance timing data
            
        Returns:
            Comprehensive risk metrics
        """
        calculation_start_time = datetime.now()
        
        # Calculate component metrics
        basic_metrics = self._calculate_basic_metrics(superposition, validation)
        sequential_metrics = self._calculate_sequential_metrics(superposition, sequential_context)
        risk_attribution = self._calculate_risk_attribution(superposition, sequential_context)
        performance_metrics = self._calculate_performance_metrics(performance_data)
        var_metrics = self._calculate_var_metrics(superposition)
        correlation_metrics = self._calculate_correlation_metrics(superposition)
        emergency_metrics = self._calculate_emergency_metrics(superposition)
        quality_metrics = self._calculate_quality_metrics(superposition, validation)
        confidence_metrics = self._calculate_confidence_metrics(superposition)
        
        # Calculate overall scores
        total_risk_score = self._calculate_total_risk_score(
            basic_metrics, sequential_metrics, risk_attribution, var_metrics
        )
        
        calculation_time = (datetime.now() - calculation_start_time).total_seconds() * 1000
        
        # Create comprehensive metrics
        metrics = SuperpositionRiskMetrics(
            timestamp=datetime.now(),
            superposition_id=f"superposition_{superposition.timestamp.isoformat()}",
            total_risk_score=total_risk_score,
            quality_score=basic_metrics['quality_score'],
            consistency_score=basic_metrics['consistency_score'],
            performance_score=basic_metrics['performance_score'],
            
            sequential_coordination_score=sequential_metrics['coordination_score'],
            decision_coherence_score=sequential_metrics['coherence_score'],
            context_preservation_score=sequential_metrics['context_preservation_score'],
            
            position_sizing_risk_contribution=risk_attribution['position_sizing_contribution'],
            stop_target_risk_contribution=risk_attribution['stop_target_contribution'],
            risk_monitor_risk_contribution=risk_attribution['risk_monitor_contribution'],
            portfolio_optimizer_risk_contribution=risk_attribution['portfolio_optimizer_contribution'],
            
            calculation_time_ms=calculation_time,
            var_calculation_time_ms=performance_metrics['var_time_ms'],
            correlation_update_time_ms=performance_metrics['correlation_time_ms'],
            performance_target_met=performance_metrics['target_met'],
            
            portfolio_var=var_metrics['portfolio_var'],
            component_vars=var_metrics['component_vars'],
            var_utilization=var_metrics['var_utilization'],
            var_efficiency=var_metrics['var_efficiency'],
            
            correlation_regime=correlation_metrics['regime'],
            avg_correlation=correlation_metrics['avg_correlation'],
            max_correlation=correlation_metrics['max_correlation'],
            correlation_stability=correlation_metrics['stability'],
            
            emergency_protocols_active=emergency_metrics['active_protocols'],
            emergency_severity_score=emergency_metrics['severity_score'],
            emergency_response_time_ms=emergency_metrics['response_time_ms'],
            
            completeness_score=quality_metrics['completeness_score'],
            validation_score=quality_metrics['validation_score'],
            constraint_satisfaction_score=quality_metrics['constraint_satisfaction_score'],
            
            overall_confidence=confidence_metrics['overall_confidence'],
            component_confidences=confidence_metrics['component_confidences'],
            uncertainty_score=confidence_metrics['uncertainty_score']
        )
        
        # Store metrics
        self.metrics_history.append(metrics)
        
        return metrics
    
    def _calculate_basic_metrics(self, 
                               superposition: RiskSuperposition,
                               validation: RiskSuperpositionValidation) -> Dict[str, float]:
        """Calculate basic risk metrics"""
        # Quality score from validation
        quality_score = validation.quality_score
        
        # Consistency score
        consistency_score = validation.consistency_score
        
        # Performance score
        performance_score = validation.performance_score
        
        return {
            'quality_score': quality_score,
            'consistency_score': consistency_score,
            'performance_score': performance_score
        }
    
    def _calculate_sequential_metrics(self, 
                                    superposition: RiskSuperposition,
                                    sequential_context: SequentialContext) -> Dict[str, float]:
        """Calculate sequential coordination metrics"""
        # Coordination score based on decision flow
        coordination_score = self._assess_decision_coordination(superposition, sequential_context)
        
        # Coherence score based on decision consistency
        coherence_score = self._assess_decision_coherence(superposition, sequential_context)
        
        # Context preservation score
        context_preservation_score = self._assess_context_preservation(sequential_context)
        
        return {
            'coordination_score': coordination_score,
            'coherence_score': coherence_score,
            'context_preservation_score': context_preservation_score
        }
    
    def _assess_decision_coordination(self, 
                                    superposition: RiskSuperposition,
                                    sequential_context: SequentialContext) -> float:
        """Assess how well sequential decisions coordinate"""
        coordination_factors = []
        
        # Check position sizing → stop/target coordination
        if superposition.position_allocations and superposition.stop_loss_orders:
            position_assets = set(superposition.position_allocations.keys())
            stop_assets = set(superposition.stop_loss_orders.keys())
            overlap = len(position_assets.intersection(stop_assets))
            coordination_factors.append(overlap / len(position_assets) if position_assets else 0)
        
        # Check stop/target → risk monitor coordination
        if superposition.stop_loss_orders and superposition.emergency_protocols:
            # If emergency protocols are active, stops should be tighter
            avg_stop = np.mean(list(superposition.stop_loss_orders.values())) if superposition.stop_loss_orders else 0
            emergency_adjustment = 0.8 if superposition.emergency_protocols else 1.0
            coordination_factors.append(min(1.0, emergency_adjustment / (avg_stop + 0.01)))
        
        # Check risk monitor → portfolio optimizer coordination
        if superposition.emergency_protocols and superposition.position_allocations:
            # Portfolio should be more conservative if emergency protocols are active
            total_allocation = sum(abs(pos) for pos in superposition.position_allocations.values())
            emergency_factor = 0.8 if superposition.emergency_protocols else 1.0
            coordination_factors.append(min(1.0, emergency_factor / (total_allocation + 0.01)))
        
        return np.mean(coordination_factors) if coordination_factors else 0.5
    
    def _assess_decision_coherence(self, 
                                 superposition: RiskSuperposition,
                                 sequential_context: SequentialContext) -> float:
        """Assess coherence of sequential decisions"""
        coherence_factors = []
        
        # Check risk-reward coherence
        if superposition.position_allocations and superposition.stop_loss_orders and superposition.target_profit_orders:
            for asset in superposition.position_allocations.keys():
                position = superposition.position_allocations.get(asset, 0)
                stop = superposition.stop_loss_orders.get(asset, 0.01)
                target = superposition.target_profit_orders.get(asset, 0.01)
                
                if position != 0 and stop > 0 and target > 0:
                    # Risk-reward ratio should be reasonable
                    risk_reward_ratio = target / stop
                    coherence_factors.append(min(1.0, risk_reward_ratio / 3.0))  # Max credit for 3:1 ratio
        
        # Check correlation coherence
        if superposition.correlation_adjustments:
            # Correlation adjustments should be consistent with regime
            correlation_regime = self.correlation_tracker.current_regime
            if correlation_regime in [CorrelationRegime.CRISIS, CorrelationRegime.SHOCK]:
                # Should have significant correlation adjustments
                adjustment_magnitude = np.mean([abs(adj) for adj in superposition.correlation_adjustments.values() if isinstance(adj, (int, float))])
                coherence_factors.append(min(1.0, adjustment_magnitude * 2.0))
        
        return np.mean(coherence_factors) if coherence_factors else 0.5
    
    def _assess_context_preservation(self, sequential_context: SequentialContext) -> float:
        """Assess how well context is preserved across decisions"""
        preservation_factors = []
        
        # Check metadata preservation
        if sequential_context.execution_metadata:
            metadata = sequential_context.execution_metadata
            if 'timestamp' in metadata and 'sequence_step' in metadata:
                preservation_factors.append(1.0)
        
        # Check performance metrics preservation
        if sequential_context.performance_metrics:
            metrics = sequential_context.performance_metrics
            if 'var_estimate' in metrics and 'calculation_time_ms' in metrics:
                preservation_factors.append(1.0)
        
        # Check correlation context preservation
        if sequential_context.correlation_context:
            context = sequential_context.correlation_context
            if 'avg_correlation' in context and 'regime' in context:
                preservation_factors.append(1.0)
        
        return np.mean(preservation_factors) if preservation_factors else 0.0
    
    def _calculate_risk_attribution(self, 
                                  superposition: RiskSuperposition,
                                  sequential_context: SequentialContext) -> Dict[str, float]:
        """Calculate risk attribution across sequential decisions"""
        
        # Get total risk baseline
        total_risk_baseline = self._calculate_baseline_risk(superposition)
        
        # Calculate individual contributions
        position_sizing_contribution = self._calculate_position_sizing_contribution(
            superposition, sequential_context, total_risk_baseline
        )
        
        stop_target_contribution = self._calculate_stop_target_contribution(
            superposition, sequential_context, total_risk_baseline
        )
        
        risk_monitor_contribution = self._calculate_risk_monitor_contribution(
            superposition, sequential_context, total_risk_baseline
        )
        
        portfolio_optimizer_contribution = self._calculate_portfolio_optimizer_contribution(
            superposition, sequential_context, total_risk_baseline
        )
        
        return {
            'position_sizing_contribution': position_sizing_contribution,
            'stop_target_contribution': stop_target_contribution,
            'risk_monitor_contribution': risk_monitor_contribution,
            'portfolio_optimizer_contribution': portfolio_optimizer_contribution
        }
    
    def _calculate_baseline_risk(self, superposition: RiskSuperposition) -> float:
        """Calculate baseline risk for attribution"""
        # Use VaR estimates if available
        if superposition.var_estimates:
            var_values = [v for v in superposition.var_estimates.values() if isinstance(v, (int, float))]
            if var_values:
                return np.mean(var_values)
        
        # Fallback to position-based risk
        if superposition.position_allocations:
            total_allocation = sum(abs(pos) for pos in superposition.position_allocations.values())
            return total_allocation * 0.02  # 2% risk per unit allocation
        
        return 0.02  # Default 2% risk
    
    def _calculate_position_sizing_contribution(self, 
                                              superposition: RiskSuperposition,
                                              sequential_context: SequentialContext,
                                              baseline_risk: float) -> float:
        """Calculate position sizing contribution to risk"""
        if not superposition.position_allocations:
            return 0.0
        
        # Calculate concentration risk
        positions = np.array(list(superposition.position_allocations.values()))
        concentration = np.sum(positions ** 2)
        
        # Calculate diversification benefit
        diversification_benefit = 1.0 - min(1.0, concentration)
        
        # Calculate leverage impact
        total_leverage = np.sum(np.abs(positions))
        leverage_impact = min(1.0, total_leverage)
        
        # Combine factors
        contribution = baseline_risk * leverage_impact * (1.0 - diversification_benefit * 0.5)
        
        return contribution
    
    def _calculate_stop_target_contribution(self, 
                                          superposition: RiskSuperposition,
                                          sequential_context: SequentialContext,
                                          baseline_risk: float) -> float:
        """Calculate stop/target contribution to risk"""
        if not superposition.stop_loss_orders:
            return 0.0
        
        # Calculate risk reduction from stops
        stop_values = list(superposition.stop_loss_orders.values())
        avg_stop = np.mean(stop_values)
        
        # Risk reduction is inversely related to stop distance
        risk_reduction = min(0.8, 0.05 / (avg_stop + 0.001))  # Max 80% reduction
        
        # Calculate target benefit
        target_benefit = 0.0
        if superposition.target_profit_orders:
            target_values = list(superposition.target_profit_orders.values())
            avg_target = np.mean(target_values)
            risk_reward_ratio = avg_target / avg_stop
            target_benefit = min(0.2, risk_reward_ratio * 0.1)  # Max 20% benefit
        
        # Combine factors
        contribution = baseline_risk * (1.0 - risk_reduction + target_benefit)
        
        return contribution
    
    def _calculate_risk_monitor_contribution(self, 
                                           superposition: RiskSuperposition,
                                           sequential_context: SequentialContext,
                                           baseline_risk: float) -> float:
        """Calculate risk monitor contribution to risk"""
        
        # Emergency protocol impact
        emergency_impact = 0.0
        if superposition.emergency_protocols:
            # Emergency protocols should reduce risk
            emergency_impact = -0.3 * len(superposition.emergency_protocols)
        
        # Correlation adjustment impact
        correlation_impact = 0.0
        if superposition.correlation_adjustments:
            # Correlation adjustments should account for regime changes
            correlation_regime = self.correlation_tracker.current_regime
            if correlation_regime in [CorrelationRegime.CRISIS, CorrelationRegime.SHOCK]:
                correlation_impact = -0.2  # Risk reduction in crisis
        
        # Combine factors
        contribution = baseline_risk * (1.0 + emergency_impact + correlation_impact)
        
        return max(0.0, contribution)
    
    def _calculate_portfolio_optimizer_contribution(self, 
                                                  superposition: RiskSuperposition,
                                                  sequential_context: SequentialContext,
                                                  baseline_risk: float) -> float:
        """Calculate portfolio optimizer contribution to risk"""
        
        # Diversification benefit
        diversification_benefit = 0.0
        if superposition.position_allocations:
            positions = np.array(list(superposition.position_allocations.values()))
            if len(positions) > 1:
                # Calculate entropy as diversification measure
                normalized_positions = np.abs(positions) / np.sum(np.abs(positions))
                entropy = -np.sum(normalized_positions * np.log(normalized_positions + 1e-8))
                max_entropy = np.log(len(positions))
                diversification_benefit = entropy / max_entropy if max_entropy > 0 else 0
        
        # Optimization quality
        optimization_quality = 0.0
        if superposition.confidence_scores:
            portfolio_confidence = superposition.confidence_scores.get('portfolio_optimizer', 0.0)
            optimization_quality = portfolio_confidence
        
        # Combine factors
        contribution = baseline_risk * (1.0 - diversification_benefit * 0.3) * (1.0 - optimization_quality * 0.2)
        
        return contribution
    
    def _calculate_performance_metrics(self, performance_data: Dict[str, Any]) -> Dict[str, float]:
        """Calculate performance-related metrics"""
        var_time_ms = performance_data.get('var_calculation_time_ms', 0.0)
        correlation_time_ms = performance_data.get('correlation_update_time_ms', 0.0)
        
        var_target_met = var_time_ms < self.var_target_ms
        correlation_target_met = correlation_time_ms < self.correlation_target_ms
        overall_target_met = var_target_met and correlation_target_met
        
        return {
            'var_time_ms': var_time_ms,
            'correlation_time_ms': correlation_time_ms,
            'target_met': overall_target_met,
            'var_target_met': var_target_met,
            'correlation_target_met': correlation_target_met
        }
    
    def _calculate_var_metrics(self, superposition: RiskSuperposition) -> Dict[str, Any]:
        """Calculate VaR-related metrics"""
        # Get latest VaR result
        var_result = self.var_calculator.get_latest_var()
        
        if var_result:
            portfolio_var = var_result.portfolio_var
            component_vars = var_result.component_vars
            
            # Calculate VaR utilization
            var_limit = self.risk_tolerance * 1_000_000  # Assume 1M portfolio
            var_utilization = portfolio_var / var_limit if var_limit > 0 else 0
            
            # Calculate VaR efficiency
            var_efficiency = self._calculate_var_efficiency(var_result)
            
            return {
                'portfolio_var': portfolio_var,
                'component_vars': component_vars,
                'var_utilization': var_utilization,
                'var_efficiency': var_efficiency
            }
        
        return {
            'portfolio_var': 0.0,
            'component_vars': {},
            'var_utilization': 0.0,
            'var_efficiency': 0.0
        }
    
    def _calculate_var_efficiency(self, var_result: VaRResult) -> float:
        """Calculate VaR efficiency score"""
        if var_result.performance_ms > 0:
            # Efficiency is inverse of calculation time
            time_efficiency = min(1.0, self.var_target_ms / var_result.performance_ms)
            
            # Accuracy efficiency (simplified)
            accuracy_efficiency = 0.9  # Assume 90% accuracy
            
            return (time_efficiency + accuracy_efficiency) / 2.0
        
        return 0.0
    
    def _calculate_correlation_metrics(self, superposition: RiskSuperposition) -> Dict[str, Any]:
        """Calculate correlation-related metrics"""
        # Get current correlation state
        correlation_matrix = self.correlation_tracker.get_correlation_matrix()
        regime = self.correlation_tracker.current_regime
        
        if correlation_matrix is not None:
            # Calculate average correlation
            upper_tri = np.triu(correlation_matrix, k=1)
            avg_correlation = np.mean(upper_tri[upper_tri != 0])
            
            # Calculate max correlation
            max_correlation = np.max(correlation_matrix[correlation_matrix < 1.0])
            
            # Calculate stability
            stability = self._calculate_correlation_stability()
            
            return {
                'regime': regime.value,
                'avg_correlation': avg_correlation,
                'max_correlation': max_correlation,
                'stability': stability
            }
        
        return {
            'regime': 'UNKNOWN',
            'avg_correlation': 0.0,
            'max_correlation': 0.0,
            'stability': 0.0
        }
    
    def _calculate_correlation_stability(self) -> float:
        """Calculate correlation stability score"""
        # Get recent correlation history
        if hasattr(self.correlation_tracker, 'correlation_history') and self.correlation_tracker.correlation_history:
            recent_correlations = list(self.correlation_tracker.correlation_history)[-10:]
            if len(recent_correlations) > 1:
                correlation_values = [corr for _, corr in recent_correlations]
                stability = 1.0 - np.std(correlation_values)
                return max(0.0, min(1.0, stability))
        
        return 0.5  # Default stability
    
    def _calculate_emergency_metrics(self, superposition: RiskSuperposition) -> Dict[str, Any]:
        """Calculate emergency protocol metrics"""
        active_protocols = len(superposition.emergency_protocols)
        
        # Calculate severity score
        severity_score = 0.0
        if superposition.emergency_protocols:
            # Higher severity for more protocols
            severity_score = min(1.0, active_protocols * 0.3)
        
        # Calculate response time (simplified)
        response_time_ms = 100.0 * active_protocols  # Assume 100ms per protocol
        
        return {
            'active_protocols': active_protocols,
            'severity_score': severity_score,
            'response_time_ms': response_time_ms
        }
    
    def _calculate_quality_metrics(self, 
                                 superposition: RiskSuperposition,
                                 validation: RiskSuperpositionValidation) -> Dict[str, float]:
        """Calculate quality metrics"""
        completeness_score = validation.completeness_score
        validation_score = 1.0 - len(validation.validation_errors) * 0.2
        
        # Calculate constraint satisfaction
        constraint_satisfaction_score = 0.0
        if superposition.risk_limits:
            # Check if risk limits are reasonable
            constraint_satisfaction_score = 1.0
            for limit_type, limit_value in superposition.risk_limits.items():
                if limit_value <= 0:
                    constraint_satisfaction_score -= 0.25
        
        return {
            'completeness_score': completeness_score,
            'validation_score': max(0.0, validation_score),
            'constraint_satisfaction_score': max(0.0, constraint_satisfaction_score)
        }
    
    def _calculate_confidence_metrics(self, superposition: RiskSuperposition) -> Dict[str, Any]:
        """Calculate confidence metrics"""
        # Overall confidence from superposition
        overall_confidence = 0.0
        component_confidences = {}
        
        if superposition.confidence_scores:
            component_confidences = superposition.confidence_scores
            overall_confidence = np.mean(list(component_confidences.values()))
        
        # Calculate uncertainty score
        uncertainty_score = 1.0 - overall_confidence
        
        return {
            'overall_confidence': overall_confidence,
            'component_confidences': component_confidences,
            'uncertainty_score': uncertainty_score
        }
    
    def _calculate_total_risk_score(self, 
                                  basic_metrics: Dict[str, float],
                                  sequential_metrics: Dict[str, float],
                                  risk_attribution: Dict[str, float],
                                  var_metrics: Dict[str, Any]) -> float:
        """Calculate total risk score"""
        # Combine different components
        quality_component = basic_metrics['quality_score'] * 0.3
        performance_component = basic_metrics['performance_score'] * 0.2
        coordination_component = sequential_metrics['coordination_score'] * 0.2
        var_component = min(1.0, var_metrics['var_utilization']) * 0.3
        
        total_score = quality_component + performance_component + coordination_component + var_component
        
        return min(1.0, total_score)
    
    def calculate_sequential_attribution(self, 
                                       superposition: RiskSuperposition,
                                       sequential_context: SequentialContext) -> SequentialRiskAttribution:
        """Calculate detailed sequential risk attribution"""
        
        # Position sizing attribution
        position_sizing_var = self._calculate_position_sizing_var_contribution(superposition, sequential_context)
        position_sizing_correlation = self._calculate_position_sizing_correlation_impact(superposition, sequential_context)
        position_sizing_leverage = self._calculate_position_sizing_leverage_impact(superposition, sequential_context)
        
        # Stop/target attribution
        stop_target_risk_reduction = self._calculate_stop_target_risk_reduction(superposition, sequential_context)
        stop_target_volatility = self._calculate_stop_target_volatility_impact(superposition, sequential_context)
        stop_target_drawdown = self._calculate_stop_target_drawdown_protection(superposition, sequential_context)
        
        # Risk monitor attribution
        risk_monitor_alert = self._calculate_risk_monitor_alert_effectiveness(superposition, sequential_context)
        risk_monitor_emergency = self._calculate_risk_monitor_emergency_accuracy(superposition, sequential_context)
        risk_monitor_correlation = self._calculate_risk_monitor_correlation_detection(superposition, sequential_context)
        
        # Portfolio optimizer attribution
        portfolio_diversification = self._calculate_portfolio_diversification_benefit(superposition, sequential_context)
        portfolio_risk_parity = self._calculate_portfolio_risk_parity_score(superposition, sequential_context)
        portfolio_constraints = self._calculate_portfolio_constraint_satisfaction(superposition, sequential_context)
        
        # Interaction effects
        position_stop_interaction = self._calculate_position_stop_interaction(superposition, sequential_context)
        stop_monitor_interaction = self._calculate_stop_monitor_interaction(superposition, sequential_context)
        monitor_optimizer_interaction = self._calculate_monitor_optimizer_interaction(superposition, sequential_context)
        
        # Overall attribution
        total_risk_reduction = self._calculate_total_risk_reduction(superposition, sequential_context)
        sequential_synergy = self._calculate_sequential_synergy_score(superposition, sequential_context)
        
        attribution = SequentialRiskAttribution(
            timestamp=datetime.now(),
            position_sizing_var_contribution=position_sizing_var,
            position_sizing_correlation_impact=position_sizing_correlation,
            position_sizing_leverage_impact=position_sizing_leverage,
            stop_target_risk_reduction=stop_target_risk_reduction,
            stop_target_volatility_impact=stop_target_volatility,
            stop_target_drawdown_protection=stop_target_drawdown,
            risk_monitor_alert_effectiveness=risk_monitor_alert,
            risk_monitor_emergency_trigger_accuracy=risk_monitor_emergency,
            risk_monitor_correlation_detection=risk_monitor_correlation,
            portfolio_optimizer_diversification_benefit=portfolio_diversification,
            portfolio_optimizer_risk_parity_score=portfolio_risk_parity,
            portfolio_optimizer_constraint_satisfaction=portfolio_constraints,
            position_stop_interaction=position_stop_interaction,
            stop_monitor_interaction=stop_monitor_interaction,
            monitor_optimizer_interaction=monitor_optimizer_interaction,
            total_risk_reduction=total_risk_reduction,
            sequential_synergy_score=sequential_synergy
        )
        
        self.attribution_history.append(attribution)
        return attribution
    
    def calculate_performance_risk_metrics(self, performance_data: Dict[str, Any]) -> PerformanceRiskMetrics:
        """Calculate performance-related risk metrics"""
        
        # Extract timing data
        total_time = performance_data.get('total_integration_time_ms', 0.0)
        var_time = performance_data.get('var_calculation_time_ms', 0.0)
        correlation_time = performance_data.get('correlation_update_time_ms', 0.0)
        decision_time = performance_data.get('decision_making_time_ms', 0.0)
        
        # Check targets
        var_target_met = var_time < self.var_target_ms
        correlation_target_met = correlation_time < self.correlation_target_ms
        overall_target_met = var_target_met and correlation_target_met
        
        # Calculate violations
        violation_severity = 0.0
        if not overall_target_met:
            var_violation = max(0, var_time - self.var_target_ms) / self.var_target_ms
            correlation_violation = max(0, correlation_time - self.correlation_target_ms) / self.correlation_target_ms
            violation_severity = max(var_violation, correlation_violation)
        
        # Calculate degradation
        degradation_rate = self._calculate_performance_degradation_rate()
        
        # Identify bottlenecks
        bottlenecks = self._identify_performance_bottlenecks(performance_data)
        
        # Calculate system resilience
        resilience_score = self._calculate_system_resilience_score(performance_data)
        
        metrics = PerformanceRiskMetrics(
            timestamp=datetime.now(),
            total_processing_time_ms=total_time,
            var_calculation_time_ms=var_time,
            correlation_update_time_ms=correlation_time,
            decision_making_time_ms=decision_time,
            var_target_ms=self.var_target_ms,
            var_target_met=var_target_met,
            correlation_target_ms=self.correlation_target_ms,
            correlation_target_met=correlation_target_met,
            overall_target_met=overall_target_met,
            performance_violation_severity=violation_severity,
            violation_count=1 if not overall_target_met else 0,
            violation_duration_ms=max(0, total_time - (self.var_target_ms + self.correlation_target_ms)),
            performance_degradation_rate=degradation_rate,
            bottleneck_identification=bottlenecks,
            optimization_opportunities=self._identify_optimization_opportunities(performance_data),
            recovery_time_ms=performance_data.get('recovery_time_ms', 0.0),
            fallback_usage_rate=performance_data.get('fallback_usage_rate', 0.0),
            system_resilience_score=resilience_score
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def _calculate_performance_degradation_rate(self) -> float:
        """Calculate performance degradation rate"""
        if len(self.performance_history) < 10:
            return 0.0
        
        recent_metrics = list(self.performance_history)[-10:]
        performance_scores = [m.overall_target_met for m in recent_metrics]
        
        # Calculate trend
        if len(performance_scores) > 1:
            early_performance = np.mean(performance_scores[:5])
            late_performance = np.mean(performance_scores[5:])
            return max(0.0, early_performance - late_performance)
        
        return 0.0
    
    def _identify_performance_bottlenecks(self, performance_data: Dict[str, Any]) -> Dict[str, float]:
        """Identify performance bottlenecks"""
        bottlenecks = {}
        
        var_time = performance_data.get('var_calculation_time_ms', 0.0)
        correlation_time = performance_data.get('correlation_update_time_ms', 0.0)
        
        # Identify primary bottleneck
        if var_time > self.var_target_ms:
            bottlenecks['var_calculation'] = var_time / self.var_target_ms
        
        if correlation_time > self.correlation_target_ms:
            bottlenecks['correlation_update'] = correlation_time / self.correlation_target_ms
        
        return bottlenecks
    
    def _identify_optimization_opportunities(self, performance_data: Dict[str, Any]) -> List[str]:
        """Identify optimization opportunities"""
        opportunities = []
        
        var_time = performance_data.get('var_calculation_time_ms', 0.0)
        correlation_time = performance_data.get('correlation_update_time_ms', 0.0)
        
        if var_time > self.var_target_ms * 0.8:
            opportunities.append('var_calculation_optimization')
        
        if correlation_time > self.correlation_target_ms * 0.8:
            opportunities.append('correlation_update_optimization')
        
        if len(self.performance_history) > 5:
            recent_violations = sum(1 for m in list(self.performance_history)[-5:] if not m.overall_target_met)
            if recent_violations > 2:
                opportunities.append('overall_system_optimization')
        
        return opportunities
    
    def _calculate_system_resilience_score(self, performance_data: Dict[str, Any]) -> float:
        """Calculate system resilience score"""
        resilience_factors = []
        
        # Recovery capability
        recovery_time = performance_data.get('recovery_time_ms', 0.0)
        if recovery_time > 0:
            recovery_score = min(1.0, 1000.0 / recovery_time)  # Better recovery = higher score
            resilience_factors.append(recovery_score)
        
        # Fallback usage
        fallback_rate = performance_data.get('fallback_usage_rate', 0.0)
        fallback_score = 1.0 - fallback_rate  # Less fallback = higher score
        resilience_factors.append(fallback_score)
        
        # Historical stability
        if len(self.performance_history) >= 10:
            recent_stability = np.mean([m.overall_target_met for m in list(self.performance_history)[-10:]])
            resilience_factors.append(recent_stability)
        
        return np.mean(resilience_factors) if resilience_factors else 0.5
    
    # Placeholder methods for detailed attribution calculations
    def _calculate_position_sizing_var_contribution(self, superposition, context): return 0.0
    def _calculate_position_sizing_correlation_impact(self, superposition, context): return 0.0
    def _calculate_position_sizing_leverage_impact(self, superposition, context): return 0.0
    def _calculate_stop_target_risk_reduction(self, superposition, context): return 0.0
    def _calculate_stop_target_volatility_impact(self, superposition, context): return 0.0
    def _calculate_stop_target_drawdown_protection(self, superposition, context): return 0.0
    def _calculate_risk_monitor_alert_effectiveness(self, superposition, context): return 0.0
    def _calculate_risk_monitor_emergency_accuracy(self, superposition, context): return 0.0
    def _calculate_risk_monitor_correlation_detection(self, superposition, context): return 0.0
    def _calculate_portfolio_diversification_benefit(self, superposition, context): return 0.0
    def _calculate_portfolio_risk_parity_score(self, superposition, context): return 0.0
    def _calculate_portfolio_constraint_satisfaction(self, superposition, context): return 0.0
    def _calculate_position_stop_interaction(self, superposition, context): return 0.0
    def _calculate_stop_monitor_interaction(self, superposition, context): return 0.0
    def _calculate_monitor_optimizer_interaction(self, superposition, context): return 0.0
    def _calculate_total_risk_reduction(self, superposition, context): return 0.0
    def _calculate_sequential_synergy_score(self, superposition, context): return 0.0
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """Get comprehensive metrics summary"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = list(self.metrics_history)[-100:]
        
        return {
            'total_metrics_calculated': len(self.metrics_history),
            'avg_total_risk_score': np.mean([m.total_risk_score for m in recent_metrics]),
            'avg_quality_score': np.mean([m.quality_score for m in recent_metrics]),
            'avg_performance_score': np.mean([m.performance_score for m in recent_metrics]),
            'avg_coordination_score': np.mean([m.sequential_coordination_score for m in recent_metrics]),
            'performance_target_met_rate': np.mean([m.performance_target_met for m in recent_metrics]),
            'avg_var_utilization': np.mean([m.var_utilization for m in recent_metrics]),
            'avg_correlation_stability': np.mean([m.correlation_stability for m in recent_metrics]),
            'emergency_activation_rate': np.mean([m.emergency_protocols_active for m in recent_metrics]),
            'overall_confidence': np.mean([m.overall_confidence for m in recent_metrics])
        }


def create_superposition_risk_metrics_calculator(
    correlation_tracker: CorrelationTracker,
    var_calculator: VaRCalculator,
    event_bus: EventBus,
    config: Optional[Dict[str, Any]] = None
) -> SuperpositionRiskMetricsCalculator:
    """
    Factory function to create superposition risk metrics calculator
    
    Args:
        correlation_tracker: Correlation tracker
        var_calculator: VaR calculator
        event_bus: Event bus
        config: Optional configuration
        
    Returns:
        Configured SuperpositionRiskMetricsCalculator instance
    """
    default_config = {
        'var_target_ms': 5.0,
        'correlation_target_ms': 2.0,
        'risk_tolerance': 0.05
    }
    
    if config:
        default_config.update(config)
    
    return SuperpositionRiskMetricsCalculator(
        correlation_tracker=correlation_tracker,
        var_calculator=var_calculator,
        event_bus=event_bus,
        config=default_config
    )