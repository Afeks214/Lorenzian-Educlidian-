"""
Performance Benchmark Suite - Comprehensive performance testing
Tests all performance optimizations and measures improvements.
"""

import asyncio
import time
import statistics
from typing import Dict, List, Any, Optional
import json
from dataclasses import dataclass
import structlog

from .async_event_bus import AsyncEventBus, BatchingStrategy
from .memory_manager import MemoryManager
from .connection_pool import ConnectionManager, ConnectionType, ConnectionConfig
from .performance_optimizer import PerformanceOptimizer
from ..core.events import Event, EventType

logger = structlog.get_logger(__name__)


@dataclass
class BenchmarkResult:
    """Results from a benchmark test."""
    test_name: str
    duration_seconds: float
    throughput_ops_per_second: float
    latency_p50_ms: float
    latency_p95_ms: float
    latency_p99_ms: float
    memory_usage_mb: float
    cpu_usage_percent: float
    errors: int
    success_rate: float
    additional_metrics: Dict[str, Any]


class PerformanceBenchmark:
    """
    Comprehensive performance benchmark suite.
    
    Tests:
    - Event bus performance with different configurations
    - Memory management efficiency
    - Connection pooling performance
    - End-to-end system performance
    - Regression testing
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results: List[BenchmarkResult] = []
        
        # Benchmark parameters
        self.warmup_iterations = config.get('warmup_iterations', 100)
        self.test_iterations = config.get('test_iterations', 1000)
        self.test_duration = config.get('test_duration', 60)  # seconds
        
        logger.info("Performance benchmark initialized", config=config)
    
    async def run_all_benchmarks(self) -> Dict[str, Any]:
        """Run all benchmark tests."""
        start_time = time.time()
        
        try:
            # 1. Event Bus Benchmarks
            logger.info("Running event bus benchmarks")
            event_bus_results = await self._benchmark_event_bus()
            
            # 2. Memory Management Benchmarks
            logger.info("Running memory management benchmarks")
            memory_results = await self._benchmark_memory_management()
            
            # 3. Connection Pool Benchmarks
            logger.info("Running connection pool benchmarks")
            connection_results = await self._benchmark_connection_pools()
            
            # 4. End-to-End System Benchmarks
            logger.info("Running end-to-end system benchmarks")
            system_results = await self._benchmark_system_performance()
            
            # 5. Regression Testing
            logger.info("Running regression tests")
            regression_results = await self._benchmark_regression_testing()
            
            total_duration = time.time() - start_time
            
            # Compile final results
            final_results = {\n                'timestamp': start_time,\n                'total_duration_seconds': total_duration,\n                'event_bus_results': event_bus_results,\n                'memory_results': memory_results,\n                'connection_results': connection_results,\n                'system_results': system_results,\n                'regression_results': regression_results,\n                'summary': self._generate_summary()\n            }\n            \n            logger.info(\"All benchmarks completed\", \n                       total_duration=total_duration,\n                       total_tests=len(self.results))\n            \n            return final_results\n            \n        except Exception as e:\n            logger.error(\"Benchmark suite failed\", error=str(e))\n            raise\n    \n    async def _benchmark_event_bus(self) -> Dict[str, Any]:\n        \"\"\"Benchmark event bus performance with different configurations.\"\"\"\n        results = {}\n        \n        # Test different batching strategies\n        strategies = [\n            BatchingStrategy.TIME_BASED,\n            BatchingStrategy.SIZE_BASED,\n            BatchingStrategy.HYBRID,\n            BatchingStrategy.PRIORITY_BASED\n        ]\n        \n        for strategy in strategies:\n            logger.info(f\"Testing event bus with {strategy.value} strategy\")\n            \n            # Create event bus with strategy\n            event_bus = AsyncEventBus(\n                max_workers=8,\n                batch_window_ms=10,\n                max_batch_size=100,\n                strategy=strategy,\n                enable_monitoring=True\n            )\n            \n            await event_bus.start()\n            \n            try:\n                # Benchmark event processing\n                result = await self._benchmark_event_processing(event_bus, strategy.value)\n                results[strategy.value] = result\n                \n            finally:\n                await event_bus.stop()\n        \n        return results\n    \n    async def _benchmark_event_processing(self, event_bus: AsyncEventBus, strategy_name: str) -> BenchmarkResult:\n        \"\"\"Benchmark event processing performance.\"\"\"\n        \n        # Setup event subscribers\n        processed_events = []\n        \n        def event_handler(event: Event):\n            processed_events.append(time.time())\n        \n        # Subscribe to different event types\n        event_types = [\n            EventType.NEW_TICK,\n            EventType.NEW_5MIN_BAR,\n            EventType.TRADE_QUALIFIED,\n            EventType.RISK_UPDATE\n        ]\n        \n        for event_type in event_types:\n            event_bus.subscribe(event_type, event_handler)\n        \n        # Warmup\n        for _ in range(self.warmup_iterations):\n            event = event_bus.create_event(\n                EventType.NEW_TICK,\n                {'price': 100.0, 'volume': 1000},\n                'benchmark'\n            )\n            event_bus.publish(event)\n        \n        # Wait for warmup to complete\n        await asyncio.sleep(0.1)\n        \n        # Clear processed events\n        processed_events.clear()\n        \n        # Benchmark test\n        start_time = time.time()\n        start_memory = self._get_memory_usage()\n        \n        # Publish events at high frequency\n        latencies = []\n        \n        for i in range(self.test_iterations):\n            event_start = time.time()\n            \n            event = event_bus.create_event(\n                event_types[i % len(event_types)],\n                {'iteration': i, 'data': f'test_data_{i}'},\n                'benchmark'\n            )\n            \n            event_bus.publish(event)\n            \n            # Measure publish latency\n            latencies.append((time.time() - event_start) * 1000)\n            \n            # Small delay to simulate realistic load\n            if i % 100 == 0:\n                await asyncio.sleep(0.001)\n        \n        # Wait for all events to be processed\n        await asyncio.sleep(0.5)\n        \n        end_time = time.time()\n        end_memory = self._get_memory_usage()\n        \n        # Calculate metrics\n        duration = end_time - start_time\n        throughput = len(processed_events) / duration\n        \n        # Get event bus metrics\n        bus_metrics = event_bus.get_metrics()\n        \n        result = BenchmarkResult(\n            test_name=f'event_bus_{strategy_name}',\n            duration_seconds=duration,\n            throughput_ops_per_second=throughput,\n            latency_p50_ms=statistics.median(latencies),\n            latency_p95_ms=statistics.quantiles(latencies, n=20)[18],  # 95th percentile\n            latency_p99_ms=statistics.quantiles(latencies, n=100)[98],  # 99th percentile\n            memory_usage_mb=end_memory - start_memory,\n            cpu_usage_percent=self._get_cpu_usage(),\n            errors=bus_metrics.get('callback_errors', 0),\n            success_rate=len(processed_events) / self.test_iterations,\n            additional_metrics={\n                'events_processed': bus_metrics.get('events_processed', 0),\n                'batches_processed': bus_metrics.get('batches_processed', 0),\n                'avg_batch_size': bus_metrics.get('avg_batch_size', 0),\n                'batch_efficiency': bus_metrics.get('batch_efficiency', 0),\n                'queue_depth': bus_metrics.get('queue_depth', 0)\n            }\n        )\n        \n        self.results.append(result)\n        return result\n    \n    async def _benchmark_memory_management(self) -> Dict[str, Any]:\n        \"\"\"Benchmark memory management performance.\"\"\"\n        results = {}\n        \n        # Test with different memory configurations\n        configs = [\n            {'enable_pools': True, 'enable_tracking': True},\n            {'enable_pools': True, 'enable_tracking': False},\n            {'enable_pools': False, 'enable_tracking': True},\n            {'enable_pools': False, 'enable_tracking': False}\n        ]\n        \n        for i, config in enumerate(configs):\n            config_name = f\"config_{i}_pools_{config['enable_pools']}_tracking_{config['enable_tracking']}\"\n            logger.info(f\"Testing memory management with {config_name}\")\n            \n            memory_manager = MemoryManager(\n                enable_pools=config['enable_pools'],\n                enable_tracking=config['enable_tracking'],\n                cleanup_interval_seconds=30\n            )\n            \n            try:\n                result = await self._benchmark_memory_operations(memory_manager, config_name)\n                results[config_name] = result\n                \n            finally:\n                memory_manager.stop()\n        \n        return results\n    \n    async def _benchmark_memory_operations(self, memory_manager: MemoryManager, config_name: str) -> BenchmarkResult:\n        \"\"\"Benchmark memory operations performance.\"\"\"\n        \n        import torch\n        \n        # Setup\n        tensors = []\n        latencies = []\n        \n        start_time = time.time()\n        start_memory = self._get_memory_usage()\n        \n        # Test tensor allocation and deallocation\n        for i in range(self.test_iterations):\n            op_start = time.time()\n            \n            # Create tensor\n            shape = torch.Size([32, 128])  # Typical model tensor size\n            tensor = memory_manager.get_tensor(shape, torch.float32, torch.device('cpu'))\n            \n            # Simulate some operations\n            tensor.fill_(i)\n            result = tensor.sum().item()\n            \n            # Return to pool\n            memory_manager.return_tensor(tensor)\n            \n            latencies.append((time.time() - op_start) * 1000)\n            \n            # Periodic cleanup test\n            if i % 100 == 0:\n                memory_manager.cleanup_memory()\n        \n        end_time = time.time()\n        end_memory = self._get_memory_usage()\n        \n        # Get memory statistics\n        memory_stats = memory_manager.get_memory_stats()\n        \n        duration = end_time - start_time\n        throughput = self.test_iterations / duration\n        \n        result = BenchmarkResult(\n            test_name=f'memory_{config_name}',\n            duration_seconds=duration,\n            throughput_ops_per_second=throughput,\n            latency_p50_ms=statistics.median(latencies),\n            latency_p95_ms=statistics.quantiles(latencies, n=20)[18],\n            latency_p99_ms=statistics.quantiles(latencies, n=100)[98],\n            memory_usage_mb=end_memory - start_memory,\n            cpu_usage_percent=self._get_cpu_usage(),\n            errors=0,\n            success_rate=1.0,\n            additional_metrics={\n                'total_allocated_mb': memory_stats.total_allocated_mb,\n                'active_tensors': memory_stats.active_tensors,\n                'leak_candidates': memory_stats.leak_candidates,\n                'pool_usage': memory_stats.pool_usage_mb\n            }\n        )\n        \n        self.results.append(result)\n        return result\n    \n    async def _benchmark_connection_pools(self) -> Dict[str, Any]:\n        \"\"\"Benchmark connection pool performance.\"\"\"\n        results = {}\n        \n        # Test different connection types\n        connection_types = [\n            {'type': ConnectionType.REDIS, 'name': 'redis'},\n            {'type': ConnectionType.DATABASE, 'name': 'database'},\n            {'type': ConnectionType.HTTP, 'name': 'http'}\n        ]\n        \n        for conn_config in connection_types:\n            logger.info(f\"Testing {conn_config['name']} connection pool\")\n            \n            connection_manager = ConnectionManager()\n            \n            try:\n                result = await self._benchmark_connection_operations(connection_manager, conn_config)\n                results[conn_config['name']] = result\n                \n            finally:\n                await connection_manager.stop_all()\n        \n        return results\n    \n    async def _benchmark_connection_operations(self, connection_manager: ConnectionManager, conn_config: Dict[str, Any]) -> BenchmarkResult:\n        \"\"\"Benchmark connection operations performance.\"\"\"\n        \n        # Create connection pool\n        config = ConnectionConfig(\n            connection_type=conn_config['type'],\n            host='localhost',\n            port=6379 if conn_config['type'] == ConnectionType.REDIS else 5432,\n            max_connections=20,\n            min_connections=5\n        )\n        \n        pool = await connection_manager.create_pool(config)\n        \n        # Warmup\n        for _ in range(self.warmup_iterations // 10):\n            try:\n                if conn_config['type'] == ConnectionType.REDIS:\n                    await pool.execute('ping')\n                elif conn_config['type'] == ConnectionType.DATABASE:\n                    await pool.execute('fetchrow', 'SELECT 1')\n                else:\n                    await pool.execute('get', 'http://localhost:8080/health')\n            except (ConnectionError, TimeoutError, OSError, AttributeError) as e:\n                pass  # Mock connections may fail\n        \n        # Benchmark test\n        start_time = time.time()\n        start_memory = self._get_memory_usage()\n        \n        latencies = []\n        errors = 0\n        \n        for i in range(self.test_iterations):\n            op_start = time.time()\n            \n            try:\n                if conn_config['type'] == ConnectionType.REDIS:\n                    await pool.execute_with_caching('set', f'key_{i}', f'value_{i}', cache_key=f'cache_{i}')\n                    await pool.execute_with_caching('get', f'key_{i}', cache_key=f'cache_{i}')\n                elif conn_config['type'] == ConnectionType.DATABASE:\n                    await pool.execute_with_caching('fetchrow', 'SELECT $1', i, cache_key=f'query_{i}')\n                else:\n                    await pool.execute_with_caching('get', f'http://localhost:8080/api/data/{i}', cache_key=f'http_{i}')\n                \n            except Exception as e:\n                errors += 1\n            \n            latencies.append((time.time() - op_start) * 1000)\n        \n        end_time = time.time()\n        end_memory = self._get_memory_usage()\n        \n        # Get pool statistics\n        pool_stats = pool.get_stats()\n        \n        duration = end_time - start_time\n        throughput = (self.test_iterations - errors) / duration\n        \n        result = BenchmarkResult(\n            test_name=f'connection_{conn_config[\"name\"]}',\n            duration_seconds=duration,\n            throughput_ops_per_second=throughput,\n            latency_p50_ms=statistics.median(latencies),\n            latency_p95_ms=statistics.quantiles(latencies, n=20)[18],\n            latency_p99_ms=statistics.quantiles(latencies, n=100)[98],\n            memory_usage_mb=end_memory - start_memory,\n            cpu_usage_percent=self._get_cpu_usage(),\n            errors=errors,\n            success_rate=(self.test_iterations - errors) / self.test_iterations,\n            additional_metrics={\n                'total_connections': pool_stats.total_connections,\n                'active_connections': pool_stats.active_connections,\n                'cache_hit_rate': pool_stats.cache_hit_rate,\n                'avg_response_time_ms': pool_stats.avg_response_time_ms\n            }\n        )\n        \n        self.results.append(result)\n        return result\n    \n    async def _benchmark_system_performance(self) -> Dict[str, Any]:\n        \"\"\"Benchmark end-to-end system performance.\"\"\"\n        results = {}\n        \n        # Test complete system with all optimizations\n        optimizer_config = {\n            'event_bus': {\n                'max_workers': 8,\n                'batch_window_ms': 10,\n                'max_batch_size': 100\n            },\n            'memory': {\n                'enable_pools': True,\n                'enable_tracking': True,\n                'cleanup_interval': 30\n            },\n            'redis': {\n                'enabled': True,\n                'host': 'localhost',\n                'port': 6379\n            },\n            'auto_optimize': True,\n            'optimization_interval': 60\n        }\n        \n        optimizer = PerformanceOptimizer(optimizer_config)\n        \n        try:\n            await optimizer.start()\n            \n            # Run system benchmark\n            result = await self._benchmark_integrated_system(optimizer)\n            results['integrated_system'] = result\n            \n        finally:\n            await optimizer.stop()\n        \n        return results\n    \n    async def _benchmark_integrated_system(self, optimizer: PerformanceOptimizer) -> BenchmarkResult:\n        \"\"\"Benchmark integrated system performance.\"\"\"\n        \n        # Simulate realistic system load\n        start_time = time.time()\n        start_memory = self._get_memory_usage()\n        \n        latencies = []\n        errors = 0\n        \n        # Mixed workload simulation\n        for i in range(self.test_iterations):\n            op_start = time.time()\n            \n            try:\n                # Simulate event processing\n                if optimizer._event_bus:\n                    event = optimizer._event_bus.create_event(\n                        EventType.NEW_TICK,\n                        {'price': 100.0 + i * 0.01, 'volume': 1000},\n                        'benchmark'\n                    )\n                    optimizer._event_bus.publish(event)\n                \n                # Simulate memory operations\n                if optimizer._memory_manager:\n                    import torch\n                    tensor = optimizer._memory_manager.get_tensor(\n                        torch.Size([16, 64]), \n                        torch.float32, \n                        torch.device('cpu')\n                    )\n                    tensor.fill_(i)\n                    optimizer._memory_manager.return_tensor(tensor)\n                \n                # Simulate connection operations\n                if optimizer._connection_manager:\n                    pool = await optimizer._connection_manager.get_pool(\n                        ConnectionType.REDIS, 'localhost', 6379\n                    )\n                    if pool:\n                        await pool.execute('ping')\n                \n            except Exception as e:\n                errors += 1\n            \n            latencies.append((time.time() - op_start) * 1000)\n            \n            # Periodic system checks\n            if i % 100 == 0:\n                await asyncio.sleep(0.001)  # Allow system to process\n        \n        end_time = time.time()\n        end_memory = self._get_memory_usage()\n        \n        # Get system metrics\n        dashboard = optimizer.get_performance_dashboard()\n        current_metrics = dashboard.get('current_metrics', {})\n        \n        duration = end_time - start_time\n        throughput = (self.test_iterations - errors) / duration\n        \n        result = BenchmarkResult(\n            test_name='integrated_system',\n            duration_seconds=duration,\n            throughput_ops_per_second=throughput,\n            latency_p50_ms=statistics.median(latencies),\n            latency_p95_ms=statistics.quantiles(latencies, n=20)[18],\n            latency_p99_ms=statistics.quantiles(latencies, n=100)[98],\n            memory_usage_mb=end_memory - start_memory,\n            cpu_usage_percent=self._get_cpu_usage(),\n            errors=errors,\n            success_rate=(self.test_iterations - errors) / self.test_iterations,\n            additional_metrics={\n                'performance_score': current_metrics.get('performance_score', 0),\n                'event_throughput': current_metrics.get('event_throughput_eps', 0),\n                'memory_leaks': current_metrics.get('memory_leaks', 0),\n                'cache_hit_rate': current_metrics.get('cache_hit_rate', 0)\n            }\n        )\n        \n        self.results.append(result)\n        return result\n    \n    async def _benchmark_regression_testing(self) -> Dict[str, Any]:\n        \"\"\"Run regression tests to ensure performance hasn't degraded.\"\"\"\n        results = {}\n        \n        # Load baseline metrics if available\n        baseline_metrics = self._load_baseline_metrics()\n        \n        if baseline_metrics:\n            # Compare current results with baseline\n            regression_analysis = self._analyze_regression(baseline_metrics)\n            results['regression_analysis'] = regression_analysis\n        else:\n            results['regression_analysis'] = {'status': 'no_baseline_available'}\n        \n        # Save current results as new baseline\n        self._save_baseline_metrics()\n        \n        return results\n    \n    def _load_baseline_metrics(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Load baseline metrics from file.\"\"\"\n        try:\n            with open('performance_baseline.json', 'r') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            return None\n    \n    def _save_baseline_metrics(self):\n        \"\"\"Save current results as baseline metrics.\"\"\"\n        baseline_data = {\n            'timestamp': time.time(),\n            'results': [result.__dict__ for result in self.results]\n        }\n        \n        with open('performance_baseline.json', 'w') as f:\n            json.dump(baseline_data, f, indent=2)\n    \n    def _analyze_regression(self, baseline_metrics: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Analyze performance regression compared to baseline.\"\"\"\n        regression_analysis = {\n            'regressions': [],\n            'improvements': [],\n            'stable_metrics': []\n        }\n        \n        # Compare results with baseline\n        baseline_results = {r['test_name']: r for r in baseline_metrics.get('results', [])}\n        \n        for result in self.results:\n            if result.test_name in baseline_results:\n                baseline = baseline_results[result.test_name]\n                \n                # Check for significant changes (>10% change)\n                throughput_change = (result.throughput_ops_per_second - baseline['throughput_ops_per_second']) / baseline['throughput_ops_per_second']\n                latency_change = (result.latency_p99_ms - baseline['latency_p99_ms']) / baseline['latency_p99_ms']\n                \n                if throughput_change < -0.1 or latency_change > 0.1:\n                    regression_analysis['regressions'].append({\n                        'test_name': result.test_name,\n                        'throughput_change': throughput_change,\n                        'latency_change': latency_change\n                    })\n                elif throughput_change > 0.1 or latency_change < -0.1:\n                    regression_analysis['improvements'].append({\n                        'test_name': result.test_name,\n                        'throughput_change': throughput_change,\n                        'latency_change': latency_change\n                    })\n                else:\n                    regression_analysis['stable_metrics'].append(result.test_name)\n        \n        return regression_analysis\n    \n    def _generate_summary(self) -> Dict[str, Any]:\n        \"\"\"Generate benchmark summary.\"\"\"\n        if not self.results:\n            return {'status': 'no_results'}\n        \n        # Calculate overall statistics\n        total_throughput = sum(r.throughput_ops_per_second for r in self.results)\n        avg_latency = statistics.mean(r.latency_p99_ms for r in self.results)\n        total_errors = sum(r.errors for r in self.results)\n        avg_success_rate = statistics.mean(r.success_rate for r in self.results)\n        \n        # Performance targets\n        targets = {\n            'target_latency_ms': 5.0,\n            'target_throughput_ops': 10000,\n            'target_success_rate': 0.99\n        }\n        \n        # Calculate performance score\n        latency_score = min(1.0, targets['target_latency_ms'] / avg_latency)\n        throughput_score = min(1.0, total_throughput / targets['target_throughput_ops'])\n        success_score = avg_success_rate / targets['target_success_rate']\n        \n        overall_score = (latency_score + throughput_score + success_score) / 3.0\n        \n        return {\n            'total_tests': len(self.results),\n            'total_throughput_ops_per_second': total_throughput,\n            'average_latency_p99_ms': avg_latency,\n            'total_errors': total_errors,\n            'average_success_rate': avg_success_rate,\n            'performance_score': overall_score * 10,  # Scale to 0-10\n            'targets_met': {\n                'latency': avg_latency <= targets['target_latency_ms'],\n                'throughput': total_throughput >= targets['target_throughput_ops'],\n                'success_rate': avg_success_rate >= targets['target_success_rate']\n            },\n            'best_performing_test': max(self.results, key=lambda r: r.throughput_ops_per_second).test_name,\n            'worst_performing_test': min(self.results, key=lambda r: r.throughput_ops_per_second).test_name\n        }\n    \n    def _get_memory_usage(self) -> float:\n        \"\"\"Get current memory usage in MB.\"\"\"\n        try:\n            import psutil\n            process = psutil.Process()\n            return process.memory_info().rss / 1024 / 1024\n        except ImportError:\n            return 0.0\n    \n    def _get_cpu_usage(self) -> float:\n        \"\"\"Get current CPU usage percentage.\"\"\"\n        try:\n            import psutil\n            return psutil.cpu_percent(interval=0.1)\n        except ImportError:\n            return 0.0\n    \n    def export_results(self, filepath: str):\n        \"\"\"Export benchmark results to file.\"\"\"\n        results_data = {\n            'timestamp': time.time(),\n            'config': self.config,\n            'results': [result.__dict__ for result in self.results],\n            'summary': self._generate_summary()\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(results_data, f, indent=2)\n        \n        logger.info(\"Benchmark results exported\", filepath=filepath)\n\n\nasync def run_performance_benchmark(config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    \"\"\"Run comprehensive performance benchmark.\"\"\"\n    if config is None:\n        config = {\n            'warmup_iterations': 100,\n            'test_iterations': 1000,\n            'test_duration': 60\n        }\n    \n    benchmark = PerformanceBenchmark(config)\n    results = await benchmark.run_all_benchmarks()\n    \n    # Export results\n    benchmark.export_results('performance_benchmark_results.json')\n    \n    return results\n\n\nif __name__ == \"__main__\":\n    # Run benchmark when script is executed directly\n    async def main():\n        config = {\n            'warmup_iterations': 50,\n            'test_iterations': 500,\n            'test_duration': 30\n        }\n        \n        results = await run_performance_benchmark(config)\n        \n        # Print summary\n        summary = results['summary']\n        print(\"\\n\" + \"=\"*80)\n        print(\"PERFORMANCE BENCHMARK RESULTS\")\n        print(\"=\"*80)\n        \n        print(f\"Total Tests: {summary['total_tests']}\")\n        print(f\"Overall Performance Score: {summary['performance_score']:.1f}/10\")\n        print(f\"Total Throughput: {summary['total_throughput_ops_per_second']:.1f} ops/sec\")\n        print(f\"Average Latency (P99): {summary['average_latency_p99_ms']:.2f}ms\")\n        print(f\"Success Rate: {summary['average_success_rate']:.1%}\")\n        \n        print(\"\\nTargets Met:\")\n        for target, met in summary['targets_met'].items():\n            status = \"✅ PASS\" if met else \"❌ FAIL\"\n            print(f\"  {target}: {status}\")\n        \n        print(f\"\\nBest Performing Test: {summary['best_performing_test']}\")\n        print(f\"Worst Performing Test: {summary['worst_performing_test']}\")\n        print(\"\\n\" + \"=\"*80)\n    \n    asyncio.run(main())