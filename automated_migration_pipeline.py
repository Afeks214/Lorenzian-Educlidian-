#!/usr/bin/env python3
"""
Automated Migration Pipeline for GrandModel

This pipeline automates the promotion of validated components from development 
to staging to production with comprehensive validation at each stage.
"""

import os
import sys
import json
import yaml
import shutil
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import hashlib
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AutomatedMigrationPipeline:
    """Automated pipeline for promoting components through environments."""
    
    def __init__(self, dev_path: str = "/home/QuantNova/GrandModel",
                 staging_path: str = "/home/QuantNova/GrandModel-Staging", 
                 prod_path: str = "/home/QuantNova/GrandModel-Production"):
        self.dev_path = Path(dev_path)
        self.staging_path = Path(staging_path)
        self.prod_path = Path(prod_path)
        
        # Migration criteria
        self.promotion_criteria = {
            "code_quality": {
                "unit_test_coverage": 0.95,
                "integration_test_pass": True,
                "security_scan_pass": True,
                "performance_benchmark_pass": True
            },
            "business_validation": {
                "backtesting_sharpe_ratio": 1.5,
                "max_drawdown": 0.10,
                "risk_metrics_within_limits": True,
                "latency_targets_met": True
            },
            "deployment_validation": {
                "docker_build_success": True,
                "k8s_manifest_valid": True,
                "health_check_pass": True,
                "smoke_test_pass": True
            }
        }
        
        # Performance targets
        self.performance_targets = {
            "build_time_seconds": 180,  # 3 minutes
            "deployment_time_seconds": 120,  # 2 minutes
            "rollback_time_seconds": 30,
            "api_response_time_ms": 10,
            "system_availability": 0.999
        }
    
    def analyze_component_readiness(self, component_path: Path) -> Dict:
        """Analyze if a component is ready for promotion."""
        logger.info(f"üîç Analyzing component readiness: {component_path}")
        
        readiness_report = {
            "component": str(component_path),
            "analysis_time": datetime.now().isoformat(),
            "ready_for_promotion": False,
            "quality_score": 0.0,
            "checks": {},
            "issues": [],
            "recommendations": []
        }
        
        try:
            # Code quality checks
            readiness_report["checks"]["code_quality"] = self._check_code_quality(component_path)
            
            # Security checks
            readiness_report["checks"]["security"] = self._check_security(component_path)
            
            # Performance checks
            readiness_report["checks"]["performance"] = self._check_performance(component_path)
            
            # Business validation checks
            readiness_report["checks"]["business_validation"] = self._check_business_metrics(component_path)
            
            # Calculate overall quality score
            readiness_report["quality_score"] = self._calculate_quality_score(readiness_report["checks"])
            
            # Determine if ready for promotion
            readiness_report["ready_for_promotion"] = (
                readiness_report["quality_score"] >= 0.90 and
                all(check.get("passed", False) for check in readiness_report["checks"].values())
            )
            
            if not readiness_report["ready_for_promotion"]:
                readiness_report["issues"] = self._identify_issues(readiness_report["checks"])
                readiness_report["recommendations"] = self._generate_recommendations(readiness_report["issues"])
            
        except Exception as e:
            logger.error(f"‚ùå Error analyzing component {component_path}: {e}")
            readiness_report["issues"].append(f"Analysis failed: {e}")
        
        return readiness_report
    
    def _check_code_quality(self, component_path: Path) -> Dict:
        """Check code quality metrics."""
        logger.info("üß™ Checking code quality...")
        
        quality_check = {
            "passed": False,
            "coverage": 0.0,
            "test_results": {},
            "linting_results": {},
            "complexity_score": 0
        }
        
        try:
            # Run tests if test files exist
            test_files = list(component_path.rglob("test_*.py"))
            if test_files:
                # Run pytest with coverage
                cmd = f"cd {component_path} && python -m pytest --cov=. --cov-report=json --json-report --json-report-file=test_results.json"
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
                
                # Parse coverage results
                coverage_file = component_path / "coverage.json"
                if coverage_file.exists():
                    with open(coverage_file) as f:
                        coverage_data = json.load(f)
                        quality_check["coverage"] = coverage_data.get("totals", {}).get("percent_covered", 0) / 100
                
                # Parse test results
                test_results_file = component_path / "test_results.json"
                if test_results_file.exists():
                    with open(test_results_file) as f:
                        test_data = json.load(f)
                        quality_check["test_results"] = {
                            "passed": test_data.get("summary", {}).get("passed", 0),
                            "failed": test_data.get("summary", {}).get("failed", 0),
                            "total": test_data.get("summary", {}).get("total", 0)
                        }
            
            # Check if meets criteria
            quality_check["passed"] = (
                quality_check["coverage"] >= self.promotion_criteria["code_quality"]["unit_test_coverage"] and
                quality_check["test_results"].get("failed", 1) == 0
            )
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Code quality check failed: {e}")
        
        return quality_check
    
    def _check_security(self, component_path: Path) -> Dict:
        """Check security vulnerabilities."""
        logger.info("üîí Checking security...")
        
        security_check = {
            "passed": False,
            "vulnerabilities": [],
            "security_score": 0.0,
            "recommendations": []
        }
        
        try:
            # Check for common security issues
            security_issues = []
            
            for py_file in component_path.rglob("*.py"):
                with open(py_file, 'r') as f:
                    content = f.read()
                    
                    # Check for hardcoded secrets
                    if any(pattern in content.lower() for pattern in ['password=', 'api_key=', 'secret=', 'token=']):
                        security_issues.append(f"Potential hardcoded secret in {py_file}")
                    
                    # Check for dangerous functions
                    if any(func in content for func in ['eval(', 'exec(', 'pickle.loads']):
                        security_issues.append(f"Dangerous function usage in {py_file}")
                    
                    # Check for SQL injection risks
                    if 'execute(' in content and '%s' in content:
                        security_issues.append(f"Potential SQL injection risk in {py_file}")
            
            security_check["vulnerabilities"] = security_issues
            security_check["passed"] = len(security_issues) == 0
            security_check["security_score"] = max(0, 1.0 - (len(security_issues) * 0.2))
            
            if security_issues:
                security_check["recommendations"] = [
                    "Use environment variables for secrets",
                    "Implement input validation",
                    "Use parameterized queries",
                    "Review code for security best practices"
                ]
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Security check failed: {e}")
        
        return security_check
    
    def _check_performance(self, component_path: Path) -> Dict:
        """Check performance metrics."""
        logger.info("‚ö° Checking performance...")
        
        performance_check = {
            "passed": False,
            "latency_ms": 0,
            "throughput": 0,
            "memory_usage_mb": 0,
            "optimization_score": 0.0
        }
        
        try:
            # Look for performance benchmarks
            benchmark_files = list(component_path.rglob("*benchmark*.py"))
            if benchmark_files:
                # Run performance tests
                for benchmark_file in benchmark_files:
                    cmd = f"cd {component_path} && python {benchmark_file.name}"
                    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
                    
                    # Parse performance results (simplified)
                    if "latency" in result.stdout.lower():
                        # Extract latency information
                        lines = result.stdout.split('\n')
                        for line in lines:
                            if 'latency' in line.lower() and 'ms' in line.lower():
                                try:
                                    latency = float(''.join(filter(str.isdigit, line.split('ms')[0])))
                                    performance_check["latency_ms"] = latency
                                except:
                                    pass
            
            # Check JIT optimization usage
            jit_optimized = False
            for py_file in component_path.rglob("*.py"):
                with open(py_file, 'r') as f:
                    if '@jit' in f.read():
                        jit_optimized = True
                        break
            
            performance_check["optimization_score"] = 0.8 if jit_optimized else 0.4
            performance_check["passed"] = (
                performance_check["latency_ms"] <= self.performance_targets["api_response_time_ms"] or
                performance_check["latency_ms"] == 0  # No latency data available
            )
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Performance check failed: {e}")
        
        return performance_check
    
    def _check_business_metrics(self, component_path: Path) -> Dict:
        """Check business validation metrics."""
        logger.info("üìä Checking business metrics...")
        
        business_check = {
            "passed": False,
            "sharpe_ratio": 0.0,
            "max_drawdown": 1.0,
            "risk_score": 0.0,
            "trading_performance": {}
        }
        
        try:
            # Look for backtest results
            result_files = list(component_path.rglob("*results*.json"))
            for result_file in result_files:
                try:
                    with open(result_file) as f:
                        results = json.load(f)
                        
                        # Extract business metrics
                        if "sharpe_ratio" in results:
                            business_check["sharpe_ratio"] = results["sharpe_ratio"]
                        
                        if "max_drawdown" in results:
                            business_check["max_drawdown"] = abs(results["max_drawdown"])
                        
                        if "risk_metrics" in results:
                            business_check["risk_score"] = results["risk_metrics"].get("score", 0.0)
                        
                        business_check["trading_performance"] = results
                        break
                
                except Exception as e:
                    continue
            
            # Check if meets business criteria
            business_check["passed"] = (
                business_check["sharpe_ratio"] >= self.promotion_criteria["business_validation"]["backtesting_sharpe_ratio"] and
                business_check["max_drawdown"] <= self.promotion_criteria["business_validation"]["max_drawdown"]
            )
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Business metrics check failed: {e}")
        
        return business_check
    
    def _calculate_quality_score(self, checks: Dict) -> float:
        """Calculate overall quality score."""
        scores = []
        weights = {
            "code_quality": 0.3,
            "security": 0.3,
            "performance": 0.2,
            "business_validation": 0.2
        }
        
        for check_name, check_data in checks.items():
            if check_name in weights:
                if check_data.get("passed", False):
                    score = 1.0
                else:
                    # Partial score based on specific metrics
                    if check_name == "code_quality":
                        score = check_data.get("coverage", 0.0)
                    elif check_name == "security":
                        score = check_data.get("security_score", 0.0)
                    elif check_name == "performance":
                        score = check_data.get("optimization_score", 0.0)
                    else:
                        score = 0.5 if check_data.get("sharpe_ratio", 0) > 0 else 0.0
                
                scores.append(score * weights[check_name])
        
        return sum(scores)
    
    def _identify_issues(self, checks: Dict) -> List[str]:
        """Identify issues preventing promotion."""
        issues = []
        
        for check_name, check_data in checks.items():
            if not check_data.get("passed", False):
                if check_name == "code_quality":
                    if check_data.get("coverage", 0) < 0.95:
                        issues.append(f"Code coverage too low: {check_data.get('coverage', 0):.1%}")
                    if check_data.get("test_results", {}).get("failed", 0) > 0:
                        issues.append("Some tests are failing")
                
                elif check_name == "security":
                    if check_data.get("vulnerabilities"):
                        issues.append(f"Security vulnerabilities found: {len(check_data['vulnerabilities'])}")
                
                elif check_name == "performance":
                    if check_data.get("latency_ms", 0) > 10:
                        issues.append(f"Latency too high: {check_data['latency_ms']}ms")
                
                elif check_name == "business_validation":
                    if check_data.get("sharpe_ratio", 0) < 1.5:
                        issues.append(f"Sharpe ratio too low: {check_data['sharpe_ratio']:.2f}")
                    if check_data.get("max_drawdown", 1) > 0.10:
                        issues.append(f"Max drawdown too high: {check_data['max_drawdown']:.1%}")
        
        return issues
    
    def _generate_recommendations(self, issues: List[str]) -> List[str]:
        """Generate recommendations to fix issues."""
        recommendations = []
        
        for issue in issues:
            if "coverage" in issue.lower():
                recommendations.append("Add more unit tests to increase coverage")
            elif "test" in issue.lower() and "failing" in issue.lower():
                recommendations.append("Fix failing tests before promotion")
            elif "security" in issue.lower():
                recommendations.append("Address security vulnerabilities")
            elif "latency" in issue.lower():
                recommendations.append("Optimize performance or enable JIT compilation")
            elif "sharpe" in issue.lower():
                recommendations.append("Improve trading strategy performance")
            elif "drawdown" in issue.lower():
                recommendations.append("Implement better risk management")
        
        return recommendations
    
    def promote_component(self, component_path: Path, target_env: str = "staging") -> Dict:
        """Promote a component to the target environment."""
        logger.info(f"üöÄ Promoting component {component_path} to {target_env}")
        
        promotion_report = {
            "component": str(component_path),
            "target_environment": target_env,
            "promotion_time": datetime.now().isoformat(),
            "success": False,
            "steps_completed": [],
            "errors": []
        }
        
        try:
            # Step 1: Analyze readiness
            readiness = self.analyze_component_readiness(component_path)
            promotion_report["readiness_analysis"] = readiness
            
            if not readiness["ready_for_promotion"]:
                promotion_report["errors"].append("Component not ready for promotion")
                promotion_report["errors"].extend(readiness["issues"])
                return promotion_report
            
            promotion_report["steps_completed"].append("readiness_analysis")
            
            # Step 2: Determine target path
            if target_env == "staging":
                target_path = self.staging_path
            elif target_env == "production":
                target_path = self.prod_path
            else:
                raise ValueError(f"Unknown target environment: {target_env}")
            
            # Step 3: Copy component
            relative_path = component_path.relative_to(self.dev_path)
            dest_path = target_path / relative_path
            dest_path.parent.mkdir(parents=True, exist_ok=True)
            
            if component_path.is_dir():
                shutil.copytree(component_path, dest_path, dirs_exist_ok=True)
            else:
                shutil.copy2(component_path, dest_path)
            
            promotion_report["steps_completed"].append("component_copy")
            
            # Step 4: Optimize for target environment
            self._optimize_for_environment(dest_path, target_env)
            promotion_report["steps_completed"].append("optimization")
            
            # Step 5: Validate deployment
            validation_result = self._validate_deployment(dest_path, target_env)
            promotion_report["deployment_validation"] = validation_result
            
            if validation_result["passed"]:
                promotion_report["steps_completed"].append("deployment_validation")
                promotion_report["success"] = True
            else:
                promotion_report["errors"].extend(validation_result["errors"])
        
        except Exception as e:
            logger.error(f"‚ùå Error promoting component: {e}")
            promotion_report["errors"].append(str(e))
        
        return promotion_report
    
    def _optimize_for_environment(self, component_path: Path, environment: str) -> None:
        """Optimize component for specific environment."""
        logger.info(f"üîß Optimizing for {environment} environment")
        
        if environment == "production":
            # Production optimizations
            for py_file in component_path.rglob("*.py"):
                try:
                    with open(py_file, 'r') as f:
                        content = f.read()
                    
                    # Remove debug statements
                    lines = content.split('\n')
                    optimized_lines = []
                    
                    for line in lines:
                        if not any(keyword in line.lower() for keyword in ['print("debug', '# debug', 'logger.debug']):
                            optimized_lines.append(line)
                    
                    with open(py_file, 'w') as f:
                        f.write('\n'.join(optimized_lines))
                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to optimize {py_file}: {e}")
    
    def _validate_deployment(self, component_path: Path, environment: str) -> Dict:
        """Validate component deployment."""
        logger.info(f"‚úÖ Validating deployment for {environment}")
        
        validation_result = {
            "passed": False,
            "checks": {},
            "errors": []
        }
        
        try:
            # Check if Python files are syntactically correct
            syntax_check = True
            for py_file in component_path.rglob("*.py"):
                try:
                    with open(py_file, 'r') as f:
                        compile(f.read(), py_file, 'exec')
                except SyntaxError as e:
                    syntax_check = False
                    validation_result["errors"].append(f"Syntax error in {py_file}: {e}")
            
            validation_result["checks"]["syntax"] = syntax_check
            
            # Check for required files
            required_files = ["__init__.py"] if component_path.is_dir() else []
            files_check = all((component_path / req_file).exists() for req_file in required_files)
            validation_result["checks"]["required_files"] = files_check
            
            # Overall validation
            validation_result["passed"] = syntax_check and files_check
        
        except Exception as e:
            validation_result["errors"].append(str(e))
        
        return validation_result
    
    def create_deployment_package(self, components: List[Path], target_env: str = "production") -> Dict:
        """Create a deployment package with all components."""
        logger.info(f"üì¶ Creating deployment package for {target_env}")
        
        package_report = {
            "target_environment": target_env,
            "package_time": datetime.now().isoformat(),
            "components": [],
            "success": False,
            "package_path": None,
            "deployment_ready": False
        }
        
        try:
            # Create package directory
            package_name = f"grandmodel-{target_env}-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            if target_env == "staging":
                package_path = self.staging_path / package_name
            else:
                package_path = self.prod_path / package_name
            
            package_path.mkdir(parents=True, exist_ok=True)
            
            # Promote each component
            all_promotions_successful = True
            for component in components:
                promotion_result = self.promote_component(component, target_env)
                package_report["components"].append(promotion_result)
                
                if not promotion_result["success"]:
                    all_promotions_successful = False
            
            # Create deployment manifest
            manifest = {
                "package_name": package_name,
                "target_environment": target_env,
                "components": [str(comp) for comp in components],
                "creation_time": package_report["package_time"],
                "performance_targets": self.performance_targets,
                "validation_results": package_report["components"]
            }
            
            manifest_file = package_path / "deployment_manifest.json"
            with open(manifest_file, 'w') as f:
                json.dump(manifest, f, indent=2)
            
            package_report["package_path"] = str(package_path)
            package_report["success"] = True
            package_report["deployment_ready"] = all_promotions_successful
        
        except Exception as e:
            logger.error(f"‚ùå Error creating deployment package: {e}")
            package_report["errors"] = [str(e)]
        
        return package_report
    
    def automated_promotion_pipeline(self, trigger_event: str = "manual") -> Dict:
        """Run the complete automated promotion pipeline."""
        logger.info("üöÄ Starting automated promotion pipeline")
        
        pipeline_report = {
            "pipeline_start": datetime.now().isoformat(),
            "trigger_event": trigger_event,
            "stages": {},
            "overall_success": False,
            "deployment_ready": False
        }
        
        try:
            # Stage 1: Identify candidates for promotion
            logger.info("Stage 1: Identifying promotion candidates")
            candidates = self._identify_promotion_candidates()
            pipeline_report["stages"]["candidate_identification"] = {
                "candidates": [str(c) for c in candidates],
                "count": len(candidates)
            }
            
            # Stage 2: Analyze readiness
            logger.info("Stage 2: Analyzing candidate readiness")
            ready_components = []
            for candidate in candidates:
                readiness = self.analyze_component_readiness(candidate)
                if readiness["ready_for_promotion"]:
                    ready_components.append(candidate)
            
            pipeline_report["stages"]["readiness_analysis"] = {
                "ready_components": [str(c) for c in ready_components],
                "ready_count": len(ready_components),
                "total_candidates": len(candidates)
            }
            
            # Stage 3: Create staging deployment
            if ready_components:
                logger.info("Stage 3: Creating staging deployment")
                staging_package = self.create_deployment_package(ready_components, "staging")
                pipeline_report["stages"]["staging_deployment"] = staging_package
                
                # Stage 4: Validate staging
                if staging_package["success"]:
                    logger.info("Stage 4: Validating staging deployment")
                    staging_validation = self._run_staging_validation()
                    pipeline_report["stages"]["staging_validation"] = staging_validation
                    
                    # Stage 5: Production deployment (if staging passes)
                    if staging_validation["passed"]:
                        logger.info("Stage 5: Creating production deployment")
                        prod_package = self.create_deployment_package(ready_components, "production")
                        pipeline_report["stages"]["production_deployment"] = prod_package
                        
                        pipeline_report["overall_success"] = prod_package["success"]
                        pipeline_report["deployment_ready"] = prod_package["deployment_ready"]
            
            pipeline_report["pipeline_end"] = datetime.now().isoformat()
        
        except Exception as e:
            logger.error(f"‚ùå Pipeline error: {e}")
            pipeline_report["error"] = str(e)
        
        # Save pipeline report
        report_file = self.dev_path / f"migration_pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(pipeline_report, f, indent=2)
        
        logger.info(f"üìä Pipeline report saved: {report_file}")
        return pipeline_report
    
    def _identify_promotion_candidates(self) -> List[Path]:
        """Identify components that are candidates for promotion."""
        candidates = []
        
        # Core notebooks
        notebook_dir = self.dev_path / "train_notebooks"
        if notebook_dir.exists():
            for notebook in notebook_dir.glob("*.ipynb"):
                candidates.append(notebook)
        
        # Source modules
        src_dir = self.dev_path / "src"
        if src_dir.exists():
            for module_dir in src_dir.iterdir():
                if module_dir.is_dir() and module_dir.name != "__pycache__":
                    candidates.append(module_dir)
        
        return candidates
    
    def _run_staging_validation(self) -> Dict:
        """Run validation tests on staging environment."""
        logger.info("üß™ Running staging validation")
        
        validation_result = {
            "passed": False,
            "tests_run": 0,
            "tests_passed": 0,
            "performance_metrics": {},
            "errors": []
        }
        
        try:
            # Simplified validation - in practice this would run comprehensive tests
            staging_exists = self.staging_path.exists()
            validation_result["tests_run"] = 1
            validation_result["tests_passed"] = 1 if staging_exists else 0
            validation_result["passed"] = staging_exists
            
            if not staging_exists:
                validation_result["errors"].append("Staging environment not found")
        
        except Exception as e:
            validation_result["errors"].append(str(e))
        
        return validation_result

def main():
    """Main execution function."""
    print("üöÄ Automated Migration Pipeline")
    print("="*40)
    
    pipeline = AutomatedMigrationPipeline()
    
    import argparse
    parser = argparse.ArgumentParser(description="Automated Migration Pipeline")
    parser.add_argument("--component", help="Specific component to analyze/promote")
    parser.add_argument("--target-env", choices=["staging", "production"], default="staging")
    parser.add_argument("--full-pipeline", action="store_true", help="Run full automated pipeline")
    
    args = parser.parse_args()
    
    try:
        if args.full_pipeline:
            # Run complete automated pipeline
            result = pipeline.automated_promotion_pipeline()
            
            print(f"\nüéâ Pipeline completed!")
            print(f"Overall success: {result['overall_success']}")
            print(f"Deployment ready: {result['deployment_ready']}")
            
            if result["deployment_ready"]:
                print("üöÄ Components are ready for production deployment!")
            else:
                print("‚ö†Ô∏è Some components need more work before production.")
        
        elif args.component:
            # Analyze specific component
            component_path = Path(args.component)
            if not component_path.exists():
                component_path = Path("/home/QuantNova/GrandModel") / args.component
            
            if component_path.exists():
                readiness = pipeline.analyze_component_readiness(component_path)
                
                print(f"\nüìä Component Analysis: {component_path}")
                print(f"Ready for promotion: {readiness['ready_for_promotion']}")
                print(f"Quality score: {readiness['quality_score']:.2f}")
                
                if readiness["issues"]:
                    print("\n‚ö†Ô∏è Issues to address:")
                    for issue in readiness["issues"]:
                        print(f"  ‚Ä¢ {issue}")
                
                if readiness["recommendations"]:
                    print("\nüí° Recommendations:")
                    for rec in readiness["recommendations"]:
                        print(f"  ‚Ä¢ {rec}")
                
                # Optionally promote if ready
                if readiness["ready_for_promotion"]:
                    promote = input(f"\nPromote to {args.target_env}? (y/N): ")
                    if promote.lower() == 'y':
                        promotion_result = pipeline.promote_component(component_path, args.target_env)
                        print(f"Promotion {'successful' if promotion_result['success'] else 'failed'}")
            else:
                print(f"‚ùå Component not found: {component_path}")
        
        else:
            # Show candidates for promotion
            candidates = pipeline._identify_promotion_candidates()
            print(f"\nüìã Found {len(candidates)} promotion candidates:")
            for candidate in candidates:
                print(f"  ‚Ä¢ {candidate}")
            
            print("\nRun with --full-pipeline to start automated promotion")
    
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()