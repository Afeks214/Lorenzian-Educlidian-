{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlgoSpace Strategy - Optimized Production Version\n",
    "\n",
    "**Version**: 4.0 (Optimized)\n",
    "**Features**: \n",
    "- Ultra-fast Numba JIT compilation\n",
    "- Efficient memory usage\n",
    "- Robust error handling\n",
    "- Production-ready code\n",
    "- No emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: Environment Setup and Imports ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "import numba\n",
    "from numba import jit, njit, prange, typed, types\n",
    "from numba.experimental import jitclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Numba settings for maximum performance\n",
    "numba.config.THREADING_LAYER = 'threadsafe'\n",
    "\n",
    "print(\"AlgoSpace Multi-Indicator Trading Strategy\")\n",
    "print(\"Version: 4.0 - Optimized Production\")\n",
    "print(\"Status: Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: Optimized Data Loading ===\n",
    "\n",
    "def load_and_prepare_data(file_path, timeframe='5m'):\n",
    "    \"\"\"Optimized data loading with proper datetime handling\"\"\"\n",
    "    # Read CSV with optimized settings\n",
    "    df = pd.read_csv(file_path, parse_dates=True, infer_datetime_format=True)\n",
    "    \n",
    "    # Find datetime column\n",
    "    datetime_cols = ['Timestamp', 'timestamp', 'Date', 'date', 'Time', 'time', 'Datetime', 'datetime']\n",
    "    datetime_col = next((col for col in datetime_cols if col in df.columns), None)\n",
    "    \n",
    "    if datetime_col:\n",
    "        # Parse datetime with multiple format attempts\n",
    "        for date_format in ['%d/%m/%Y %H:%M', '%m/%d/%Y %H:%M', '%Y-%m-%d %H:%M:%S', None]:\n",
    "            try:\n",
    "                if date_format:\n",
    "                    df['Datetime'] = pd.to_datetime(df[datetime_col], format=date_format)\n",
    "                else:\n",
    "                    df['Datetime'] = pd.to_datetime(df[datetime_col], dayfirst=True)\n",
    "                \n",
    "                if df['Datetime'].notna().sum() > len(df) * 0.8:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Set index\n",
    "    if 'Datetime' in df.columns and df['Datetime'].notna().sum() > len(df) * 0.8:\n",
    "        df.set_index('Datetime', inplace=True)\n",
    "    else:\n",
    "        # Create synthetic index\n",
    "        freq = '5T' if timeframe == '5m' else '30T'\n",
    "        df.index = pd.date_range(start='2020-01-01', periods=len(df), freq=freq)\n",
    "    \n",
    "    # Standardize columns\n",
    "    column_map = {\n",
    "        'open': 'Open', 'high': 'High', 'low': 'Low',\n",
    "        'close': 'Close', 'volume': 'Volume'\n",
    "    }\n",
    "    df.rename(columns={k: v for k, v in column_map.items() if k in df.columns.str.lower()}, inplace=True)\n",
    "    \n",
    "    # Ensure numeric types\n",
    "    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Data loading functions optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: Load Data Files ===\n",
    "\n",
    "# File paths\n",
    "file_path_5min = \"/home/QuantNova/AlgoSpace-Strategy-1/@NQ - 5 min - ETH.csv\"\n",
    "file_path_30min = \"/home/QuantNova/AlgoSpace-Strategy-1/NQ - 30 min - ETH.csv\"\n",
    "\n",
    "# Load data with error handling\n",
    "try:\n",
    "    print(\"Loading 5-minute data...\")\n",
    "    df_5m = load_and_prepare_data(file_path_5min, '5m')\n",
    "    print(f\"5-minute data: {df_5m.shape[0]:,} rows, {df_5m.index[0]} to {df_5m.index[-1]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading 5-minute data: {e}\")\n",
    "    df_5m = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    print(\"\\nLoading 30-minute data...\")\n",
    "    df_30m = load_and_prepare_data(file_path_30min, '30m')\n",
    "    print(f\"30-minute data: {df_30m.shape[0]:,} rows, {df_30m.index[0]} to {df_30m.index[-1]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading 30-minute data: {e}\")\n",
    "    df_30m = pd.DataFrame()\n",
    "\n",
    "print(\"\\nData loading complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: Ultra-Fast FVG Detection ===\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def detect_fvg_optimized(high, low, lookback=3, validity=20):\n",
    "    \"\"\"Ultra-fast FVG detection with parallel processing\"\"\"\n",
    "    n = len(high)\n",
    "    bull_fvg = np.zeros(n, dtype=np.bool_)\n",
    "    bear_fvg = np.zeros(n, dtype=np.bool_)\n",
    "    bull_active = np.zeros(n, dtype=np.bool_)\n",
    "    bear_active = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    # Parallel FVG detection\n",
    "    for i in prange(lookback, n):\n",
    "        # Bullish FVG\n",
    "        if low[i] > high[i-lookback]:\n",
    "            bull_fvg[i] = True\n",
    "            # Mark active period\n",
    "            end_idx = min(i + validity, n)\n",
    "            for j in range(i, end_idx):\n",
    "                if low[j] >= high[i-lookback]:\n",
    "                    bull_active[j] = True\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        # Bearish FVG\n",
    "        if high[i] < low[i-lookback]:\n",
    "            bear_fvg[i] = True\n",
    "            # Mark active period\n",
    "            end_idx = min(i + validity, n)\n",
    "            for j in range(i, end_idx):\n",
    "                if high[j] <= low[i-lookback]:\n",
    "                    bear_active[j] = True\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    return bull_fvg, bear_fvg, bull_active, bear_active\n",
    "\n",
    "# Calculate FVG\n",
    "if not df_5m.empty:\n",
    "    print(\"Calculating FVG indicators...\")\n",
    "    bull_fvg, bear_fvg, bull_active, bear_active = detect_fvg_optimized(\n",
    "        df_5m['High'].values, df_5m['Low'].values\n",
    "    )\n",
    "    \n",
    "    df_5m['FVG_Bull_Detected'] = bull_fvg\n",
    "    df_5m['FVG_Bear_Detected'] = bear_fvg\n",
    "    df_5m['FVG_Bull_Active'] = bull_active\n",
    "    df_5m['FVG_Bear_Active'] = bear_active\n",
    "    \n",
    "    print(f\"FVG Complete: Bull={bull_fvg.sum():,}, Bear={bear_fvg.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: Optimized MLMI with KNN ===\n",
    "\n",
    "# MLMI Data Container\n",
    "spec = [\n",
    "    ('features', numba.float64[:, :]),\n",
    "    ('labels', numba.int64[:]),\n",
    "    ('size', numba.int64),\n",
    "    ('capacity', numba.int64)\n",
    "]\n",
    "\n",
    "@jitclass(spec)\n",
    "class MLMIData:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.features = np.zeros((capacity, 2), dtype=np.float64)\n",
    "        self.labels = np.zeros(capacity, dtype=np.int64)\n",
    "        self.size = 0\n",
    "    \n",
    "    def add(self, f1, f2, label):\n",
    "        if self.size >= self.capacity:\n",
    "            # Shift data\n",
    "            shift = self.capacity // 4\n",
    "            self.features[:-shift] = self.features[shift:]\n",
    "            self.labels[:-shift] = self.labels[shift:]\n",
    "            self.size = self.capacity - shift\n",
    "        \n",
    "        self.features[self.size, 0] = f1\n",
    "        self.features[self.size, 1] = f2\n",
    "        self.labels[self.size] = label\n",
    "        self.size += 1\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def wma_fast(values, period):\n",
    "    \"\"\"Fast weighted moving average\"\"\"\n",
    "    n = len(values)\n",
    "    result = np.full(n, np.nan)\n",
    "    \n",
    "    if period > n:\n",
    "        return result\n",
    "    \n",
    "    weights = np.arange(1, period + 1, dtype=np.float64)\n",
    "    sum_weights = weights.sum()\n",
    "    \n",
    "    for i in range(period - 1, n):\n",
    "        weighted_sum = 0.0\n",
    "        for j in range(period):\n",
    "            weighted_sum += values[i - period + j + 1] * weights[j]\n",
    "        result[i] = weighted_sum / sum_weights\n",
    "    \n",
    "    return result\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def rsi_fast(prices, period):\n",
    "    \"\"\"Fast RSI calculation\"\"\"\n",
    "    n = len(prices)\n",
    "    rsi = np.full(n, 50.0)\n",
    "    \n",
    "    if period >= n:\n",
    "        return rsi\n",
    "    \n",
    "    gains = np.zeros(n)\n",
    "    losses = np.zeros(n)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        diff = prices[i] - prices[i-1]\n",
    "        if diff > 0:\n",
    "            gains[i] = diff\n",
    "        else:\n",
    "            losses[i] = -diff\n",
    "    \n",
    "    avg_gain = np.mean(gains[1:period+1])\n",
    "    avg_loss = np.mean(losses[1:period+1])\n",
    "    \n",
    "    if avg_loss > 0:\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi[period] = 100 - (100 / (1 + rs))\n",
    "    else:\n",
    "        rsi[period] = 100\n",
    "    \n",
    "    for i in range(period + 1, n):\n",
    "        avg_gain = (avg_gain * (period - 1) + gains[i]) / period\n",
    "        avg_loss = (avg_loss * (period - 1) + losses[i]) / period\n",
    "        \n",
    "        if avg_loss > 0:\n",
    "            rs = avg_gain / avg_loss\n",
    "            rsi[i] = 100 - (100 / (1 + rs))\n",
    "        else:\n",
    "            rsi[i] = 100\n",
    "    \n",
    "    return rsi\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def knn_predict(features, labels, query, k):\n",
    "    \"\"\"Fast k-nearest neighbors prediction\"\"\"\n",
    "    n = len(labels)\n",
    "    if n == 0 or k == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        dist = 0.0\n",
    "        for j in range(2):\n",
    "            diff = features[i, j] - query[j]\n",
    "            dist += diff * diff\n",
    "        distances[i] = np.sqrt(dist)\n",
    "    \n",
    "    # Find k nearest neighbors\n",
    "    k = min(k, n)\n",
    "    indices = np.argsort(distances)[:k]\n",
    "    \n",
    "    # Vote\n",
    "    vote = 0\n",
    "    for i in range(k):\n",
    "        vote += labels[indices[i]]\n",
    "    \n",
    "    return float(vote)\n",
    "\n",
    "def calculate_mlmi_fast(df, k_neighbors=200):\n",
    "    \"\"\"Fast MLMI calculation\"\"\"\n",
    "    print(\"Calculating MLMI indicators...\")\n",
    "    \n",
    "    close = df['Close'].values\n",
    "    n = len(close)\n",
    "    \n",
    "    # Calculate indicators\n",
    "    ma_fast = wma_fast(close, 5)\n",
    "    ma_slow = wma_fast(close, 20)\n",
    "    rsi_fast_vals = rsi_fast(close, 5)\n",
    "    rsi_slow_vals = rsi_fast(close, 20)\n",
    "    \n",
    "    # Smooth RSI\n",
    "    rsi_fast_smooth = wma_fast(rsi_fast_vals, 20)\n",
    "    rsi_slow_smooth = wma_fast(rsi_slow_vals, 20)\n",
    "    \n",
    "    # Initialize MLMI data\n",
    "    mlmi_data = MLMIData(min(10000, n))\n",
    "    mlmi_values = np.zeros(n)\n",
    "    \n",
    "    # Process crossovers\n",
    "    for i in range(1, n):\n",
    "        # Detect crossovers\n",
    "        if ma_fast[i] > ma_slow[i] and ma_fast[i-1] <= ma_slow[i-1]:\n",
    "            if not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "                label = 1 if i < n-1 and close[i+1] > close[i] else -1\n",
    "                mlmi_data.add(rsi_slow_smooth[i], rsi_fast_smooth[i], label)\n",
    "        \n",
    "        elif ma_fast[i] < ma_slow[i] and ma_fast[i-1] >= ma_slow[i-1]:\n",
    "            if not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "                label = -1 if i < n-1 and close[i+1] < close[i] else 1\n",
    "                mlmi_data.add(rsi_slow_smooth[i], rsi_fast_smooth[i], label)\n",
    "        \n",
    "        # Make prediction\n",
    "        if mlmi_data.size > 0 and not np.isnan(rsi_fast_smooth[i]) and not np.isnan(rsi_slow_smooth[i]):\n",
    "            query = np.array([rsi_slow_smooth[i], rsi_fast_smooth[i]])\n",
    "            mlmi_values[i] = knn_predict(\n",
    "                mlmi_data.features[:mlmi_data.size],\n",
    "                mlmi_data.labels[:mlmi_data.size],\n",
    "                query,\n",
    "                min(k_neighbors, mlmi_data.size)\n",
    "            )\n",
    "    \n",
    "    # Add to dataframe\n",
    "    df['mlmi'] = mlmi_values\n",
    "    df['mlmi_ma'] = wma_fast(mlmi_values, 20)\n",
    "    df['mlmi_bull'] = mlmi_values > 0\n",
    "    df['mlmi_bear'] = mlmi_values < 0\n",
    "    \n",
    "    # Crossovers\n",
    "    mlmi_bull_cross = np.zeros(n, dtype=bool)\n",
    "    mlmi_bear_cross = np.zeros(n, dtype=bool)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        if mlmi_values[i] > 0 and mlmi_values[i-1] <= 0:\n",
    "            mlmi_bull_cross[i] = True\n",
    "        elif mlmi_values[i] < 0 and mlmi_values[i-1] >= 0:\n",
    "            mlmi_bear_cross[i] = True\n",
    "    \n",
    "    df['mlmi_bull_cross'] = mlmi_bull_cross\n",
    "    df['mlmi_bear_cross'] = mlmi_bear_cross\n",
    "    \n",
    "    print(f\"MLMI Complete: Range=[{mlmi_values.min():.1f}, {mlmi_values.max():.1f}]\")\n",
    "    return df\n",
    "\n",
    "# Calculate MLMI\n",
    "if not df_30m.empty:\n",
    "    df_30m = calculate_mlmi_fast(df_30m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: Optimized NW-RQK ===\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def rational_quadratic_kernel(x, h, r):\n",
    "    \"\"\"Rational quadratic kernel function\"\"\"\n",
    "    return (1.0 + (x * x) / (h * h * 2.0 * r)) ** (-r)\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def nadaraya_watson_fast(prices, h, r, min_periods=25):\n",
    "    \"\"\"Fast Nadaraya-Watson regression\"\"\"\n",
    "    n = len(prices)\n",
    "    result = np.full(n, np.nan)\n",
    "    \n",
    "    for i in prange(min_periods, n):\n",
    "        weighted_sum = 0.0\n",
    "        weight_sum = 0.0\n",
    "        \n",
    "        # Process window\n",
    "        window_size = min(i + 1, 500)  # Limit window for performance\n",
    "        \n",
    "        for j in range(window_size):\n",
    "            if i - j >= 0:\n",
    "                weight = rational_quadratic_kernel(float(j), h, r)\n",
    "                weighted_sum += prices[i - j] * weight\n",
    "                weight_sum += weight\n",
    "        \n",
    "        if weight_sum > 0:\n",
    "            result[i] = weighted_sum / weight_sum\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calculate_nwrqk_fast(df, h=8.0, r=8.0, lag=2):\n",
    "    \"\"\"Fast NW-RQK calculation\"\"\"\n",
    "    print(\"Calculating NW-RQK indicators...\")\n",
    "    \n",
    "    prices = df['Close'].values\n",
    "    \n",
    "    # Calculate regression lines\n",
    "    yhat1 = nadaraya_watson_fast(prices, h, r)\n",
    "    yhat2 = nadaraya_watson_fast(prices, h - lag, r)\n",
    "    \n",
    "    # Store in dataframe\n",
    "    df['yhat1'] = yhat1\n",
    "    df['yhat2'] = yhat2\n",
    "    \n",
    "    # Calculate signals\n",
    "    n = len(df)\n",
    "    bull_change = np.zeros(n, dtype=bool)\n",
    "    bear_change = np.zeros(n, dtype=bool)\n",
    "    bull_cross = np.zeros(n, dtype=bool)\n",
    "    bear_cross = np.zeros(n, dtype=bool)\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        if not np.isnan(yhat1[i]) and not np.isnan(yhat1[i-1]) and not np.isnan(yhat1[i-2]):\n",
    "            # Trend changes\n",
    "            was_bear = yhat1[i-2] > yhat1[i-1]\n",
    "            was_bull = yhat1[i-2] < yhat1[i-1]\n",
    "            is_bear = yhat1[i-1] > yhat1[i]\n",
    "            is_bull = yhat1[i-1] < yhat1[i]\n",
    "            \n",
    "            if is_bull and was_bear:\n",
    "                bull_change[i] = True\n",
    "            elif is_bear and was_bull:\n",
    "                bear_change[i] = True\n",
    "        \n",
    "        # Crossovers\n",
    "        if not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]):\n",
    "            if i > 0 and not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]):\n",
    "                if yhat2[i] > yhat1[i] and yhat2[i-1] <= yhat1[i-1]:\n",
    "                    bull_cross[i] = True\n",
    "                elif yhat2[i] < yhat1[i] and yhat2[i-1] >= yhat1[i-1]:\n",
    "                    bear_cross[i] = True\n",
    "    \n",
    "    df['nwrqk_bull'] = bull_change\n",
    "    df['nwrqk_bear'] = bear_change\n",
    "    df['nwrqk_bull_cross'] = bull_cross\n",
    "    df['nwrqk_bear_cross'] = bear_cross\n",
    "    \n",
    "    print(f\"NW-RQK Complete: Bull={bull_change.sum():,}, Bear={bear_change.sum():,}\")\n",
    "    return df\n",
    "\n",
    "# Calculate NW-RQK\n",
    "if not df_30m.empty:\n",
    "    df_30m = calculate_nwrqk_fast(df_30m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7: Data Alignment ===\n",
    "\n",
    "def align_timeframes(df_30m, df_5m):\n",
    "    \"\"\"Efficiently align 30-minute indicators to 5-minute data\"\"\"\n",
    "    print(\"\\nAligning timeframes...\")\n",
    "    \n",
    "    # Check datetime indices\n",
    "    if not isinstance(df_5m.index, pd.DatetimeIndex) or not isinstance(df_30m.index, pd.DatetimeIndex):\n",
    "        print(\"Error: Both dataframes must have datetime index\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Start with 5-minute data\n",
    "    df = df_5m.copy()\n",
    "    \n",
    "    # Align 30-minute indicators\n",
    "    indicators = ['mlmi', 'mlmi_bull', 'mlmi_bear', 'nwrqk_bull', 'nwrqk_bear']\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        if indicator in df_30m.columns:\n",
    "            df[f'{indicator}_30m'] = df_30m[indicator].reindex(df.index, method='ffill')\n",
    "    \n",
    "    # Add derived features\n",
    "    df['returns'] = df['Close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    df['volatility'] = df['returns'].rolling(20).std()\n",
    "    df['atr'] = (df['High'] - df['Low']).rolling(20).mean()\n",
    "    \n",
    "    # Drop initial NaN rows\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    print(f\"Alignment complete: {len(df):,} rows\")\n",
    "    return df\n",
    "\n",
    "# Prepare backtesting data\n",
    "if not df_30m.empty and not df_5m.empty:\n",
    "    df_backtest = align_timeframes(df_30m, df_5m)\n",
    "    print(f\"Backtest data ready: {df_backtest.index[0]} to {df_backtest.index[-1]}\")\n",
    "else:\n",
    "    print(\"Missing required data\")\n",
    "    df_backtest = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 8: Optimized Synergy Detection ===\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def detect_all_synergies(mlmi_bull, mlmi_bear, fvg_bull, fvg_bear, \n",
    "                        nwrqk_bull, nwrqk_bear, window=30):\n",
    "    \"\"\"Detect all 4 synergy types in one pass\"\"\"\n",
    "    n = len(mlmi_bull)\n",
    "    \n",
    "    # Initialize signal arrays for all synergy types\n",
    "    syn1_long = np.zeros(n, dtype=np.bool_)\n",
    "    syn1_short = np.zeros(n, dtype=np.bool_)\n",
    "    syn2_long = np.zeros(n, dtype=np.bool_)\n",
    "    syn2_short = np.zeros(n, dtype=np.bool_)\n",
    "    syn3_long = np.zeros(n, dtype=np.bool_)\n",
    "    syn3_short = np.zeros(n, dtype=np.bool_)\n",
    "    syn4_long = np.zeros(n, dtype=np.bool_)\n",
    "    syn4_short = np.zeros(n, dtype=np.bool_)\n",
    "    \n",
    "    half_window = window // 2\n",
    "    \n",
    "    for i in range(2, n - window):\n",
    "        # Type 1: MLMI → FVG → NWRQK\n",
    "        if mlmi_bull[i] and not mlmi_bull[i-1]:\n",
    "            for j in range(i+1, min(i+half_window, n)):\n",
    "                if fvg_bull[j]:\n",
    "                    for k in range(j+1, min(i+window, n)):\n",
    "                        if nwrqk_bull[k]:\n",
    "                            syn1_long[k] = True\n",
    "                            break\n",
    "                    break\n",
    "        \n",
    "        if mlmi_bear[i] and not mlmi_bear[i-1]:\n",
    "            for j in range(i+1, min(i+half_window, n)):\n",
    "                if fvg_bear[j]:\n",
    "                    for k in range(j+1, min(i+window, n)):\n",
    "                        if nwrqk_bear[k]:\n",
    "                            syn1_short[k] = True\n",
    "                            break\n",
    "                    break\n",
    "        \n",
    "        # Type 2: MLMI → NWRQK → FVG\n",
    "        if mlmi_bull[i] and not mlmi_bull[i-1]:\n",
    "            for j in range(i+1, min(i+half_window, n)):\n",
    "                if nwrqk_bull[j]:\n",
    "                    for k in range(j+1, min(i+window, n)):\n",
    "                        if fvg_bull[k]:\n",
    "                            syn2_long[k] = True\n",
    "                            break\n",
    "                    break\n",
    "        \n",
    "        if mlmi_bear[i] and not mlmi_bear[i-1]:\n",
    "            for j in range(i+1, min(i+half_window, n)):\n",
    "                if nwrqk_bear[j]:\n",
    "                    for k in range(j+1, min(i+window, n)):\n",
    "                        if fvg_bear[k]:\n",
    "                            syn2_short[k] = True\n",
    "                            break\n",
    "                    break\n",
    "        \n",
    "        # Type 3: NWRQK → MLMI → FVG\n",
    "        if nwrqk_bull[i]:\n",
    "            for j in range(i+1, min(i+half_window, n)):\n",
    "                if mlmi_bull[j]:\n",
    "                    for k in range(j+1, min(i+window, n)):\n",
    "                        if fvg_bull[k]:\n",
    "                            syn3_long[k] = True\n",
    "                            break\n",
    "                    break\n",
    "        \n",
    "        if nwrqk_bear[i]:\n",
    "            for j in range(i+1, min(i+half_window, n)):\n",
    "                if mlmi_bear[j]:\n",
    "                    for k in range(j+1, min(i+window, n)):\n",
    "                        if fvg_bear[k]:\n",
    "                            syn3_short[k] = True\n",
    "                            break\n",
    "                    break\n",
    "        \n",
    "        # Type 4: NWRQK → FVG → MLMI\n",
    "        if nwrqk_bull[i]:\n",
    "            for j in range(i+1, min(i+half_window, n)):\n",
    "                if fvg_bull[j]:\n",
    "                    for k in range(j+1, min(i+window, n)):\n",
    "                        if mlmi_bull[k]:\n",
    "                            syn4_long[k] = True\n",
    "                            break\n",
    "                    break\n",
    "        \n",
    "        if nwrqk_bear[i]:\n",
    "            for j in range(i+1, min(i+half_window, n)):\n",
    "                if fvg_bear[j]:\n",
    "                    for k in range(j+1, min(i+window, n)):\n",
    "                        if mlmi_bear[k]:\n",
    "                            syn4_short[k] = True\n",
    "                            break\n",
    "                    break\n",
    "    \n",
    "    return (syn1_long, syn1_short, syn2_long, syn2_short,\n",
    "            syn3_long, syn3_short, syn4_long, syn4_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 9: Strategy Implementation ===\n",
    "\n",
    "class OptimizedSynergyStrategy:\n",
    "    \"\"\"Optimized strategy implementation for all synergy types\"\"\"\n",
    "    \n",
    "    def __init__(self, data, synergy_type):\n",
    "        self.data = data\n",
    "        self.synergy_type = synergy_type\n",
    "        self.synergy_names = {\n",
    "            1: \"Type 1: MLMI → FVG → NWRQK\",\n",
    "            2: \"Type 2: MLMI → NWRQK → FVG\",\n",
    "            3: \"Type 3: NWRQK → MLMI → FVG\",\n",
    "            4: \"Type 4: NWRQK → FVG → MLMI\"\n",
    "        }\n",
    "    \n",
    "    def generate_signals(self):\n",
    "        \"\"\"Generate trading signals\"\"\"\n",
    "        # Extract indicator arrays\n",
    "        mlmi_bull = self.data['mlmi_bull_30m'].fillna(False).values\n",
    "        mlmi_bear = self.data['mlmi_bear_30m'].fillna(False).values\n",
    "        fvg_bull = self.data['FVG_Bull_Active'].values\n",
    "        fvg_bear = self.data['FVG_Bear_Active'].values\n",
    "        nwrqk_bull = self.data['nwrqk_bull_30m'].fillna(False).values\n",
    "        nwrqk_bear = self.data['nwrqk_bear_30m'].fillna(False).values\n",
    "        \n",
    "        # Detect all synergies\n",
    "        all_signals = detect_all_synergies(\n",
    "            mlmi_bull, mlmi_bear, fvg_bull, fvg_bear, \n",
    "            nwrqk_bull, nwrqk_bear\n",
    "        )\n",
    "        \n",
    "        # Select signals for this synergy type\n",
    "        idx = (self.synergy_type - 1) * 2\n",
    "        long_entries = all_signals[idx]\n",
    "        short_entries = all_signals[idx + 1]\n",
    "        \n",
    "        # Simple exit logic\n",
    "        n = len(self.data)\n",
    "        long_exits = np.zeros(n, dtype=bool)\n",
    "        short_exits = np.zeros(n, dtype=bool)\n",
    "        \n",
    "        return {\n",
    "            'long_entries': pd.Series(long_entries, index=self.data.index),\n",
    "            'short_entries': pd.Series(short_entries, index=self.data.index),\n",
    "            'long_exits': pd.Series(long_exits, index=self.data.index),\n",
    "            'short_exits': pd.Series(short_exits, index=self.data.index)\n",
    "        }\n",
    "    \n",
    "    def backtest(self, init_cash=100000):\n",
    "        \"\"\"Run optimized backtest\"\"\"\n",
    "        signals = self.generate_signals()\n",
    "        \n",
    "        print(f\"\\nBacktesting {self.synergy_names[self.synergy_type]}\")\n",
    "        print(f\"Long signals: {signals['long_entries'].sum():,}\")\n",
    "        print(f\"Short signals: {signals['short_entries'].sum():,}\")\n",
    "        \n",
    "        if signals['long_entries'].sum() == 0 and signals['short_entries'].sum() == 0:\n",
    "            print(\"No signals generated\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Create portfolio\n",
    "            portfolio = vbt.Portfolio.from_signals(\n",
    "                close=self.data['Close'],\n",
    "                entries=signals['long_entries'] | signals['short_entries'],\n",
    "                exits=signals['long_exits'] | signals['short_exits'],\n",
    "                direction=np.where(signals['long_entries'], 1, \n",
    "                                 np.where(signals['short_entries'], -1, 0)),\n",
    "                size=100,\n",
    "                init_cash=init_cash,\n",
    "                fees=0.0001,\n",
    "                slippage=0.0001,\n",
    "                freq='5T'\n",
    "            )\n",
    "            \n",
    "            return portfolio\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Backtest error: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 10: Run All Backtests ===\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING BACKTESTS FOR ALL SYNERGY TYPES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "portfolios = {}\n",
    "results = []\n",
    "\n",
    "if not df_backtest.empty:\n",
    "    for synergy_type in range(1, 5):\n",
    "        strategy = OptimizedSynergyStrategy(df_backtest, synergy_type)\n",
    "        portfolio = strategy.backtest()\n",
    "        \n",
    "        if portfolio:\n",
    "            portfolios[synergy_type] = portfolio\n",
    "            \n",
    "            # Calculate metrics\n",
    "            total_return = portfolio.total_return()\n",
    "            sharpe_ratio = portfolio.sharpe_ratio()\n",
    "            max_drawdown = portfolio.max_drawdown()\n",
    "            total_trades = portfolio.stats()['Total Trades']\n",
    "            win_rate = portfolio.stats()['Win Rate [%]']\n",
    "            \n",
    "            results.append({\n",
    "                'Synergy': strategy.synergy_names[synergy_type],\n",
    "                'Return': f\"{total_return:.2%}\",\n",
    "                'Sharpe': f\"{sharpe_ratio:.2f}\",\n",
    "                'Max DD': f\"{max_drawdown:.2%}\",\n",
    "                'Trades': total_trades,\n",
    "                'Win Rate': f\"{win_rate:.1f}%\"\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nResults for {strategy.synergy_names[synergy_type]}:\")\n",
    "            print(f\"Total Return: {total_return:.2%}\")\n",
    "            print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "            print(f\"Max Drawdown: {max_drawdown:.2%}\")\n",
    "            print(f\"Total Trades: {total_trades:,}\")\n",
    "            print(f\"Win Rate: {win_rate:.1f}%\")\n",
    "else:\n",
    "    print(\"No data available for backtesting\")\n",
    "\n",
    "# Display summary table\n",
    "if results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"=\" * 80)\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 11: Monte Carlo Validation ===\n",
    "\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def monte_carlo_simulation(returns, n_sims=1000, n_periods=252):\n",
    "    \"\"\"Fast Monte Carlo simulation\"\"\"\n",
    "    n_returns = len(returns)\n",
    "    sim_returns = np.zeros(n_sims)\n",
    "    sim_sharpes = np.zeros(n_sims)\n",
    "    \n",
    "    for i in prange(n_sims):\n",
    "        # Random sampling with replacement\n",
    "        indices = np.random.randint(0, n_returns, size=n_returns)\n",
    "        sampled = returns[indices]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_return = np.prod(1 + sampled) - 1\n",
    "        mean_return = np.mean(sampled)\n",
    "        std_return = np.std(sampled)\n",
    "        \n",
    "        sim_returns[i] = total_return\n",
    "        if std_return > 0:\n",
    "            sim_sharpes[i] = mean_return / std_return * np.sqrt(n_periods)\n",
    "        else:\n",
    "            sim_sharpes[i] = 0\n",
    "    \n",
    "    return sim_returns, sim_sharpes\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MONTE CARLO VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mc_results = []\n",
    "\n",
    "for synergy_type, portfolio in portfolios.items():\n",
    "    if portfolio:\n",
    "        print(f\"\\nRunning Monte Carlo for Type {synergy_type}...\")\n",
    "        \n",
    "        # Get returns\n",
    "        returns = portfolio.returns().values\n",
    "        returns = returns[~np.isnan(returns)]\n",
    "        \n",
    "        # Original metrics\n",
    "        orig_return = portfolio.total_return()\n",
    "        orig_sharpe = portfolio.sharpe_ratio()\n",
    "        \n",
    "        # Run simulation\n",
    "        sim_returns, sim_sharpes = monte_carlo_simulation(returns)\n",
    "        \n",
    "        # Calculate percentiles\n",
    "        return_pct = np.sum(sim_returns <= orig_return) / len(sim_returns) * 100\n",
    "        sharpe_pct = np.sum(sim_sharpes <= orig_sharpe) / len(sim_sharpes) * 100\n",
    "        \n",
    "        # 95% confidence intervals\n",
    "        return_ci_low = np.percentile(sim_returns, 2.5)\n",
    "        return_ci_high = np.percentile(sim_returns, 97.5)\n",
    "        sharpe_ci_low = np.percentile(sim_sharpes, 2.5)\n",
    "        sharpe_ci_high = np.percentile(sim_sharpes, 97.5)\n",
    "        \n",
    "        mc_results.append({\n",
    "            'Synergy': f\"Type {synergy_type}\",\n",
    "            'Return Percentile': f\"{return_pct:.1f}%\",\n",
    "            'Sharpe Percentile': f\"{sharpe_pct:.1f}%\",\n",
    "            'Return CI': f\"[{return_ci_low:.2%}, {return_ci_high:.2%}]\",\n",
    "            'Sharpe CI': f\"[{sharpe_ci_low:.2f}, {sharpe_ci_high:.2f}]\"\n",
    "        })\n",
    "        \n",
    "        print(f\"Return Percentile: {return_pct:.1f}%\")\n",
    "        print(f\"Sharpe Percentile: {sharpe_pct:.1f}%\")\n",
    "        print(f\"Average Percentile: {(return_pct + sharpe_pct) / 2:.1f}%\")\n",
    "\n",
    "# Display MC summary\n",
    "if mc_results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MONTE CARLO SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    mc_df = pd.DataFrame(mc_results)\n",
    "    print(mc_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 12: Final Analysis and Recommendations ===\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL ANALYSIS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find best performers\n",
    "if portfolios:\n",
    "    best_return = -float('inf')\n",
    "    best_sharpe = -float('inf')\n",
    "    best_trades = 0\n",
    "    \n",
    "    best_return_type = None\n",
    "    best_sharpe_type = None\n",
    "    best_trades_type = None\n",
    "    \n",
    "    for synergy_type, portfolio in portfolios.items():\n",
    "        if portfolio:\n",
    "            ret = portfolio.total_return()\n",
    "            sharpe = portfolio.sharpe_ratio()\n",
    "            trades = portfolio.stats()['Total Trades']\n",
    "            \n",
    "            if ret > best_return:\n",
    "                best_return = ret\n",
    "                best_return_type = synergy_type\n",
    "            \n",
    "            if sharpe > best_sharpe:\n",
    "                best_sharpe = sharpe\n",
    "                best_sharpe_type = synergy_type\n",
    "            \n",
    "            if trades > best_trades:\n",
    "                best_trades = trades\n",
    "                best_trades_type = synergy_type\n",
    "    \n",
    "    print(\"\\nPERFORMANCE LEADERS:\")\n",
    "    print(f\"Best Return: Type {best_return_type} ({best_return:.2%})\")\n",
    "    print(f\"Best Risk-Adjusted: Type {best_sharpe_type} (Sharpe: {best_sharpe:.2f})\")\n",
    "    print(f\"Most Active: Type {best_trades_type} ({best_trades:,} trades)\")\n",
    "    \n",
    "    print(\"\\nKEY INSIGHTS:\")\n",
    "    print(\"1. Each synergy type captures unique market patterns\")\n",
    "    print(\"2. Consider combining multiple synergies for diversification\")\n",
    "    print(\"3. Monitor performance and adjust parameters regularly\")\n",
    "    print(\"4. Use position sizing based on confidence levels\")\n",
    "    \n",
    "    print(\"\\nRECOMMENDED APPROACH:\")\n",
    "    print(\"- Allocate capital across top 2-3 performing synergies\")\n",
    "    print(\"- Use Monte Carlo percentiles to size positions\")\n",
    "    print(\"- Implement proper risk management with stops\")\n",
    "    print(\"- Regular rebalancing based on performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STRATEGY ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}