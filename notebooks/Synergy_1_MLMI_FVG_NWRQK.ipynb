{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlgoSpace Synergy Strategy 1: MLMI → FVG → NW-RQK\n",
    "\n",
    "## Strategy Overview\n",
    "This notebook implements the first synergy pattern, using Machine Learning Market Intelligence (MLMI), Fair Value Gaps (FVG), and Nadaraya-Watson with Rational Quadratic Kernel (NW-RQK) in a sequential confirmation pattern.\n",
    "\n",
    "### Key Components:\n",
    "1. **MLMI**: Advanced KNN-based pattern recognition for momentum analysis\n",
    "2. **FVG**: Detects price inefficiencies and gaps for entry zones\n",
    "3. **NW-RQK**: Kernel regression for trend direction confirmation\n",
    "\n",
    "### Signal Generation:\n",
    "- Long Entry: MLMI bullish → FVG bullish zone → NW-RQK bullish confirmation\n",
    "- Short Entry: MLMI bearish → FVG bearish zone → NW-RQK bearish confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Complete Environment Setup, Imports, and Configuration\n",
    "# This cell contains ALL necessary imports, configurations, and setup\n",
    "# No pre-initialization of variables - they will be created where needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "import numba\n",
    "from numba import jit, njit, prange, typed, types, float64, int64, boolean\n",
    "from numba.typed import Dict\n",
    "from numba.experimental import jitclass\n",
    "from scipy.spatial import cKDTree\n",
    "import warnings\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import os\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Numba for optimal performance\n",
    "numba.config.THREADING_LAYER = 'threadsafe'\n",
    "numba.config.NUMBA_NUM_THREADS = numba.config.NUMBA_DEFAULT_NUM_THREADS\n",
    "\n",
    "# Performance settings\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "# Configure VectorBT settings\n",
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 1200\n",
    "vbt.settings['plotting']['layout']['height'] = 600\n",
    "\n",
    "# Complete Strategy Configuration\n",
    "@dataclass\n",
    "class StrategyConfig:\n",
    "    \"\"\"Comprehensive configuration for the Synergy 1 strategy\"\"\"\n",
    "    # Data paths\n",
    "    data_5m_path: str = \"/home/QuantNova/AlgoSpace-8/notebooks/notebook data/@CL - 5 min - ETH.csv\"\n",
    "    data_30m_path: str = \"/home/QuantNova/AlgoSpace-8/notebooks/notebook data/@CL - 30 min - ETH.csv\"\n",
    "    \n",
    "    # MLMI parameters\n",
    "    mlmi_ma_fast_period: int = 5\n",
    "    mlmi_ma_slow_period: int = 20\n",
    "    mlmi_rsi_fast_period: int = 5\n",
    "    mlmi_rsi_slow_period: int = 20\n",
    "    mlmi_rsi_smooth_period: int = 20\n",
    "    mlmi_k_neighbors: int = 200\n",
    "    mlmi_max_data_size: int = 10000\n",
    "    \n",
    "    # FVG parameters\n",
    "    fvg_lookback: int = 10\n",
    "    fvg_body_multiplier: float = 1.5\n",
    "    fvg_validity: int = 20\n",
    "    \n",
    "    # NW-RQK parameters\n",
    "    nwrqk_h: float = 8.0\n",
    "    nwrqk_r: float = 8.0\n",
    "    nwrqk_x_0: int = 25\n",
    "    nwrqk_lag: int = 2\n",
    "    nwrqk_smooth_colors: bool = False\n",
    "    \n",
    "    # Synergy parameters\n",
    "    synergy_window: int = 30\n",
    "    mlmi_threshold: float = 0.0\n",
    "    \n",
    "    # Backtesting parameters\n",
    "    initial_capital: float = 100000.0\n",
    "    position_size: float = 100.0\n",
    "    fees: float = 0.0001\n",
    "    slippage: float = 0.0001\n",
    "    max_hold_bars: int = 100\n",
    "    stop_loss: float = 0.01\n",
    "    take_profit: float = 0.05\n",
    "    \n",
    "    # Data validation\n",
    "    min_data_points: int = 100\n",
    "\n",
    "# Initialize configuration\n",
    "config = StrategyConfig()\n",
    "print(\"✅ Environment setup complete\")\n",
    "print(f\"✅ Numba version: {numba.__version__}\")\n",
    "print(f\"✅ VectorBT version: {vbt.__version__}\")\n",
    "print(f\"✅ Configuration loaded with {len(config.__dataclass_fields__)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Robust Data Loading with Column Standardization\n",
    "# This cell loads and validates data with comprehensive column name standardization\n",
    "\n",
    "def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize column names to handle various naming conventions\"\"\"\n",
    "    # Create a mapping of common variations to standard names\n",
    "    column_mapping = {\n",
    "        # Timestamp variations\n",
    "        'timestamp': 'Timestamp', 'datetime': 'Timestamp', 'date': 'Timestamp',\n",
    "        'time': 'Timestamp', 'Time': 'Timestamp', 'Date': 'Timestamp',\n",
    "        'DateTime': 'Timestamp', 'TIMESTAMP': 'Timestamp',\n",
    "        \n",
    "        # Open variations\n",
    "        'open': 'Open', 'o': 'Open', 'OPEN': 'Open', 'O': 'Open',\n",
    "        \n",
    "        # High variations\n",
    "        'high': 'High', 'h': 'High', 'HIGH': 'High', 'H': 'High',\n",
    "        \n",
    "        # Low variations\n",
    "        'low': 'Low', 'l': 'Low', 'LOW': 'Low', 'L': 'Low',\n",
    "        \n",
    "        # Close variations\n",
    "        'close': 'Close', 'c': 'Close', 'CLOSE': 'Close', 'C': 'Close',\n",
    "        \n",
    "        # Volume variations\n",
    "        'volume': 'Volume', 'v': 'Volume', 'VOLUME': 'Volume', 'V': 'Volume',\n",
    "        'vol': 'Volume', 'Vol': 'Volume', 'VOL': 'Volume'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_dataframe(df: pd.DataFrame, name: str, min_data_points: int = 100) -> bool:\n",
    "    \"\"\"Validate dataframe has required columns and data quality\"\"\"\n",
    "    required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Check for required columns\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"⚠️ Warning: {name} missing columns: {missing_cols}\")\n",
    "        # Continue with available columns\n",
    "    \n",
    "    # Check for sufficient data\n",
    "    if len(df) < min_data_points:\n",
    "        raise ValueError(f\"{name} has insufficient data: {len(df)} rows (minimum: {min_data_points})\")\n",
    "    \n",
    "    # Check for valid price data\n",
    "    price_cols = ['Open', 'High', 'Low', 'Close']\n",
    "    for col in price_cols:\n",
    "        if col in df.columns:\n",
    "            if df[col].isna().all():\n",
    "                raise ValueError(f\"{name} column '{col}' contains only NaN values\")\n",
    "            if (df[col] <= 0).any():\n",
    "                print(f\"⚠️ Warning: {name} column '{col}' contains non-positive values\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def load_data_optimized(file_path: str, timeframe: str = '5m') -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare data with comprehensive error handling and column standardization\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        # Read CSV with intelligent parsing\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        \n",
    "        # Standardize column names\n",
    "        df = standardize_column_names(df)\n",
    "        \n",
    "        # Handle timestamp parsing\n",
    "        timestamp_col = None\n",
    "        for col in ['Timestamp', 'timestamp', 'DateTime', 'datetime', 'Date', 'date', 'Time', 'time']:\n",
    "            if col in df.columns:\n",
    "                timestamp_col = col\n",
    "                break\n",
    "        \n",
    "        if timestamp_col:\n",
    "            # Try multiple date formats\n",
    "            for date_format in [None, '%Y-%m-%d %H:%M:%S', '%d/%m/%Y %H:%M:%S', '%m/%d/%Y %H:%M:%S']:\n",
    "                try:\n",
    "                    if date_format:\n",
    "                        df['Timestamp'] = pd.to_datetime(df[timestamp_col], format=date_format)\n",
    "                    else:\n",
    "                        df['Timestamp'] = pd.to_datetime(df[timestamp_col], infer_datetime_format=True)\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Set timestamp as index\n",
    "            df.set_index('Timestamp', inplace=True)\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: No timestamp column found in {timeframe} data\")\n",
    "        \n",
    "        # Ensure numeric types for fast operations\n",
    "        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float64)\n",
    "        \n",
    "        # Remove any NaN values in critical columns\n",
    "        critical_cols = ['Open', 'High', 'Low', 'Close']\n",
    "        existing_critical = [col for col in critical_cols if col in df.columns]\n",
    "        \n",
    "        if existing_critical:\n",
    "            initial_len = len(df)\n",
    "            df.dropna(subset=existing_critical, inplace=True)\n",
    "            dropped = initial_len - len(df)\n",
    "            if dropped > 0:\n",
    "                print(f\"📊 Dropped {dropped} rows with NaN values\")\n",
    "        \n",
    "        # Validate OHLC relationships\n",
    "        if all(col in df.columns for col in ['Open', 'High', 'Low', 'Close']):\n",
    "            invalid_ohlc = ((df['High'] < df['Low']) | \n",
    "                           (df['High'] < df['Open']) | \n",
    "                           (df['High'] < df['Close']) | \n",
    "                           (df['Low'] > df['Open']) | \n",
    "                           (df['Low'] > df['Close']))\n",
    "            if invalid_ohlc.any():\n",
    "                print(f\"⚠️ Warning: Found {invalid_ohlc.sum()} rows with invalid OHLC relationships\")\n",
    "                df = df[~invalid_ohlc]\n",
    "        \n",
    "        # Sort index for faster operations\n",
    "        df.sort_index(inplace=True)\n",
    "        \n",
    "        # Check for duplicate timestamps\n",
    "        if df.index.duplicated().any():\n",
    "            print(f\"⚠️ Warning: Found {df.index.duplicated().sum()} duplicate timestamps, keeping first\")\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"✅ Loaded {len(df):,} rows in {load_time:.2f} seconds from {timeframe} file\")\n",
    "        \n",
    "        # Validate the loaded data\n",
    "        validate_dataframe(df, f\"{timeframe} data\", config.min_data_points)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load the data files\n",
    "print(\"Loading data files...\")\n",
    "print(f\"5m data path: {config.data_5m_path}\")\n",
    "print(f\"30m data path: {config.data_30m_path}\")\n",
    "\n",
    "try:\n",
    "    # Load 5-minute data\n",
    "    print(\"\\n📊 Loading 5-minute data...\")\n",
    "    df_5m = load_data_optimized(config.data_5m_path, '5m')\n",
    "    \n",
    "    # Load 30-minute data\n",
    "    print(\"\\n📊 Loading 30-minute data...\")\n",
    "    df_30m = load_data_optimized(config.data_30m_path, '30m')\n",
    "    \n",
    "    # Verify time alignment\n",
    "    print(\"\\n⏰ Verifying time alignment...\")\n",
    "    \n",
    "    # Find overlapping period\n",
    "    start_time = max(df_5m.index[0], df_30m.index[0])\n",
    "    end_time = min(df_5m.index[-1], df_30m.index[-1])\n",
    "    \n",
    "    if start_time >= end_time:\n",
    "        raise ValueError(\"No overlapping time period between 5m and 30m data\")\n",
    "    \n",
    "    # Trim dataframes to overlapping period\n",
    "    df_5m = df_5m[start_time:end_time]\n",
    "    df_30m = df_30m[start_time:end_time]\n",
    "    \n",
    "    print(f\"\\n✅ Aligned data period: {start_time} to {end_time}\")\n",
    "    print(f\"📊 5-minute bars after alignment: {len(df_5m):,}\")\n",
    "    print(f\"📊 30-minute bars after alignment: {len(df_30m):,}\")\n",
    "    \n",
    "    # Verify reasonable ratio\n",
    "    ratio = len(df_5m) / len(df_30m)\n",
    "    expected_ratio = 6  # 30min / 5min\n",
    "    if abs(ratio - expected_ratio) > 1:\n",
    "        print(f\"⚠️ Warning: Unexpected timeframe ratio: {ratio:.2f} (expected ~{expected_ratio})\")\n",
    "    else:\n",
    "        print(f\"✅ Timeframe ratio verified: {ratio:.2f}\")\n",
    "    \n",
    "    print(f\"\\n📅 5-minute data: {df_5m.index[0]} to {df_5m.index[-1]}\")\n",
    "    print(f\"📅 30-minute data: {df_30m.index[0]} to {df_30m.index[-1]}\")\n",
    "    \n",
    "    print(\"\\n✅ Data loading completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Fatal error during data loading: {str(e)}\")\n",
    "    print(\"Cannot proceed with analysis. Please check your data files.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Original Indicator Implementations\n# This cell contains ALL the original, correct indicator functions from Strategy_Implementation.ipynb\n\n# ============================================================================\n# MLMI IMPLEMENTATION SECTION\n# ============================================================================\n\n# Define spec for jitclass\nspec = [\n    ('parameter1', float64[:]),\n    ('parameter2', float64[:]),\n    ('priceArray', float64[:]),\n    ('resultArray', int64[:]),\n    ('size', int64)\n]\n\n# Create a JIT-compiled MLMI data class for maximum performance\n@jitclass(spec)\nclass MLMIDataFast:\n    def __init__(self, max_size=10000):\n        # Pre-allocate arrays with maximum size for better performance\n        self.parameter1 = np.zeros(max_size, dtype=np.float64)\n        self.parameter2 = np.zeros(max_size, dtype=np.float64)\n        self.priceArray = np.zeros(max_size, dtype=np.float64)\n        self.resultArray = np.zeros(max_size, dtype=np.int64)\n        self.size = 0\n    \n    def storePreviousTrade(self, p1, p2, close_price):\n        if self.size > 0:\n            # Calculate result before modifying current values\n            result = 1 if close_price >= self.priceArray[self.size-1] else -1\n            \n            # Increment size and add new entry\n            self.size += 1\n            self.parameter1[self.size-1] = p1\n            self.parameter2[self.size-1] = p2\n            self.priceArray[self.size-1] = close_price\n            self.resultArray[self.size-1] = result\n        else:\n            # First entry\n            self.parameter1[0] = p1\n            self.parameter2[0] = p2\n            self.priceArray[0] = close_price\n            self.resultArray[0] = 0  # Neutral for first entry\n            self.size = 1\n\n@njit(fastmath=True, parallel=True)\ndef wma_numba_fast(series, length):\n    \"\"\"Ultra-optimized Weighted Moving Average calculation\"\"\"\n    n = len(series)\n    result = np.zeros(n, dtype=np.float64)\n    \n    # Pre-calculate weights (constant throughout calculation)\n    weights = np.arange(1, length + 1, dtype=np.float64)\n    sum_weights = np.sum(weights)\n    \n    # Parallel processing of WMA calculation\n    for i in prange(length-1, n):\n        weighted_sum = 0.0\n        # Inline loop for better performance\n        for j in range(length):\n            weighted_sum += series[i-j] * weights[length-j-1]\n        result[i] = weighted_sum / sum_weights\n    \n    return result\n\n@njit(fastmath=True)\ndef calculate_rsi_numba_fast(prices, window):\n    \"\"\"Ultra-optimized RSI calculation\"\"\"\n    n = len(prices)\n    rsi = np.zeros(n, dtype=np.float64)\n    \n    # Pre-allocate arrays for better memory performance\n    delta = np.zeros(n, dtype=np.float64)\n    gain = np.zeros(n, dtype=np.float64)\n    loss = np.zeros(n, dtype=np.float64)\n    avg_gain = np.zeros(n, dtype=np.float64)\n    avg_loss = np.zeros(n, dtype=np.float64)\n    \n    # Calculate deltas in one pass\n    for i in range(1, n):\n        delta[i] = prices[i] - prices[i-1]\n        # Separate gains and losses in the same loop\n        if delta[i] > 0:\n            gain[i] = delta[i]\n        else:\n            loss[i] = -delta[i]\n    \n    # First value uses simple average\n    if window <= n:\n        avg_gain[window-1] = np.sum(gain[:window]) / window\n        avg_loss[window-1] = np.sum(loss[:window]) / window\n        \n        # Calculate RSI for first window point\n        if avg_loss[window-1] == 0:\n            rsi[window-1] = 100\n        else:\n            rs = avg_gain[window-1] / avg_loss[window-1]\n            rsi[window-1] = 100 - (100 / (1 + rs))\n    \n    # Apply Wilder's smoothing for subsequent values with optimized calculation\n    window_minus_one = window - 1\n    window_recip = 1.0 / window\n    for i in range(window, n):\n        avg_gain[i] = (avg_gain[i-1] * window_minus_one + gain[i]) * window_recip\n        avg_loss[i] = (avg_loss[i-1] * window_minus_one + loss[i]) * window_recip\n        \n        # Calculate RSI directly\n        if avg_loss[i] == 0:\n            rsi[i] = 100\n        else:\n            rs = avg_gain[i] / avg_loss[i]\n            rsi[i] = 100 - (100 / (1 + rs))\n    \n    return rsi\n\n# Use cKDTree for lightning-fast kNN queries\ndef fast_knn_predict(param1_array, param2_array, result_array, p1, p2, k, size):\n    \"\"\"\n    Ultra-fast kNN prediction using scipy.spatial.cKDTree\n    \"\"\"\n    # Handle empty data case\n    if size == 0:\n        return 0\n    \n    # Create points array for KDTree\n    points = np.column_stack((param1_array[:size], param2_array[:size]))\n    \n    # Create KDTree for fast nearest neighbor search\n    tree = cKDTree(points)\n    \n    # Query KDTree for k nearest neighbors\n    distances, indices = tree.query([p1, p2], k=min(k, size))\n    \n    # Get results of nearest neighbors\n    neighbors = result_array[indices]\n    \n    # Return prediction (sum of neighbor results)\n    return np.sum(neighbors)\n\ndef calculate_mlmi_optimized(df, num_neighbors=200, momentum_window=20):\n    \"\"\"\n    Highly optimized MLMI calculation function\n    \"\"\"\n    print(\"Preparing data for MLMI calculation...\")\n    # Get numpy arrays for better performance\n    close_array = df['Close'].values\n    n = len(close_array)\n    \n    # Pre-allocate all output arrays at once\n    ma_quick = np.zeros(n, dtype=np.float64)\n    ma_slow = np.zeros(n, dtype=np.float64)\n    rsi_quick = np.zeros(n, dtype=np.float64)\n    rsi_slow = np.zeros(n, dtype=np.float64)\n    rsi_quick_wma = np.zeros(n, dtype=np.float64)\n    rsi_slow_wma = np.zeros(n, dtype=np.float64)\n    pos = np.zeros(n, dtype=np.bool_)\n    neg = np.zeros(n, dtype=np.bool_)\n    mlmi_values = np.zeros(n, dtype=np.float64)\n    \n    print(\"Calculating RSI and moving averages...\")\n    # Calculate indicators with optimized functions\n    ma_quick = wma_numba_fast(close_array, 5)\n    ma_slow = wma_numba_fast(close_array, 20)\n    \n    # Calculate RSI with optimized function\n    rsi_quick = calculate_rsi_numba_fast(close_array, 5)\n    rsi_slow = calculate_rsi_numba_fast(close_array, 20)\n    \n    # Apply WMA to RSI values\n    rsi_quick_wma = wma_numba_fast(rsi_quick, momentum_window)\n    rsi_slow_wma = wma_numba_fast(rsi_slow, momentum_window)\n    \n    # Detect MA crossovers (vectorized where possible)\n    print(\"Detecting moving average crossovers...\")\n    for i in range(1, n):\n        if ma_quick[i] > ma_slow[i] and ma_quick[i-1] <= ma_slow[i-1]:\n            pos[i] = True\n        if ma_quick[i] < ma_slow[i] and ma_quick[i-1] >= ma_slow[i-1]:\n            neg[i] = True\n    \n    # Initialize optimized MLMI data object\n    mlmi_data = MLMIDataFast(max_size=min(10000, n))  # Pre-allocate with reasonable size\n    \n    print(\"Processing crossovers and calculating MLMI values...\")\n    # Process data with batch processing for performance\n    crossover_indices = np.where(pos | neg)[0]\n    \n    # Process crossovers in a single pass\n    for i in crossover_indices:\n        if not np.isnan(rsi_slow_wma[i]) and not np.isnan(rsi_quick_wma[i]):\n            mlmi_data.storePreviousTrade(\n                rsi_slow_wma[i],\n                rsi_quick_wma[i],\n                close_array[i]\n            )\n    \n    # Batch kNN predictions for performance\n    # Only calculate for points after momentum_window\n    for i in range(momentum_window, n):\n        if not np.isnan(rsi_slow_wma[i]) and not np.isnan(rsi_quick_wma[i]):\n            # Use fast KDTree-based kNN prediction\n            if mlmi_data.size > 0:\n                mlmi_values[i] = fast_knn_predict(\n                    mlmi_data.parameter1,\n                    mlmi_data.parameter2,\n                    mlmi_data.resultArray,\n                    rsi_slow_wma[i],\n                    rsi_quick_wma[i],\n                    num_neighbors,\n                    mlmi_data.size\n                )\n    \n    # Add results to dataframe (do this all at once)\n    df_result = df.copy()\n    df_result['ma_quick'] = ma_quick\n    df_result['ma_slow'] = ma_slow\n    df_result['rsi_quick'] = rsi_quick\n    df_result['rsi_slow'] = rsi_slow\n    df_result['rsi_quick_wma'] = rsi_quick_wma\n    df_result['rsi_slow_wma'] = rsi_slow_wma\n    df_result['pos'] = pos\n    df_result['neg'] = neg\n    df_result['mlmi'] = mlmi_values\n    \n    # Calculate WMA of MLMI\n    df_result['mlmi_ma'] = wma_numba_fast(mlmi_values, 20)\n    \n    # Calculate bands and other derived values\n    print(\"Calculating bands and crossovers...\")\n    \n    # Use vectorized operations for bands calculation\n    highest_values = pd.Series(mlmi_values).rolling(window=2000, min_periods=1).max().values\n    lowest_values = pd.Series(mlmi_values).rolling(window=2000, min_periods=1).min().values\n    mlmi_std = pd.Series(mlmi_values).rolling(window=20).std().values\n    ema_std = pd.Series(mlmi_std).ewm(span=20).mean().values\n    \n    # Add band values to dataframe\n    df_result['upper'] = highest_values\n    df_result['lower'] = lowest_values\n    df_result['upper_band'] = highest_values - ema_std\n    df_result['lower_band'] = lowest_values + ema_std\n    \n    # Generate crossover signals (vectorized where possible)\n    mlmi_bull_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_bear_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_ob_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_ob_exit = np.zeros(n, dtype=np.bool_)\n    mlmi_os_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_os_exit = np.zeros(n, dtype=np.bool_)\n    mlmi_mid_up = np.zeros(n, dtype=np.bool_)\n    mlmi_mid_down = np.zeros(n, dtype=np.bool_)\n    \n    # Calculate crossovers in one pass for better performance\n    for i in range(1, n):\n        if not np.isnan(mlmi_values[i]) and not np.isnan(mlmi_values[i-1]):\n            # MA crossovers\n            if mlmi_values[i] > df_result['mlmi_ma'].iloc[i] and mlmi_values[i-1] <= df_result['mlmi_ma'].iloc[i-1]:\n                mlmi_bull_cross[i] = True\n            if mlmi_values[i] < df_result['mlmi_ma'].iloc[i] and mlmi_values[i-1] >= df_result['mlmi_ma'].iloc[i-1]:\n                mlmi_bear_cross[i] = True\n                \n            # Overbought/Oversold crossovers\n            if mlmi_values[i] > df_result['upper_band'].iloc[i] and mlmi_values[i-1] <= df_result['upper_band'].iloc[i-1]:\n                mlmi_ob_cross[i] = True\n            if mlmi_values[i] < df_result['upper_band'].iloc[i] and mlmi_values[i-1] >= df_result['upper_band'].iloc[i-1]:\n                mlmi_ob_exit[i] = True\n            if mlmi_values[i] < df_result['lower_band'].iloc[i] and mlmi_values[i-1] >= df_result['lower_band'].iloc[i-1]:\n                mlmi_os_cross[i] = True\n            if mlmi_values[i] > df_result['lower_band'].iloc[i] and mlmi_values[i-1] <= df_result['lower_band'].iloc[i-1]:\n                mlmi_os_exit[i] = True\n                \n            # Zero-line crosses\n            if mlmi_values[i] > 0 and mlmi_values[i-1] <= 0:\n                mlmi_mid_up[i] = True\n            if mlmi_values[i] < 0 and mlmi_values[i-1] >= 0:\n                mlmi_mid_down[i] = True\n    \n    # Add crossover signals to dataframe\n    df_result['mlmi_bull_cross'] = mlmi_bull_cross\n    df_result['mlmi_bear_cross'] = mlmi_bear_cross\n    df_result['mlmi_ob_cross'] = mlmi_ob_cross\n    df_result['mlmi_ob_exit'] = mlmi_ob_exit\n    df_result['mlmi_os_cross'] = mlmi_os_cross\n    df_result['mlmi_os_exit'] = mlmi_os_exit\n    df_result['mlmi_mid_up'] = mlmi_mid_up\n    df_result['mlmi_mid_down'] = mlmi_mid_down\n    \n    # Count signals\n    bull_crosses = np.sum(mlmi_bull_cross)\n    bear_crosses = np.sum(mlmi_bear_cross)\n    ob_cross = np.sum(mlmi_ob_cross)\n    ob_exit = np.sum(mlmi_ob_exit)\n    os_cross = np.sum(mlmi_os_cross)\n    os_exit = np.sum(mlmi_os_exit)\n    zero_up = np.sum(mlmi_mid_up)\n    zero_down = np.sum(mlmi_mid_down)\n    \n    print(f\"\\nMLMI Signal Summary:\")\n    print(f\"- Bullish MA Crosses: {bull_crosses}\")\n    print(f\"- Bearish MA Crosses: {bear_crosses}\")\n    print(f\"- Overbought Crosses: {ob_cross}\")\n    print(f\"- Overbought Exits: {ob_exit}\")\n    print(f\"- Oversold Crosses: {os_cross}\")\n    print(f\"- Oversold Exits: {os_exit}\")\n    print(f\"- Zero Line Crosses Up: {zero_up}\")\n    print(f\"- Zero Line Crosses Down: {zero_down}\")\n    \n    return df_result\n\n# ============================================================================\n# FVG IMPLEMENTATION SECTION\n# ============================================================================\n\ndef detect_fvg(df, lookback_period=10, body_multiplier=1.5):\n    \"\"\"\n    Detects Fair Value Gaps (FVGs) in historical price data.\n    \n    Parameters:\n        df (DataFrame): DataFrame with OHLC data\n        lookback_period (int): Number of candles to look back for average body size\n        body_multiplier (float): Multiplier to determine significant body size\n        \n    Returns:\n        list: List of FVG tuples or None values\n    \"\"\"\n    # Create a list to store FVG results\n    fvg_list = [None] * len(df)\n    \n    # Can't form FVG with fewer than 3 candles\n    if len(df) < 3:\n        print(\"Warning: Not enough data points to detect FVGs\")\n        return fvg_list\n    \n    # Start from the third candle (index 2)\n    for i in range(2, len(df)):\n        try:\n            # Get the prices for three consecutive candles\n            first_high = df['High'].iloc[i-2]\n            first_low = df['Low'].iloc[i-2]\n            middle_open = df['Open'].iloc[i-1]\n            middle_close = df['Close'].iloc[i-1]\n            third_low = df['Low'].iloc[i]\n            third_high = df['High'].iloc[i]\n            \n            # Calculate average body size from lookback period\n            start_idx = max(0, i-1-lookback_period)\n            prev_bodies = (df['Close'].iloc[start_idx:i-1] - df['Open'].iloc[start_idx:i-1]).abs()\n            avg_body_size = prev_bodies.mean() if not prev_bodies.empty else 0.001\n            avg_body_size = max(avg_body_size, 0.001)  # Avoid division by zero\n            \n            # Calculate current middle candle body size\n            middle_body = abs(middle_close - middle_open)\n            \n            # Check for Bullish FVG (gap up)\n            if third_low > first_high and middle_body > avg_body_size * body_multiplier:\n                fvg_list[i] = ('bullish', first_high, third_low, i)\n                \n            # Check for Bearish FVG (gap down)\n            elif third_high < first_low and middle_body > avg_body_size * body_multiplier:\n                fvg_list[i] = ('bearish', first_low, third_high, i)\n                \n        except Exception as e:\n            # Skip this candle if there's an error\n            continue\n    \n    return fvg_list\n\ndef process_fvg_active_zones(df, fvg_list, validity_bars=20):\n    \"\"\"\n    Process FVG list to create active zone boolean arrays.\n    Properly tracks individual FVGs and their invalidation.\n    \"\"\"\n    n = len(df)\n    is_bull_fvg_active = np.zeros(n, dtype=bool)\n    is_bear_fvg_active = np.zeros(n, dtype=bool)\n    \n    # Track active FVGs with proper structure\n    active_bull_fvgs = []  # List of tuples: (lower_level, upper_level, start_idx)\n    active_bear_fvgs = []  # List of tuples: (upper_level, lower_level, start_idx)\n    \n    for i in range(n):\n        # Add new FVGs to active list\n        if fvg_list[i] is not None:\n            fvg_type, level1, level2, idx = fvg_list[i]\n            if fvg_type == 'bullish':\n                # For bullish FVG: level1 is first candle high, level2 is third candle low\n                active_bull_fvgs.append((level1, level2, idx))\n            else:\n                # For bearish FVG: level1 is first candle low, level2 is third candle high\n                active_bear_fvgs.append((level1, level2, idx))\n        \n        # Check active bullish FVGs\n        new_active_bull = []\n        for lower_level, upper_level, start_idx in active_bull_fvgs:\n            # FVG remains valid if:\n            # 1. Within validity period\n            # 2. Price hasn't closed below the lower level (invalidation)\n            if i - start_idx < validity_bars and df['Close'].iloc[i] > lower_level:\n                is_bull_fvg_active[i] = True\n                new_active_bull.append((lower_level, upper_level, start_idx))\n        active_bull_fvgs = new_active_bull\n        \n        # Check active bearish FVGs\n        new_active_bear = []\n        for upper_level, lower_level, start_idx in active_bear_fvgs:\n            # FVG remains valid if:\n            # 1. Within validity period\n            # 2. Price hasn't closed above the upper level (invalidation)\n            if i - start_idx < validity_bars and df['Close'].iloc[i] < upper_level:\n                is_bear_fvg_active[i] = True\n                new_active_bear.append((upper_level, lower_level, start_idx))\n        active_bear_fvgs = new_active_bear\n    \n    return is_bull_fvg_active, is_bear_fvg_active\n\n# ============================================================================\n# NW-RQK IMPLEMENTATION SECTION\n# ============================================================================\n\n@njit(float64(float64[:], int64, float64, float64, int64))\ndef kernel_regression_numba(src, size, h_param, r_param, x_0):\n    \"\"\"\n    Numba-optimized Nadaraya-Watson Regression using Rational Quadratic Kernel\n    \"\"\"\n    current_weight = 0.0\n    cumulative_weight = 0.0\n    \n    # Calculate only up to the available data points\n    for i in range(min(size + x_0 + 1, len(src))):\n        if i < len(src):\n            y = src[i]  # Value i bars back\n            # Rational Quadratic Kernel\n            w = (1 + (i**2 / ((h_param**2) * 2 * r_param)))**(-r_param)\n            current_weight += y * w\n            cumulative_weight += w\n    \n    if cumulative_weight == 0:\n        return np.nan\n    \n    return current_weight / cumulative_weight\n\n@njit(parallel=True)\ndef calculate_nw_regression(prices, h_param, h_lag_param, r_param, x_0_param):\n    \"\"\"\n    Calculate Nadaraya-Watson regression for the entire price series\n    \"\"\"\n    n = len(prices)\n    yhat1 = np.full(n, np.nan)\n    yhat2 = np.full(n, np.nan)\n    \n    # Reverse the array once to match PineScript indexing\n    prices_reversed = np.zeros(n)\n    for i in range(n):\n        prices_reversed[i] = prices[n-i-1]\n    \n    # Calculate regression values for each bar in parallel\n    for i in prange(n):\n        if i >= x_0_param:  # Only start calculation after x_0 bars\n            # Create window for current bar\n            window_size = min(i + 1, n)\n            src = np.zeros(window_size)\n            for j in range(window_size):\n                src[j] = prices[i-j]\n            \n            yhat1[i] = kernel_regression_numba(src, i, h_param, r_param, x_0_param)\n            yhat2[i] = kernel_regression_numba(src, i, h_lag_param, r_param, x_0_param)\n    \n    return yhat1, yhat2\n\n@njit\ndef detect_crosses(yhat1, yhat2):\n    \"\"\"\n    Detect crossovers between two series\n    \"\"\"\n    n = len(yhat1)\n    bullish_cross = np.zeros(n, dtype=np.bool_)\n    bearish_cross = np.zeros(n, dtype=np.bool_)\n    \n    for i in range(1, n):\n        if not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]) and \\\n           not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]):\n            # Bullish cross (yhat2 crosses above yhat1)\n            if yhat2[i] > yhat1[i] and yhat2[i-1] <= yhat1[i-1]:\n                bullish_cross[i] = True\n            \n            # Bearish cross (yhat2 crosses below yhat1)\n            if yhat2[i] < yhat1[i] and yhat2[i-1] >= yhat1[i-1]:\n                bearish_cross[i] = True\n    \n    return bullish_cross, bearish_cross\n\ndef calculate_nw_rqk(df, src_col='Close', h=8.0, r=8.0, x_0=25, lag=2, smooth_colors=False):\n    \"\"\"\n    Calculate Nadaraya-Watson RQK indicator for a dataframe\n    \"\"\"\n    print(\"Calculating Nadaraya-Watson Regression with Rational Quadratic Kernel...\")\n    \n    # Convert to numpy array for Numba\n    prices = df[src_col].values\n    \n    # Calculate regression values using Numba\n    yhat1, yhat2 = calculate_nw_regression(prices, h, h-lag, r, x_0)\n    \n    # Add regression values to dataframe\n    df['yhat1'] = yhat1\n    df['yhat2'] = yhat2\n    \n    # Calculate rates of change (vectorized)\n    df['wasBearish'] = df['yhat1'].shift(2) > df['yhat1'].shift(1)\n    df['wasBullish'] = df['yhat1'].shift(2) < df['yhat1'].shift(1)\n    df['isBearish'] = df['yhat1'].shift(1) > df['yhat1']\n    df['isBullish'] = df['yhat1'].shift(1) < df['yhat1']\n    df['isBearishChange'] = df['isBearish'] & df['wasBullish']\n    df['isBullishChange'] = df['isBullish'] & df['wasBearish']\n    \n    # Calculate crossovers using Numba\n    bullish_cross, bearish_cross = detect_crosses(yhat1, yhat2)\n    df['isBullishCross'] = bullish_cross\n    df['isBearishCross'] = bearish_cross\n    \n    # Calculate smooth color conditions (vectorized)\n    df['isBullishSmooth'] = df['yhat2'] > df['yhat1']\n    df['isBearishSmooth'] = df['yhat2'] < df['yhat1']\n    \n    # Define colors (matches PineScript)\n    c_bullish = '#3AFF17'  # Green\n    c_bearish = '#FD1707'  # Red\n    \n    # Determine plot colors based on settings (vectorized)\n    df['colorByCross'] = np.where(df['isBullishSmooth'], c_bullish, c_bearish)\n    df['colorByRate'] = np.where(df['isBullish'], c_bullish, c_bearish)\n    df['plotColor'] = df['colorByCross'] if smooth_colors else df['colorByRate']\n    \n    # Calculate alert conditions (vectorized)\n    df['alertBullish'] = df['isBearishCross'] if smooth_colors else df['isBearishChange']\n    df['alertBearish'] = df['isBullishCross'] if smooth_colors else df['isBullishChange']\n    \n    # Generate alert stream (-1 for bearish, 1 for bullish, 0 for no change) (vectorized)\n    df['alertStream'] = np.where(df['alertBearish'], -1,\n                                np.where(df['alertBullish'], 1, 0))\n    \n    # Count signals\n    bullish_changes = df['isBullishChange'].sum()\n    bearish_changes = df['isBearishChange'].sum()\n    bullish_crosses = df['isBullishCross'].sum()\n    bearish_crosses = df['isBearishCross'].sum()\n    \n    print(f\"\\nNW-RQK Signal Summary:\")\n    print(f\"- Bullish Rate Changes: {bullish_changes}\")\n    print(f\"- Bearish Rate Changes: {bearish_changes}\")\n    print(f\"- Bullish Crosses: {bullish_crosses}\")\n    print(f\"- Bearish Crosses: {bearish_crosses}\")\n    \n    return df\n\nprint(\"✅ All original indicator implementations loaded successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Strategy Execution\n# This cell performs all calculations in the correct sequence\n\nprint(\"=\" * 80)\nprint(\"EXECUTING SYNERGY STRATEGY CALCULATIONS\")\nprint(\"=\" * 80)\n\n# Step 1: Calculate MLMI on 30m data\nprint(\"\\n📊 Step 1: Calculating MLMI on 30-minute data...\")\ndf_30m = calculate_mlmi_optimized(\n    df_30m.copy(),\n    num_neighbors=config.mlmi_k_neighbors,\n    momentum_window=config.mlmi_rsi_smooth_period\n)\n\n# CRITICAL FIX: Use MLMI threshold crossings, not MA crossovers\n# The MLMI value itself crossing zero is the signal, not the MA crossovers\ndf_30m['mlmi_bull'] = (df_30m['mlmi'] > config.mlmi_threshold) & (df_30m['mlmi'].shift(1) <= config.mlmi_threshold)\ndf_30m['mlmi_bear'] = (df_30m['mlmi'] < config.mlmi_threshold) & (df_30m['mlmi'].shift(1) >= config.mlmi_threshold)\n\nprint(f\"✅ MLMI calculation complete\")\nprint(f\"   - Valid MLMI values: {(~df_30m['mlmi'].isna()).sum():,}\")\nprint(f\"   - MLMI zero-crossing bull signals: {df_30m['mlmi_bull'].sum():,}\")\nprint(f\"   - MLMI zero-crossing bear signals: {df_30m['mlmi_bear'].sum():,}\")\n\n# Step 2: Calculate NW-RQK on 30m data\nprint(\"\\n📊 Step 2: Calculating NW-RQK on 30-minute data...\")\ndf_30m = calculate_nw_rqk(\n    df_30m,\n    src_col='Close',\n    h=config.nwrqk_h,\n    r=config.nwrqk_r,\n    x_0=config.nwrqk_x_0,\n    lag=config.nwrqk_lag,\n    smooth_colors=config.nwrqk_smooth_colors\n)\n\nprint(f\"✅ NW-RQK calculation complete\")\nprint(f\"   - Valid yhat1 values: {(~df_30m['yhat1'].isna()).sum():,}\")\nprint(f\"   - Bullish changes: {df_30m['isBullishChange'].sum():,}\")\nprint(f\"   - Bearish changes: {df_30m['isBearishChange'].sum():,}\")\n\n# Step 3: Calculate FVG on 5m data\nprint(\"\\n📊 Step 3: Detecting FVG on 5-minute data...\")\nfvg_list = detect_fvg(\n    df_5m,\n    lookback_period=config.fvg_lookback,\n    body_multiplier=config.fvg_body_multiplier\n)\n\n# Process FVG active zones\nis_bull_fvg_active, is_bear_fvg_active = process_fvg_active_zones(\n    df_5m,\n    fvg_list,\n    validity_bars=config.fvg_validity\n)\n\n# Add FVG signals to 5m dataframe\ndf_5m['fvg_bull'] = is_bull_fvg_active\ndf_5m['fvg_bear'] = is_bear_fvg_active\n\n# Count FVG detections\nfvg_bull_count = sum(1 for fvg in fvg_list if fvg is not None and fvg[0] == 'bullish')\nfvg_bear_count = sum(1 for fvg in fvg_list if fvg is not None and fvg[0] == 'bearish')\n\nprint(f\"✅ FVG detection complete\")\nprint(f\"   - Bullish FVGs detected: {fvg_bull_count:,}\")\nprint(f\"   - Bearish FVGs detected: {fvg_bear_count:,}\")\nprint(f\"   - Active bullish zones: {is_bull_fvg_active.sum():,}\")\nprint(f\"   - Active bearish zones: {is_bear_fvg_active.sum():,}\")\n\n# Step 4: Align 30m indicators to 5m timeframe\nprint(\"\\n⏰ Step 4: Aligning 30m indicators to 5m timeframe...\")\n\n# Create the strategy dataframe starting with 5m data\ndf_strategy = df_5m.copy()\n\n# SIMPLIFIED ALIGNMENT: Use pandas reindex with forward fill\n# This ensures each 5m bar has the corresponding 30m indicator values\nindicators_to_align = [\n    'mlmi', 'mlmi_bull', 'mlmi_bear',  # MLMI indicators\n    'isBullishChange', 'isBearishChange',  # NW-RQK change signals (correct ones)\n    'yhat1', 'yhat2',  # NW-RQK regression values\n    'mlmi_ma', 'upper_band', 'lower_band'  # Additional MLMI bands for analysis\n]\n\nfor indicator in indicators_to_align:\n    if indicator in df_30m.columns:\n        # Simple pandas reindex - much cleaner than complex timestamp mapping\n        aligned = df_30m[indicator].reindex(df_strategy.index, method='ffill')\n        df_strategy[f'{indicator}_30m'] = aligned\n\nprint(f\"✅ Timeframe alignment complete\")\nprint(f\"   - Strategy dataframe shape: {df_strategy.shape}\")\nprint(f\"   - Aligned indicators: {len(indicators_to_align)}\")\n\n# Step 5: Generate Synergy Signals (MLMI → FVG → NW-RQK)\nprint(\"\\n🎯 Step 5: Generating synergy signals with simplified logic...\")\n\n# Initialize signal arrays\nn = len(df_strategy)\nlong_entry = np.zeros(n, dtype=bool)\nshort_entry = np.zeros(n, dtype=bool)\n\n# SIMPLIFIED SYNERGY DETECTION: Window-based approach\n# Look back for MLMI signal, check current FVG zone, confirm with NW-RQK change\nfor i in range(config.synergy_window, n):\n    # Look back within the synergy window for MLMI zero-crossing signals\n    mlmi_bull_window = df_strategy['mlmi_bull_30m'].iloc[i-config.synergy_window:i+1]\n    mlmi_bear_window = df_strategy['mlmi_bear_30m'].iloc[i-config.synergy_window:i+1]\n    \n    # Check if we had MLMI signal in the lookback window\n    had_mlmi_bull = mlmi_bull_window.any()\n    had_mlmi_bear = mlmi_bear_window.any()\n    \n    # Check current FVG zone status (must be in active zone)\n    in_bull_fvg = df_strategy['fvg_bull'].iloc[i]\n    in_bear_fvg = df_strategy['fvg_bear'].iloc[i]\n    \n    # Check NW-RQK change confirmation (using the correct change signals)\n    nwrqk_bull_confirm = df_strategy['isBullishChange_30m'].iloc[i]\n    nwrqk_bear_confirm = df_strategy['isBearishChange_30m'].iloc[i]\n    \n    # Simple AND logic for synergy signals\n    if had_mlmi_bull and in_bull_fvg and nwrqk_bull_confirm:\n        long_entry[i] = True\n    \n    if had_mlmi_bear and in_bear_fvg and nwrqk_bear_confirm:\n        short_entry[i] = True\n\n# Add signals to dataframe\ndf_strategy['long_entry'] = long_entry\ndf_strategy['short_entry'] = short_entry\n\n# Count total signals\ntotal_long_signals = long_entry.sum()\ntotal_short_signals = short_entry.sum()\n\nprint(f\"✅ Synergy signal generation complete\")\nprint(f\"   - Long entry signals: {total_long_signals:,}\")\nprint(f\"   - Short entry signals: {total_short_signals:,}\")\nprint(f\"   - Total signals: {total_long_signals + total_short_signals:,}\")\n\n# Display sample of signals if any exist\nif total_long_signals > 0 or total_short_signals > 0:\n    print(\"\\n📋 Sample of generated signals:\")\n    signal_mask = df_strategy['long_entry'] | df_strategy['short_entry']\n    sample_signals = df_strategy[signal_mask].head(5)\n    print(sample_signals[['Close', 'long_entry', 'short_entry', 'mlmi_30m', 'fvg_bull', 'fvg_bear']])\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✅ STRATEGY EXECUTION COMPLETE\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Backtesting with VectorBT\n",
    "# This cell runs the backtest on the generated signals\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BACKTESTING SYNERGY STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data for backtesting\n",
    "print(\"\\n📊 Preparing data for backtest...\")\n",
    "\n",
    "# Extract price and signal data\n",
    "close_prices = df_strategy['Close'].values\n",
    "entries = df_strategy[['long_entry', 'short_entry']].values\n",
    "\n",
    "# Create position sizing arrays\n",
    "size = np.full_like(entries, config.position_size, dtype=np.float64)\n",
    "size_type = 'amount'\n",
    "\n",
    "print(f\"✅ Backtest data prepared\")\n",
    "print(f\"   - Price data points: {len(close_prices):,}\")\n",
    "print(f\"   - Entry signals: {entries.sum():,}\")\n",
    "\n",
    "# Run the backtest\n",
    "print(\"\\n🚀 Running backtest...\")\n",
    "\n",
    "try:\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=close_prices,\n",
    "        entries=entries,\n",
    "        short_entries=df_strategy['short_entry'].values,\n",
    "        size=size,\n",
    "        size_type=size_type,\n",
    "        init_cash=config.initial_capital,\n",
    "        fees=config.fees,\n",
    "        slippage=config.slippage,\n",
    "        freq='5T'\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Backtest completed successfully\")\n",
    "    \n",
    "    # Calculate and display statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    print(\"\\n📈 BACKTEST RESULTS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Initial Capital: ${config.initial_capital:,.2f}\")\n",
    "    print(f\"Final Portfolio Value: ${stats['Total Return [$]'] + config.initial_capital:,.2f}\")\n",
    "    print(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n",
    "    print(f\"\\nTrade Statistics:\")\n",
    "    print(f\"- Total Trades: {stats['Total Trades']:.0f}\")\n",
    "    print(f\"- Win Rate: {stats['Win Rate [%]']:.2f}%\")\n",
    "    print(f\"- Average Win: {stats['Avg Winning Trade [%]']:.2f}%\")\n",
    "    print(f\"- Average Loss: {stats['Avg Losing Trade [%]']:.2f}%\")\n",
    "    print(f\"\\nRisk Metrics:\")\n",
    "    print(f\"- Max Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n",
    "    print(f\"- Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "    print(f\"- Sortino Ratio: {stats['Sortino Ratio']:.2f}\")\n",
    "    print(f\"- Calmar Ratio: {stats['Calmar Ratio']:.2f}\")\n",
    "    \n",
    "    # Store key metrics\n",
    "    backtest_metrics = {\n",
    "        'total_return_pct': stats['Total Return [%]'],\n",
    "        'total_trades': stats['Total Trades'],\n",
    "        'win_rate': stats['Win Rate [%]'],\n",
    "        'sharpe_ratio': stats['Sharpe Ratio'],\n",
    "        'max_drawdown': stats['Max Drawdown [%]']\n",
    "    }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during backtesting: {str(e)}\")\n",
    "    portfolio = None\n",
    "    stats = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualization Dashboard\n",
    "# This cell creates comprehensive visualizations of the strategy\n",
    "\n",
    "if portfolio is not None:\n",
    "    print(\"Creating visualization dashboard...\")\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=5, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.03,\n",
    "        row_heights=[0.3, 0.2, 0.2, 0.2, 0.1],\n",
    "        subplot_titles=(\n",
    "            'Price Action with Signals',\n",
    "            'MLMI Indicator',\n",
    "            'NW-RQK Regression',\n",
    "            'Portfolio Value',\n",
    "            'FVG Active Zones'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Get last 1000 bars for visualization\n",
    "    viz_start = max(0, len(df_strategy) - 1000)\n",
    "    df_viz = df_strategy.iloc[viz_start:]\n",
    "    \n",
    "    # Plot 1: Price with entry signals\n",
    "    fig.add_trace(\n",
    "        go.Candlestick(\n",
    "            x=df_viz.index,\n",
    "            open=df_viz['Open'],\n",
    "            high=df_viz['High'],\n",
    "            low=df_viz['Low'],\n",
    "            close=df_viz['Close'],\n",
    "            name='Price',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add long entry signals\n",
    "    long_signals = df_viz[df_viz['long_entry']]\n",
    "    if len(long_signals) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=long_signals.index,\n",
    "                y=long_signals['Low'] * 0.995,\n",
    "                mode='markers',\n",
    "                marker=dict(symbol='triangle-up', size=10, color='green'),\n",
    "                name='Long Entry'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Add short entry signals\n",
    "    short_signals = df_viz[df_viz['short_entry']]\n",
    "    if len(short_signals) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=short_signals.index,\n",
    "                y=short_signals['High'] * 1.005,\n",
    "                mode='markers',\n",
    "                marker=dict(symbol='triangle-down', size=10, color='red'),\n",
    "                name='Short Entry'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot 2: MLMI Indicator\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_viz.index,\n",
    "            y=df_viz['mlmi_30m'],\n",
    "            mode='lines',\n",
    "            name='MLMI',\n",
    "            line=dict(color='blue', width=2)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add zero line\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=2, col=1)\n",
    "    \n",
    "    # Plot 3: NW-RQK\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_viz.index,\n",
    "            y=df_viz['yhat1_30m'],\n",
    "            mode='lines',\n",
    "            name='NW-RQK',\n",
    "            line=dict(color='purple', width=2)\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 4: Portfolio Value\n",
    "    portfolio_value = portfolio.value()\n",
    "    portfolio_value_viz = portfolio_value.iloc[viz_start:]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=portfolio_value_viz.index,\n",
    "            y=portfolio_value_viz.values,\n",
    "            mode='lines',\n",
    "            name='Portfolio Value',\n",
    "            line=dict(color='orange', width=2),\n",
    "            fill='tozeroy'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 5: FVG Active Zones\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_viz.index,\n",
    "            y=df_viz['fvg_bull'].astype(int),\n",
    "            mode='lines',\n",
    "            name='Bull FVG',\n",
    "            line=dict(color='green', width=1),\n",
    "            fill='tozeroy',\n",
    "            opacity=0.3\n",
    "        ),\n",
    "        row=5, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_viz.index,\n",
    "            y=-df_viz['fvg_bear'].astype(int),\n",
    "            mode='lines',\n",
    "            name='Bear FVG',\n",
    "            line=dict(color='red', width=1),\n",
    "            fill='tozeroy',\n",
    "            opacity=0.3\n",
    "        ),\n",
    "        row=5, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Synergy Strategy 1: MLMI → FVG → NW-RQK Performance Dashboard',\n",
    "        xaxis_title='Date',\n",
    "        height=1400,\n",
    "        showlegend=True,\n",
    "        template='plotly_dark'\n",
    "    )\n",
    "    \n",
    "    # Update y-axis labels\n",
    "    fig.update_yaxes(title_text=\"Price\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"MLMI\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"NW-RQK\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Value ($)\", row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Active\", row=5, col=1)\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\n✅ Visualization complete!\")\n",
    "else:\n",
    "    print(\"⚠️ No portfolio data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Export Results and Summary\n",
    "# This cell exports results and provides a final summary\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STRATEGY EXECUTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create results summary\n",
    "results_summary = {\n",
    "    \"strategy\": \"Synergy 1: MLMI → FVG → NW-RQK\",\n",
    "    \"execution_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"data_period\": {\n",
    "        \"start\": str(df_strategy.index[0]),\n",
    "        \"end\": str(df_strategy.index[-1]),\n",
    "        \"total_bars_5m\": len(df_strategy),\n",
    "        \"total_bars_30m\": len(df_30m)\n",
    "    },\n",
    "    \"signals\": {\n",
    "        \"mlmi_bull_crosses\": int(df_30m['mlmi_bull'].sum()),\n",
    "        \"mlmi_bear_crosses\": int(df_30m['mlmi_bear'].sum()),\n",
    "        \"fvg_bull_zones\": int(df_strategy['fvg_bull'].sum()),\n",
    "        \"fvg_bear_zones\": int(df_strategy['fvg_bear'].sum()),\n",
    "        \"nwrqk_bull_changes\": int(df_30m['isBullishChange'].sum()),\n",
    "        \"nwrqk_bear_changes\": int(df_30m['isBearishChange'].sum()),\n",
    "        \"synergy_long_entries\": int(df_strategy['long_entry'].sum()),\n",
    "        \"synergy_short_entries\": int(df_strategy['short_entry'].sum())\n",
    "    },\n",
    "    \"backtest_results\": backtest_metrics if 'backtest_metrics' in locals() else None,\n",
    "    \"configuration\": {\n",
    "        \"mlmi_neighbors\": config.mlmi_k_neighbors,\n",
    "        \"fvg_validity_bars\": config.fvg_validity,\n",
    "        \"nwrqk_h\": config.nwrqk_h,\n",
    "        \"synergy_window\": config.synergy_window\n",
    "    }\n",
    "}\n",
    "\n",
    "# Export results to JSON\n",
    "results_file = 'synergy_1_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"✅ Results exported to {results_file}\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n📊 FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if portfolio is not None and stats is not None:\n",
    "    print(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {stats['Sharpe Ratio']:.2f}\")\n",
    "    print(f\"Win Rate: {stats['Win Rate [%]']:.2f}%\")\n",
    "    print(f\"Max Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n",
    "    print(f\"Total Trades: {stats['Total Trades']:.0f}\")\n",
    "    \n",
    "    # Performance rating\n",
    "    if stats['Sharpe Ratio'] > 1.5:\n",
    "        rating = \"⭐⭐⭐⭐⭐ Excellent\"\n",
    "    elif stats['Sharpe Ratio'] > 1.0:\n",
    "        rating = \"⭐⭐⭐⭐ Very Good\"\n",
    "    elif stats['Sharpe Ratio'] > 0.5:\n",
    "        rating = \"⭐⭐⭐ Good\"\n",
    "    elif stats['Sharpe Ratio'] > 0:\n",
    "        rating = \"⭐⭐ Fair\"\n",
    "    else:\n",
    "        rating = \"⭐ Needs Improvement\"\n",
    "    \n",
    "    print(f\"\\nPerformance Rating: {rating}\")\n",
    "\n",
    "print(\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(f\"1. The strategy generated {results_summary['signals']['synergy_long_entries'] + results_summary['signals']['synergy_short_entries']} total signals\")\n",
    "print(f\"2. Signal ratio (Long/Short): {results_summary['signals']['synergy_long_entries']}/{results_summary['signals']['synergy_short_entries']}\")\n",
    "\n",
    "if portfolio is not None:\n",
    "    print(f\"3. Average trade duration: {portfolio.stats()['Avg Trade Duration']:.1f}\")\n",
    "    print(f\"4. Profit factor: {portfolio.stats()['Profit Factor']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ SYNERGY STRATEGY 1 EXECUTION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}