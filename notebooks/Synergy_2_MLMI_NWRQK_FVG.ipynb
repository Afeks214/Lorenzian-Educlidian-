{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy 2: MLMI → NW-RQK → FVG Trading Strategy\n",
    "\n",
    "**Ultra-Fast Backtesting with VectorBT and Numba JIT Compilation**\n",
    "\n",
    "This notebook implements the second synergy pattern where:\n",
    "1. MLMI provides the primary trend signal\n",
    "2. NW-RQK confirms the trend direction\n",
    "3. FVG validates the final entry zone\n",
    "\n",
    "Key differences from Synergy 1:\n",
    "- NW-RQK confirmation comes before FVG\n",
    "- May capture different market dynamics\n",
    "- Expected to generate similar trade counts but with different timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup and Imports\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict as TypeDict, Optional, List, Union, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# Scientific computing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Trading and backtesting imports\n",
    "import vectorbt as vbt\n",
    "\n",
    "# Performance optimization imports\n",
    "from numba import njit, prange, typed, types\n",
    "from numba.typed import Dict\n",
    "import numba\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('synergy_strategy.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure Numba for maximum performance\n",
    "numba.config.THREADING_LAYER = 'threadsafe'\n",
    "numba.config.NUMBA_NUM_THREADS = numba.config.NUMBA_DEFAULT_NUM_THREADS\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Version checks\n",
    "logger.info(\"Environment Setup\")\n",
    "logger.info(f\"Python version: {sys.version}\")\n",
    "logger.info(f\"NumPy version: {np.__version__}\")\n",
    "logger.info(f\"Pandas version: {pd.__version__}\")\n",
    "logger.info(f\"VectorBT version: {vbt.__version__}\")\n",
    "logger.info(f\"Numba version: {numba.__version__}\")\n",
    "logger.info(f\"Numba threads: {numba.config.NUMBA_NUM_THREADS}\")\n",
    "\n",
    "print(\"Synergy 2: MLMI → NW-RQK → FVG Strategy\")\n",
    "print(f\"Numba threads: {numba.config.NUMBA_NUM_THREADS}\")\n",
    "print(f\"VectorBT version: {vbt.__version__}\")\n",
    "print(\"Environment ready for ultra-fast backtesting!\")\n",
    "\n",
    "# Configuration dataclass\n",
    "@dataclass\n",
    "class StrategyConfig:\n",
    "    \"\"\"Configuration for the trading strategy\"\"\"\n",
    "    # Data paths\n",
    "    data_path_5m: str = \"/home/QuantNova/AlgoSpace-8/notebooks/notebook data/@CL - 5 min - ETH.csv\"\n",
    "    data_path_30m: str = \"/home/QuantNova/AlgoSpace-8/notebooks/notebook data/@CL - 30 min - ETH.csv\"\n",
    "    \n",
    "    # MLMI parameters\n",
    "    mlmi_k_neighbors: int = 200\n",
    "    mlmi_confidence_threshold: float = 0.3\n",
    "    mlmi_forward_bars: int = 5\n",
    "    \n",
    "    # NW-RQK parameters\n",
    "    nwrqk_h: float = 8.0\n",
    "    nwrqk_r: float = 8.0\n",
    "    nwrqk_lag: int = 2\n",
    "    nwrqk_strength_threshold: float = 0.2\n",
    "    \n",
    "    # FVG parameters\n",
    "    fvg_atr_multiplier: float = 1.5\n",
    "    fvg_active_bars: int = 20\n",
    "    \n",
    "    # Signal parameters\n",
    "    synergy_window: int = 30\n",
    "    \n",
    "    # Backtesting parameters\n",
    "    initial_capital: float = 100000\n",
    "    position_size_base: float = 100\n",
    "    stop_loss_atr: float = 2.0\n",
    "    max_holding_bars: int = 100\n",
    "    fees: float = 0.0001\n",
    "    slippage: float = 0.0001\n",
    "    \n",
    "    # Performance parameters\n",
    "    chunk_size: int = 10000\n",
    "    max_memory_gb: float = 8.0\n",
    "    \n",
    "    # Output parameters\n",
    "    save_results: bool = True\n",
    "    results_path: str = \"./results\"\n",
    "    checkpoint_interval: int = 1000\n",
    "\n",
    "# Create default configuration\n",
    "config = StrategyConfig()\n",
    "\n",
    "# Memory management utilities\n",
    "def check_memory_usage():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_gb = process.memory_info().rss / 1024 / 1024 / 1024\n",
    "        return mem_gb\n",
    "    except ImportError:\n",
    "        logger.warning(\"psutil not installed, memory monitoring disabled\")\n",
    "        return 0.0\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Force garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    logger.info(f\"Memory after cleanup: {check_memory_usage():.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Enhanced Data Loading with Comprehensive Error Handling\n\nclass DataLoadingError(Exception):\n    \"\"\"Custom exception for data loading errors\"\"\"\n    pass\n\nclass DataValidationError(Exception):\n    \"\"\"Custom exception for data validation errors\"\"\"\n    pass\n\ndef inspect_csv_columns(file_path: str, nrows: int = 5) -> None:\n    \"\"\"Inspect CSV columns for debugging\"\"\"\n    try:\n        df_sample = pd.read_csv(file_path, nrows=nrows)\n        print(f\"\\nInspecting file: {file_path}\")\n        print(f\"Columns found: {list(df_sample.columns)}\")\n        print(f\"First row data:\")\n        for col in df_sample.columns:\n            print(f\"  {col}: {df_sample[col].iloc[0]}\")\n    except Exception as e:\n        print(f\"Error inspecting file: {e}\")\n\ndef standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Comprehensive column name standardization that handles many variations\n    \"\"\"\n    # Create a copy to avoid modifying the original\n    df = df.copy()\n    \n    # First, strip whitespace and convert to lowercase for matching\n    df.columns = df.columns.str.strip()\n    \n    # Comprehensive mapping dictionary\n    column_mappings = {\n        # Open variations\n        'open': 'Open', 'o': 'Open', 'open_price': 'Open', 'opening': 'Open',\n        'open price': 'Open', 'opening_price': 'Open', 'o_price': 'Open',\n        \n        # High variations\n        'high': 'High', 'h': 'High', 'high_price': 'High', 'highest': 'High',\n        'high price': 'High', 'highest_price': 'High', 'h_price': 'High',\n        'max': 'High', 'maximum': 'High', 'max_price': 'High',\n        \n        # Low variations\n        'low': 'Low', 'l': 'Low', 'low_price': 'Low', 'lowest': 'Low',\n        'low price': 'Low', 'lowest_price': 'Low', 'l_price': 'Low',\n        'min': 'Low', 'minimum': 'Low', 'min_price': 'Low',\n        \n        # Close variations\n        'close': 'Close', 'c': 'Close', 'close_price': 'Close', 'closing': 'Close',\n        'close price': 'Close', 'closing_price': 'Close', 'c_price': 'Close',\n        'last': 'Close', 'last_price': 'Close', 'final': 'Close',\n        \n        # Volume variations\n        'volume': 'Volume', 'v': 'Volume', 'vol': 'Volume', 'volume_btc': 'Volume',\n        'volume_usd': 'Volume', 'volume_usdt': 'Volume', 'qty': 'Volume',\n        'quantity': 'Volume', 'amount': 'Volume', 'size': 'Volume',\n        \n        # Timestamp variations\n        'timestamp': 'Timestamp', 'datetime': 'Timestamp', 'date': 'Timestamp',\n        'time': 'Timestamp', 'gmt time': 'Timestamp', 'gmt_time': 'Timestamp',\n        'date_time': 'Timestamp', 'utc_time': 'Timestamp', 'utc': 'Timestamp',\n        'index': 'Timestamp', 'date time': 'Timestamp'\n    }\n    \n    # Apply mapping\n    rename_dict = {}\n    for col in df.columns:\n        col_lower = col.lower().strip()\n        if col_lower in column_mappings:\n            rename_dict[col] = column_mappings[col_lower]\n    \n    if rename_dict:\n        df = df.rename(columns=rename_dict)\n        logger.info(f\"Renamed columns: {rename_dict}\")\n    \n    return df\n\ndef infer_missing_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Intelligently infer missing OHLC columns from available data\n    \"\"\"\n    df = df.copy()\n    \n    # Check what we have and what we're missing\n    has_open = 'Open' in df.columns\n    has_high = 'High' in df.columns\n    has_low = 'Low' in df.columns\n    has_close = 'Close' in df.columns\n    \n    # If we have Close but missing others, use Close as base\n    if has_close:\n        if not has_open:\n            logger.warning(\"'Open' column missing - using Close as approximation\")\n            df['Open'] = df['Close']\n        \n        if not has_high:\n            logger.warning(\"'High' column missing - creating from available price data\")\n            price_cols = [col for col in ['Open', 'Close'] if col in df.columns]\n            df['High'] = df[price_cols].max(axis=1) * 1.001  # Slightly above max\n        \n        if not has_low:\n            logger.warning(\"'Low' column missing - creating from available price data\")\n            price_cols = [col for col in ['Open', 'Close'] if col in df.columns]\n            df['Low'] = df[price_cols].min(axis=1) * 0.999  # Slightly below min\n    \n    # If we have some OHLC but missing Close (rare but possible)\n    elif has_open or has_high or has_low:\n        available_price_cols = [col for col in ['Open', 'High', 'Low'] if col in df.columns]\n        if not has_close and available_price_cols:\n            logger.warning(\"'Close' column missing - using available price data\")\n            df['Close'] = df[available_price_cols].mean(axis=1)\n    \n    # If Volume is missing, create dummy volume\n    if 'Volume' not in df.columns:\n        logger.warning(\"'Volume' column missing - creating dummy volume data\")\n        df['Volume'] = 1000000  # Default volume\n    \n    return df\n\ndef validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> None:\n    \"\"\"Validate dataframe has required columns and valid data\"\"\"\n    # Check for required columns\n    missing_columns = set(required_columns) - set(df.columns)\n    if missing_columns:\n        raise DataValidationError(f\"Missing required columns: {missing_columns}\")\n    \n    # Check for empty dataframe\n    if len(df) == 0:\n        raise DataValidationError(\"Dataframe is empty\")\n    \n    # Check for sufficient data\n    if len(df) < 100:\n        logger.warning(f\"Limited data: only {len(df)} rows available\")\n    \n    # Check for NaN values in critical columns\n    critical_columns = ['Open', 'High', 'Low', 'Close']\n    nan_counts = df[critical_columns].isna().sum()\n    if nan_counts.any():\n        logger.warning(f\"NaN values found: {nan_counts.to_dict()}\")\n\ndef load_data_optimized(file_path: str, timeframe: str = '5m', \n                       config: Optional[StrategyConfig] = None) -> pd.DataFrame:\n    \"\"\"Enhanced load function with comprehensive error handling\"\"\"\n    start_time = time.time()\n    logger.info(f\"Loading {timeframe} data from {file_path}\")\n    \n    try:\n        # Check if file exists\n        if not os.path.exists(file_path):\n            raise DataLoadingError(f\"Data file not found: {file_path}\")\n        \n        # First, inspect the file to understand its structure\n        inspect_csv_columns(file_path, nrows=5)\n        \n        # Try multiple encoding options\n        encodings = ['utf-8', 'iso-8859-1', 'cp1252']\n        df = None\n        \n        for encoding in encodings:\n            try:\n                # Read CSV with flexible options\n                df = pd.read_csv(\n                    file_path,\n                    encoding=encoding,\n                    # Don't assume column names yet\n                    header=0,\n                    # Handle various date formats\n                    parse_dates=True,\n                    infer_datetime_format=True,\n                    # Be flexible with data types\n                    dtype=None,\n                    # Handle various separators\n                    sep=None,\n                    engine='python'\n                )\n                logger.info(f\"Successfully read file with {encoding} encoding\")\n                break\n            except UnicodeDecodeError:\n                continue\n            except Exception as e:\n                logger.warning(f\"Failed with {encoding}: {str(e)}\")\n                continue\n        \n        if df is None:\n            raise DataLoadingError(\"Failed to read CSV with any encoding\")\n        \n        # Log original columns for debugging\n        logger.info(f\"Original columns: {list(df.columns)}\")\n        \n        # Step 1: Standardize column names\n        df = standardize_column_names(df)\n        \n        # Step 2: Find and set timestamp index\n        timestamp_col = None\n        for col in df.columns:\n            if col == 'Timestamp' or 'time' in col.lower() or 'date' in col.lower():\n                timestamp_col = col\n                break\n        \n        if timestamp_col and timestamp_col != 'Timestamp':\n            df.rename(columns={timestamp_col: 'Timestamp'}, inplace=True)\n        \n        # Set timestamp as index\n        if 'Timestamp' in df.columns:\n            df['Timestamp'] = pd.to_datetime(df['Timestamp'], dayfirst=True, errors='coerce')\n            df = df.set_index('Timestamp')\n        else:\n            logger.warning(\"No timestamp column found - using sequential index\")\n            # Create a synthetic timestamp based on timeframe\n            if timeframe == '5m':\n                freq = '5T'\n            elif timeframe == '30m':\n                freq = '30T'\n            else:\n                freq = 'T'\n            df.index = pd.date_range(start='2020-01-01', periods=len(df), freq=freq)\n        \n        # Step 3: Infer missing columns\n        df = infer_missing_columns(df)\n        \n        # Step 4: Final validation (more flexible)\n        required_columns = ['Open', 'High', 'Low', 'Close']\n        missing_critical = [col for col in required_columns if col not in df.columns]\n        \n        if missing_critical:\n            # Last resort: try to find any price column\n            price_patterns = ['price', 'close', 'last', 'value', 'rate']\n            price_col = None\n            \n            for col in df.columns:\n                if any(pattern in col.lower() for pattern in price_patterns):\n                    price_col = col\n                    break\n            \n            if price_col:\n                logger.warning(f\"Using '{price_col}' as price data for all OHLC\")\n                for ohlc in ['Open', 'High', 'Low', 'Close']:\n                    if ohlc not in df.columns:\n                        df[ohlc] = df[price_col]\n            else:\n                raise DataValidationError(f\"No price data found in columns: {list(df.columns)}\")\n        \n        # Now validate after all attempts to create required columns\n        validate_dataframe(df, ['Open', 'High', 'Low', 'Close', 'Volume'])\n        \n        # Step 5: Ensure numeric types\n        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n        for col in numeric_cols:\n            if col in df.columns:\n                df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float64)\n        \n        # Step 6: Clean data\n        df = df[df.index.notnull()]\n        initial_len = len(df)\n        df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n        \n        if len(df) < initial_len:\n            logger.warning(f\"Dropped {initial_len - len(df)} rows with NaN values\")\n        \n        # Step 7: Fix OHLC relationships\n        df['High'] = df[['Open', 'High', 'Close']].max(axis=1)\n        df['Low'] = df[['Open', 'Low', 'Close']].min(axis=1)\n        \n        # Step 8: Sort and remove duplicates\n        df.sort_index(inplace=True)\n        df = df[~df.index.duplicated(keep='first')]\n        \n        # Step 9: Add calculated features\n        df['Returns'] = df['Close'].pct_change().fillna(0)\n        df['LogReturns'] = np.log1p(df['Returns'])\n        df['HL_Range'] = df['High'] - df['Low']\n        df['OC_Range'] = abs(df['Open'] - df['Close'])\n        df['DataQuality'] = 1.0\n        df.loc[df['Volume'] == 0, 'DataQuality'] *= 0.8\n        df.loc[df['HL_Range'] == 0, 'DataQuality'] *= 0.9\n        \n        load_time = time.time() - start_time\n        logger.info(f\"Successfully loaded {len(df):,} rows in {load_time:.2f} seconds\")\n        logger.info(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n        logger.info(f\"Final columns: {list(df.columns)}\")\n        logger.info(f\"Average data quality: {df['DataQuality'].mean():.3f}\")\n        \n        # Memory optimization\n        df = df.astype({col: 'float32' for col in numeric_cols if col in df.columns})\n        \n        return df\n        \n    except Exception as e:\n        logger.error(f\"Critical error loading data: {str(e)}\")\n        logger.error(f\"File path: {file_path}\")\n        raise DataLoadingError(f\"Failed to load {timeframe} data: {str(e)}\")\n\n# Pre-compile all Numba functions\nprint(\"Pre-compiling Numba functions for maximum speed...\")\n\n@njit(cache=True)\ndef dummy_compile():\n    \"\"\"Dummy function to trigger compilation\"\"\"\n    return np.array([1.0, 2.0, 3.0]).sum()\n\n_ = dummy_compile()  # Trigger compilation\n\n# Load data files with error handling\nprint(\"\\nLoading data files with enhanced error handling...\")\n\ntry:\n    # Check if config paths exist, otherwise try alternative paths\n    data_paths_5m = [\n        config.data_path_5m,\n        \"./data/@CL - 5 min - ETH.csv\",\n        \"../data/@CL - 5 min - ETH.csv\",\n        \"data/@CL - 5 min - ETH.csv\"\n    ]\n    \n    data_paths_30m = [\n        config.data_path_30m,\n        \"./data/@CL - 30 min - ETH.csv\",\n        \"../data/@CL - 30 min - ETH.csv\",\n        \"data/@CL - 30 min - ETH.csv\"\n    ]\n    \n    # Try to load 5m data\n    df_5m = None\n    for path in data_paths_5m:\n        if os.path.exists(path):\n            try:\n                df_5m = load_data_optimized(path, '5m', config)\n                break\n            except Exception as e:\n                logger.warning(f\"Failed to load from {path}: {e}\")\n                continue\n    \n    if df_5m is None:\n        raise DataLoadingError(\"Could not load 5m data from any path\")\n    \n    # Try to load 30m data\n    df_30m = None\n    for path in data_paths_30m:\n        if os.path.exists(path):\n            try:\n                df_30m = load_data_optimized(path, '30m', config)\n                break\n            except Exception as e:\n                logger.warning(f\"Failed to load from {path}: {e}\")\n                continue\n    \n    if df_30m is None:\n        raise DataLoadingError(\"Could not load 30m data from any path\")\n    \n    # Ensure time alignment\n    common_start = max(df_5m.index[0], df_30m.index[0])\n    common_end = min(df_5m.index[-1], df_30m.index[-1])\n    \n    df_5m = df_5m.loc[common_start:common_end]\n    df_30m = df_30m.loc[common_start:common_end]\n    \n    print(f\"\\n5-minute data: {df_5m.index[0]} to {df_5m.index[-1]} ({len(df_5m):,} bars)\")\n    print(f\"30-minute data: {df_30m.index[0]} to {df_30m.index[-1]} ({len(df_30m):,} bars)\")\n    print(f\"Memory usage: {check_memory_usage():.2f} GB\")\n    \n    # Save checkpoint\n    if config.save_results:\n        os.makedirs(config.results_path, exist_ok=True)\n        checkpoint_path = os.path.join(config.results_path, 'data_checkpoint.pkl')\n        with open(checkpoint_path, 'wb') as f:\n            pickle.dump({\n                'df_5m_shape': df_5m.shape,\n                'df_30m_shape': df_30m.shape,\n                'date_range': (df_5m.index[0], df_5m.index[-1])\n            }, f)\n        logger.info(f\"Saved data checkpoint to {checkpoint_path}\")\n    \nexcept DataLoadingError as e:\n    logger.error(f\"Data loading failed: {e}\")\n    print(f\"\\nERROR: {e}\")\n    print(\"\\nTroubleshooting steps:\")\n    print(\"1. Check that your CSV files have price data columns\")\n    print(\"2. Ensure the file paths are correct\")\n    print(\"3. Verify the CSV format is valid\")\n    print(\"\\nYou can also manually inspect your CSV structure using:\")\n    print(\"pd.read_csv('your_file.csv', nrows=5)\")\n    raise\nexcept Exception as e:\n    logger.error(f\"Unexpected error: {e}\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Original FVG Detection and Optimized Indicators\n\n@njit(fastmath=True, cache=True, parallel=True)\ndef calculate_all_indicators(close: np.ndarray, high: np.ndarray, low: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Calculate all basic indicators with comprehensive error handling\"\"\"\n    n = len(close)\n    \n    # Pre-allocate arrays with default values\n    ma5 = np.full(n, np.nan, dtype=np.float64)\n    ma20 = np.full(n, np.nan, dtype=np.float64)\n    rsi5 = np.full(n, 50.0, dtype=np.float64)\n    rsi20 = np.full(n, 50.0, dtype=np.float64)\n    atr = np.full(n, np.nan, dtype=np.float64)\n    \n    # Input validation\n    if n == 0:\n        return ma5, ma20, rsi5, rsi20, atr\n    \n    # Weighted Moving Averages with safe calculations\n    weights5 = np.arange(1, 6, dtype=np.float64)\n    weights20 = np.arange(1, 21, dtype=np.float64)\n    sum_w5 = weights5.sum()\n    sum_w20 = weights20.sum()\n    \n    # Calculate WMAs in parallel chunks with bounds checking\n    for i in prange(n):\n        # 5-period WMA\n        if i >= 4:\n            window_data = close[i-4:i+1]\n            if not np.any(np.isnan(window_data)):\n                ma5[i] = np.dot(window_data, weights5) / sum_w5\n        \n        # 20-period WMA\n        if i >= 19:\n            window_data = close[i-19:i+1]\n            if not np.any(np.isnan(window_data)):\n                ma20[i] = np.dot(window_data, weights20) / sum_w20\n    \n    # RSI calculation with safe division\n    if n > 1:\n        deltas = np.diff(close)\n        gains = np.maximum(deltas, 0)\n        losses = -np.minimum(deltas, 0)\n        \n        # RSI 5\n        if len(gains) >= 5:\n            avg_gain5 = np.mean(gains[:5])\n            avg_loss5 = np.mean(losses[:5])\n            \n            if avg_loss5 > 0:\n                rs5 = avg_gain5 / avg_loss5\n                rsi5[5] = 100 - (100 / (1 + rs5))\n            else:\n                rsi5[5] = 100 if avg_gain5 > 0 else 50\n            \n            # Calculate remaining RSI values\n            for i in range(5, min(n - 1, len(gains))):\n                avg_gain5 = (avg_gain5 * 4 + gains[i]) / 5\n                avg_loss5 = (avg_loss5 * 4 + losses[i]) / 5\n                \n                if avg_loss5 > 0:\n                    rs5 = avg_gain5 / avg_loss5\n                    rsi5[i + 1] = 100 - (100 / (1 + rs5))\n                else:\n                    rsi5[i + 1] = 100 if avg_gain5 > 0 else 50\n        \n        # RSI 20\n        if len(gains) >= 20:\n            avg_gain20 = np.mean(gains[:20])\n            avg_loss20 = np.mean(losses[:20])\n            \n            if avg_loss20 > 0:\n                rs20 = avg_gain20 / avg_loss20\n                rsi20[20] = 100 - (100 / (1 + rs20))\n            else:\n                rsi20[20] = 100 if avg_gain20 > 0 else 50\n            \n            # Calculate remaining RSI values\n            for i in range(20, min(n - 1, len(gains))):\n                avg_gain20 = (avg_gain20 * 19 + gains[i]) / 20\n                avg_loss20 = (avg_loss20 * 19 + losses[i]) / 20\n                \n                if avg_loss20 > 0:\n                    rs20 = avg_gain20 / avg_loss20\n                    rsi20[i + 1] = 100 - (100 / (1 + rs20))\n                else:\n                    rsi20[i + 1] = 100 if avg_gain20 > 0 else 50\n    \n    # ATR calculation with safe operations\n    if n > 1:\n        # Calculate true range\n        tr = np.zeros(n, dtype=np.float64)\n        tr[0] = high[0] - low[0] if not np.isnan(high[0]) and not np.isnan(low[0]) else 0\n        \n        for i in range(1, n):\n            if not np.isnan(high[i]) and not np.isnan(low[i]) and not np.isnan(close[i-1]):\n                hl = high[i] - low[i]\n                hc = abs(high[i] - close[i-1])\n                lc = abs(low[i] - close[i-1])\n                tr[i] = max(hl, hc, lc)\n            else:\n                tr[i] = 0\n        \n        # Calculate ATR\n        for i in range(14, n):\n            window = tr[i-13:i+1]\n            valid_values = window[window > 0]\n            if len(valid_values) > 0:\n                atr[i] = np.mean(valid_values)\n    \n    return ma5, ma20, rsi5, rsi20, atr\n\ndef detect_fvg(df, lookback_period=10, body_multiplier=1.5):\n    \"\"\"\n    Original FVG detection from Strategy Implementation.ipynb\n    Detects Fair Value Gaps (FVGs) in historical price data.\n    \n    Parameters:\n        df (DataFrame): DataFrame with OHLC data\n        lookback_period (int): Number of candles to look back for average body size\n        body_multiplier (float): Multiplier to determine significant body size\n        \n    Returns:\n        list: List of FVG tuples or None values\n    \"\"\"\n    # Create a list to store FVG results\n    fvg_list = [None] * len(df)\n    \n    # Can't form FVG with fewer than 3 candles\n    if len(df) < 3:\n        print(\"Warning: Not enough data points to detect FVGs\")\n        return fvg_list\n    \n    # Start from the third candle (index 2)\n    for i in range(2, len(df)):\n        try:\n            # Get the prices for three consecutive candles\n            first_high = df['High'].iloc[i-2]\n            first_low = df['Low'].iloc[i-2]\n            middle_open = df['Open'].iloc[i-1]\n            middle_close = df['Close'].iloc[i-1]\n            third_low = df['Low'].iloc[i]\n            third_high = df['High'].iloc[i]\n            \n            # Calculate average body size from lookback period\n            start_idx = max(0, i-1-lookback_period)\n            prev_bodies = (df['Close'].iloc[start_idx:i-1] - df['Open'].iloc[start_idx:i-1]).abs()\n            avg_body_size = prev_bodies.mean() if not prev_bodies.empty else 0.001\n            avg_body_size = max(avg_body_size, 0.001)  # Avoid division by zero\n            \n            # Calculate current middle candle body size\n            middle_body = abs(middle_close - middle_open)\n            \n            # Check for Bullish FVG (gap up)\n            if third_low > first_high and middle_body > avg_body_size * body_multiplier:\n                fvg_list[i] = ('bullish', first_high, third_low, i)\n                \n            # Check for Bearish FVG (gap down)\n            elif third_high < first_low and middle_body > avg_body_size * body_multiplier:\n                fvg_list[i] = ('bearish', first_low, third_high, i)\n                \n        except Exception as e:\n            # Skip this candle if there's an error\n            continue\n    \n    return fvg_list\n\n# FIX: Updated function to accept DataFrame as parameter instead of using global variable\ndef process_fvg_for_strategy(df, fvg_list, n, active_bars=20):\n    \"\"\"Convert FVG list to boolean arrays with active zones\n    \n    Parameters:\n        df (DataFrame): DataFrame with OHLC data\n        fvg_list (list): List of FVG tuples from detect_fvg\n        n (int): Length of the data\n        active_bars (int): Number of bars to keep FVG active\n        \n    Returns:\n        tuple: (bull_active, bear_active) boolean arrays\n    \"\"\"\n    bull_active = np.zeros(n, dtype=np.bool_)\n    bear_active = np.zeros(n, dtype=np.bool_)\n    \n    for i, fvg in enumerate(fvg_list):\n        if fvg is not None:\n            fvg_type, level1, level2, idx = fvg\n            \n            if fvg_type == 'bullish':\n                # Mark active zone for bullish FVG\n                for j in range(i, min(i + active_bars, n)):\n                    bull_active[j] = True\n                    # Check if price breaks below the gap bottom (invalidation)\n                    if j < n and 'Low' in df.columns:\n                        if df.iloc[j]['Low'] < level1:\n                            break\n            \n            elif fvg_type == 'bearish':\n                # Mark active zone for bearish FVG\n                for j in range(i, min(i + active_bars, n)):\n                    bear_active[j] = True\n                    # Check if price breaks above the gap top (invalidation)\n                    if j < n and 'High' in df.columns:\n                        if df.iloc[j]['High'] > level1:\n                            break\n    \n    return bull_active, bear_active\n\n# Safe smoothing function\ndef safe_smooth(data: np.ndarray, window: int = 20) -> np.ndarray:\n    \"\"\"Apply smoothing with NaN handling\"\"\"\n    if len(data) < window:\n        return data\n    \n    # Replace NaN with forward fill for smoothing\n    filled_data = pd.Series(data).ffill().bfill().values\n    \n    # Apply convolution\n    kernel = np.ones(window) / window\n    smoothed = np.convolve(filled_data, kernel, mode='same')\n    \n    # Restore NaN where original data had NaN\n    smoothed[np.isnan(data)] = np.nan\n    \n    return smoothed\n\nprint(\"\\nCalculating all indicators with parallel processing...\")\nlogger.info(\"Starting indicator calculations\")\nstart_time = time.time()\n\ntry:\n    # Calculate 30-minute indicators\n    close_30m = df_30m['Close'].values.astype(np.float64)\n    high_30m = df_30m['High'].values.astype(np.float64)\n    low_30m = df_30m['Low'].values.astype(np.float64)\n    \n    ma5, ma20, rsi5, rsi20, atr_30m = calculate_all_indicators(\n        close_30m, high_30m, low_30m\n    )\n    \n    # Smooth RSI with safety\n    rsi5_smooth = safe_smooth(rsi5, 20)\n    rsi20_smooth = safe_smooth(rsi20, 20)\n    \n    # Calculate 5-minute indicators\n    close_5m = df_5m['Close'].values.astype(np.float64)\n    high_5m = df_5m['High'].values.astype(np.float64)\n    low_5m = df_5m['Low'].values.astype(np.float64)\n    \n    _, _, _, _, atr_5m = calculate_all_indicators(\n        close_5m, high_5m, low_5m\n    )\n    \n    # Detect FVG using original function\n    print(\"Detecting FVGs using original logic...\")\n    fvg_list = detect_fvg(df_5m, lookback_period=10, body_multiplier=1.5)\n    \n    # FIX: Pass DataFrame as parameter to process_fvg_for_strategy\n    fvg_bull, fvg_bear = process_fvg_for_strategy(df_5m, fvg_list, len(df_5m), config.fvg_active_bars)\n    \n    calc_time = time.time() - start_time\n    \n    # Log statistics\n    logger.info(f\"Indicators calculated in {calc_time:.3f} seconds\")\n    logger.info(f\"MA5 valid values: {(~np.isnan(ma5)).sum()}/{len(ma5)}\")\n    logger.info(f\"MA20 valid values: {(~np.isnan(ma20)).sum()}/{len(ma20)}\")\n    logger.info(f\"RSI5 range: [{np.nanmin(rsi5):.1f}, {np.nanmax(rsi5):.1f}]\")\n    logger.info(f\"RSI20 range: [{np.nanmin(rsi20):.1f}, {np.nanmax(rsi20):.1f}]\")\n    logger.info(f\"ATR 30m valid: {(~np.isnan(atr_30m)).sum()}/{len(atr_30m)}\")\n    logger.info(f\"ATR 5m valid: {(~np.isnan(atr_5m)).sum()}/{len(atr_5m)}\")\n    \n    # Count FVGs\n    fvg_count = sum(1 for fvg in fvg_list if fvg is not None)\n    bull_fvg_count = sum(1 for fvg in fvg_list if fvg is not None and fvg[0] == 'bullish')\n    bear_fvg_count = sum(1 for fvg in fvg_list if fvg is not None and fvg[0] == 'bearish')\n    \n    logger.info(f\"FVGs detected - Total: {fvg_count}, Bull: {bull_fvg_count}, Bear: {bear_fvg_count}\")\n    logger.info(f\"FVG zones - Bull: {fvg_bull.sum():,}, Bear: {fvg_bear.sum():,}\")\n    \n    print(f\"All indicators calculated in {calc_time:.3f} seconds\")\n    print(f\"FVGs detected - Total: {fvg_count}, Bullish: {bull_fvg_count}, Bearish: {bear_fvg_count}\")\n    print(f\"FVG zones active - Bull: {fvg_bull.sum():,}, Bear: {fvg_bear.sum():,}\")\n    \n    # Memory cleanup\n    if check_memory_usage() > config.max_memory_gb * 0.8:\n        cleanup_memory()\n    \nexcept Exception as e:\n    logger.error(f\"Error calculating indicators: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Original MLMI Implementation with cKDTree\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit, prange, float64, int64, boolean\nfrom numba.experimental import jitclass\nfrom scipy.spatial import cKDTree  # Using cKDTree for fast kNN\n\n# Define spec for jitclass\nspec = [\n    ('parameter1', float64[:]),\n    ('parameter2', float64[:]),\n    ('priceArray', float64[:]),\n    ('resultArray', int64[:]),\n    ('size', int64)\n]\n\n# Create a JIT-compiled MLMI data class for maximum performance\n@jitclass(spec)\nclass MLMIDataFast:\n    def __init__(self, max_size=10000):\n        # Pre-allocate arrays with maximum size for better performance\n        self.parameter1 = np.zeros(max_size, dtype=np.float64)\n        self.parameter2 = np.zeros(max_size, dtype=np.float64)\n        self.priceArray = np.zeros(max_size, dtype=np.float64)\n        self.resultArray = np.zeros(max_size, dtype=np.int64)\n        self.size = 0\n    \n    def storePreviousTrade(self, p1, p2, close_price):\n        if self.size > 0:\n            # Calculate result before modifying current values\n            result = 1 if close_price >= self.priceArray[self.size-1] else -1\n            \n            # Increment size and add new entry\n            self.size += 1\n            self.parameter1[self.size-1] = p1\n            self.parameter2[self.size-1] = p2\n            self.priceArray[self.size-1] = close_price\n            self.resultArray[self.size-1] = result\n        else:\n            # First entry\n            self.parameter1[0] = p1\n            self.parameter2[0] = p2\n            self.priceArray[0] = close_price\n            self.resultArray[0] = 0  # Neutral for first entry\n            self.size = 1\n\n# Optimized core functions with parallel processing\n@njit(fastmath=True, parallel=True, cache=True)\ndef wma_numba_fast(series, length):\n    \"\"\"Ultra-optimized Weighted Moving Average calculation\"\"\"\n    n = len(series)\n    result = np.zeros(n, dtype=np.float64)\n    \n    # Pre-calculate weights (constant throughout calculation)\n    weights = np.arange(1, length + 1, dtype=np.float64)\n    sum_weights = np.sum(weights)\n    \n    # Parallel processing of WMA calculation\n    for i in prange(length-1, n):\n        weighted_sum = 0.0\n        # Inline loop for better performance\n        for j in range(length):\n            weighted_sum += series[i-j] * weights[length-j-1]\n        result[i] = weighted_sum / sum_weights\n    \n    return result\n\n@njit(fastmath=True, cache=True)\ndef calculate_rsi_numba_fast(prices, window):\n    \"\"\"Ultra-optimized RSI calculation\"\"\"\n    n = len(prices)\n    rsi = np.zeros(n, dtype=np.float64)\n    \n    # Pre-allocate arrays for better memory performance\n    delta = np.zeros(n, dtype=np.float64)\n    gain = np.zeros(n, dtype=np.float64)\n    loss = np.zeros(n, dtype=np.float64)\n    avg_gain = np.zeros(n, dtype=np.float64)\n    avg_loss = np.zeros(n, dtype=np.float64)\n    \n    # Calculate deltas in one pass\n    for i in range(1, n):\n        delta[i] = prices[i] - prices[i-1]\n        # Separate gains and losses in the same loop\n        if delta[i] > 0:\n            gain[i] = delta[i]\n        else:\n            loss[i] = -delta[i]\n    \n    # First value uses simple average\n    if window <= n:\n        avg_gain[window-1] = np.sum(gain[:window]) / window\n        avg_loss[window-1] = np.sum(loss[:window]) / window\n        \n        # Calculate RSI for first window point\n        if avg_loss[window-1] == 0:\n            rsi[window-1] = 100\n        else:\n            rs = avg_gain[window-1] / avg_loss[window-1]\n            rsi[window-1] = 100 - (100 / (1 + rs))\n    \n    # Apply Wilder's smoothing for subsequent values with optimized calculation\n    window_minus_one = window - 1\n    window_recip = 1.0 / window\n    for i in range(window, n):\n        avg_gain[i] = (avg_gain[i-1] * window_minus_one + gain[i]) * window_recip\n        avg_loss[i] = (avg_loss[i-1] * window_minus_one + loss[i]) * window_recip\n        \n        # Calculate RSI directly\n        if avg_loss[i] == 0:\n            rsi[i] = 100\n        else:\n            rs = avg_gain[i] / avg_loss[i]\n            rsi[i] = 100 - (100 / (1 + rs))\n    \n    return rsi\n\n# Use cKDTree for lightning-fast kNN queries\ndef fast_knn_predict(param1_array, param2_array, result_array, p1, p2, k, size):\n    \"\"\"\n    Ultra-fast kNN prediction using scipy.spatial.cKDTree\n    \"\"\"\n    # Handle empty data case\n    if size == 0:\n        return 0\n    \n    # Create points array for KDTree\n    points = np.column_stack((param1_array[:size], param2_array[:size]))\n    \n    # Create KDTree for fast nearest neighbor search\n    tree = cKDTree(points)\n    \n    # Query KDTree for k nearest neighbors\n    distances, indices = tree.query([p1, p2], k=min(k, size))\n    \n    # Get results of nearest neighbors\n    neighbors = result_array[indices]\n    \n    # Return prediction (sum of neighbor results)\n    return np.sum(neighbors)\n\ndef calculate_mlmi_optimized(df, num_neighbors=200, momentum_window=20):\n    \"\"\"\n    Original MLMI calculation function from Strategy Implementation.ipynb\n    \"\"\"\n    print(\"Preparing data for MLMI calculation...\")\n    # Get numpy arrays for better performance\n    close_array = df['Close'].values\n    n = len(close_array)\n    \n    # Pre-allocate all output arrays at once\n    ma_quick = np.zeros(n, dtype=np.float64)\n    ma_slow = np.zeros(n, dtype=np.float64)\n    rsi_quick = np.zeros(n, dtype=np.float64)\n    rsi_slow = np.zeros(n, dtype=np.float64)\n    rsi_quick_wma = np.zeros(n, dtype=np.float64)\n    rsi_slow_wma = np.zeros(n, dtype=np.float64)\n    pos = np.zeros(n, dtype=np.bool_)\n    neg = np.zeros(n, dtype=np.bool_)\n    mlmi_values = np.zeros(n, dtype=np.float64)\n    \n    print(\"Calculating RSI and moving averages...\")\n    # Calculate indicators with optimized functions\n    ma_quick = wma_numba_fast(close_array, 5)\n    ma_slow = wma_numba_fast(close_array, 20)\n    \n    # Calculate RSI with optimized function\n    rsi_quick = calculate_rsi_numba_fast(close_array, 5)\n    rsi_slow = calculate_rsi_numba_fast(close_array, 20)\n    \n    # Apply WMA to RSI values\n    rsi_quick_wma = wma_numba_fast(rsi_quick, momentum_window)\n    rsi_slow_wma = wma_numba_fast(rsi_slow, momentum_window)\n    \n    # Detect MA crossovers (vectorized where possible)\n    print(\"Detecting moving average crossovers...\")\n    for i in range(1, n):\n        if ma_quick[i] > ma_slow[i] and ma_quick[i-1] <= ma_slow[i-1]:\n            pos[i] = True\n        if ma_quick[i] < ma_slow[i] and ma_quick[i-1] >= ma_slow[i-1]:\n            neg[i] = True\n    \n    # Initialize optimized MLMI data object\n    mlmi_data = MLMIDataFast(max_size=min(10000, n))  # Pre-allocate with reasonable size\n    \n    print(\"Processing crossovers and calculating MLMI values...\")\n    # Process data with batch processing for performance\n    crossover_indices = np.where(pos | neg)[0]\n    \n    # Process crossovers in a single pass\n    for i in crossover_indices:\n        if not np.isnan(rsi_slow_wma[i]) and not np.isnan(rsi_quick_wma[i]):\n            mlmi_data.storePreviousTrade(\n                rsi_slow_wma[i],\n                rsi_quick_wma[i],\n                close_array[i]\n            )\n    \n    # Batch kNN predictions for performance\n    # Only calculate for points after momentum_window\n    for i in range(momentum_window, n):\n        if not np.isnan(rsi_slow_wma[i]) and not np.isnan(rsi_quick_wma[i]):\n            # Use fast KDTree-based kNN prediction\n            if mlmi_data.size > 0:\n                mlmi_values[i] = fast_knn_predict(\n                    mlmi_data.parameter1,\n                    mlmi_data.parameter2,\n                    mlmi_data.resultArray,\n                    rsi_slow_wma[i],\n                    rsi_quick_wma[i],\n                    num_neighbors,\n                    mlmi_data.size\n                )\n    \n    # Add results to dataframe (do this all at once)\n    df_result = df.copy()\n    df_result['ma_quick'] = ma_quick\n    df_result['ma_slow'] = ma_slow\n    df_result['rsi_quick'] = rsi_quick\n    df_result['rsi_slow'] = rsi_slow\n    df_result['rsi_quick_wma'] = rsi_quick_wma\n    df_result['rsi_slow_wma'] = rsi_slow_wma\n    df_result['pos'] = pos\n    df_result['neg'] = neg\n    df_result['mlmi'] = mlmi_values\n    \n    # Calculate WMA of MLMI\n    df_result['mlmi_ma'] = wma_numba_fast(mlmi_values, 20)\n    \n    # Calculate bands and other derived values\n    print(\"Calculating bands and crossovers...\")\n    \n    # Use vectorized operations for bands calculation\n    highest_values = pd.Series(mlmi_values).rolling(window=2000, min_periods=1).max().values\n    lowest_values = pd.Series(mlmi_values).rolling(window=2000, min_periods=1).min().values\n    mlmi_std = pd.Series(mlmi_values).rolling(window=20).std().values\n    ema_std = pd.Series(mlmi_std).ewm(span=20).mean().values\n    \n    # Add band values to dataframe\n    df_result['upper'] = highest_values\n    df_result['lower'] = lowest_values\n    df_result['upper_band'] = highest_values - ema_std\n    df_result['lower_band'] = lowest_values + ema_std\n    \n    # Generate crossover signals (vectorized where possible)\n    mlmi_bull_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_bear_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_ob_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_ob_exit = np.zeros(n, dtype=np.bool_)\n    mlmi_os_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_os_exit = np.zeros(n, dtype=np.bool_)\n    mlmi_mid_up = np.zeros(n, dtype=np.bool_)\n    mlmi_mid_down = np.zeros(n, dtype=np.bool_)\n    \n    # Calculate crossovers in one pass for better performance\n    for i in range(1, n):\n        if not np.isnan(mlmi_values[i]) and not np.isnan(mlmi_values[i-1]):\n            # MA crossovers\n            if mlmi_values[i] > df_result['mlmi_ma'].iloc[i] and mlmi_values[i-1] <= df_result['mlmi_ma'].iloc[i-1]:\n                mlmi_bull_cross[i] = True\n            if mlmi_values[i] < df_result['mlmi_ma'].iloc[i] and mlmi_values[i-1] >= df_result['mlmi_ma'].iloc[i-1]:\n                mlmi_bear_cross[i] = True\n                \n            # Overbought/Oversold crossovers\n            if mlmi_values[i] > df_result['upper_band'].iloc[i] and mlmi_values[i-1] <= df_result['upper_band'].iloc[i-1]:\n                mlmi_ob_cross[i] = True\n            if mlmi_values[i] < df_result['upper_band'].iloc[i] and mlmi_values[i-1] >= df_result['upper_band'].iloc[i-1]:\n                mlmi_ob_exit[i] = True\n            if mlmi_values[i] < df_result['lower_band'].iloc[i] and mlmi_values[i-1] >= df_result['lower_band'].iloc[i-1]:\n                mlmi_os_cross[i] = True\n            if mlmi_values[i] > df_result['lower_band'].iloc[i] and mlmi_values[i-1] <= df_result['lower_band'].iloc[i-1]:\n                mlmi_os_exit[i] = True\n                \n            # Zero-line crosses\n            if mlmi_values[i] > 0 and mlmi_values[i-1] <= 0:\n                mlmi_mid_up[i] = True\n            if mlmi_values[i] < 0 and mlmi_values[i-1] >= 0:\n                mlmi_mid_down[i] = True\n    \n    # Add crossover signals to dataframe\n    df_result['mlmi_bull_cross'] = mlmi_bull_cross\n    df_result['mlmi_bear_cross'] = mlmi_bear_cross\n    df_result['mlmi_ob_cross'] = mlmi_ob_cross\n    df_result['mlmi_ob_exit'] = mlmi_ob_exit\n    df_result['mlmi_os_cross'] = mlmi_os_cross\n    df_result['mlmi_os_exit'] = mlmi_os_exit\n    df_result['mlmi_mid_up'] = mlmi_mid_up\n    df_result['mlmi_mid_down'] = mlmi_mid_down\n    \n    # Count signals\n    bull_crosses = np.sum(mlmi_bull_cross)\n    bear_crosses = np.sum(mlmi_bear_cross)\n    ob_cross = np.sum(mlmi_ob_cross)\n    ob_exit = np.sum(mlmi_ob_exit)\n    os_cross = np.sum(mlmi_os_cross)\n    os_exit = np.sum(mlmi_os_exit)\n    zero_up = np.sum(mlmi_mid_up)\n    zero_down = np.sum(mlmi_mid_down)\n    \n    print(f\"\\nMLMI Signal Summary:\")\n    print(f\"- Bullish MA Crosses: {bull_crosses}\")\n    print(f\"- Bearish MA Crosses: {bear_crosses}\")\n    print(f\"- Overbought Crosses: {ob_cross}\")\n    print(f\"- Overbought Exits: {ob_exit}\")\n    print(f\"- Oversold Crosses: {os_cross}\")\n    print(f\"- Oversold Exits: {os_exit}\")\n    print(f\"- Zero Line Crosses Up: {zero_up}\")\n    print(f\"- Zero Line Crosses Down: {zero_down}\")\n    \n    return df_result\n\n# Calculate MLMI using the original function\nprint(\"\\nCalculating original MLMI with kNN pattern matching...\")\nstart_time = time.time()\n\n# Apply the optimized MLMI calculation to the 30-minute data\ndf_30m = calculate_mlmi_optimized(df_30m, num_neighbors=config.mlmi_k_neighbors, momentum_window=20)\n\nmlmi_time = time.time() - start_time\nprint(f\"Original MLMI calculated in {mlmi_time:.3f} seconds\")\n\n# FIX: Use state-based MLMI signals instead of event-based crossovers\n# A bullish state is when MLMI is above its MA, bearish when below\ndf_30m['mlmi_bull'] = df_30m['mlmi'] > df_30m['mlmi_ma']\ndf_30m['mlmi_bear'] = df_30m['mlmi'] < df_30m['mlmi_ma']\n\n# Store MLMI values\nmlmi_values = df_30m['mlmi'].values\n\nprint(f\"MLMI range: [{mlmi_values.min():.1f}, {mlmi_values.max():.1f}]\")\nprint(f\"MLMI bullish signals (above MA): {df_30m['mlmi_bull'].sum():,}\")\nprint(f\"MLMI bearish signals (below MA): {df_30m['mlmi_bear'].sum():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Original NW-RQK Implementation\n\nimport numpy as np\nimport pandas as pd\nfrom numba import jit, njit, prange, float64, boolean\n\n# Define parameters (matching PineScript defaults)\nsrc_col = 'Close'  # Default source is close price\nh = 8.0            # Lookback window\nr = 8.0            # Relative weighting\nx_0 = 25           # Start regression at bar\nlag = 2            # Lag for crossover detection\nsmooth_colors = False  # Smooth colors option\n\n# JIT-compiled kernel regression function\n@njit(float64(float64[:], int64, float64, float64), cache=True)\ndef kernel_regression_numba(src, size, h_param, r_param):\n    \"\"\"\n    Numba-optimized Nadaraya-Watson Regression using Rational Quadratic Kernel\n    \"\"\"\n    current_weight = 0.0\n    cumulative_weight = 0.0\n    \n    # Calculate only up to the available data points\n    for i in range(min(size + x_0 + 1, len(src))):\n        if i < len(src):\n            y = src[i]  # Value i bars back\n            # Rational Quadratic Kernel\n            w = (1 + (i**2 / ((h_param**2) * 2 * r_param)))**(-r_param)\n            current_weight += y * w\n            cumulative_weight += w\n    \n    if cumulative_weight == 0:\n        return np.nan\n    \n    return current_weight / cumulative_weight\n\n# JIT-compiled function to process the entire series\n@njit(parallel=True, cache=True)\ndef calculate_nw_regression(prices, h_param, h_lag_param, r_param, x_0_param):\n    \"\"\"\n    Calculate Nadaraya-Watson regression for the entire price series\n    \"\"\"\n    n = len(prices)\n    yhat1 = np.full(n, np.nan)\n    yhat2 = np.full(n, np.nan)\n    \n    # Reverse the array once to match PineScript indexing\n    prices_reversed = np.zeros(n)\n    for i in range(n):\n        prices_reversed[i] = prices[n-i-1]\n    \n    # Calculate regression values for each bar in parallel\n    for i in prange(n):\n        if i >= x_0_param:  # Only start calculation after x_0 bars\n            # Create window for current bar\n            window_size = min(i + 1, n)\n            src = np.zeros(window_size)\n            for j in range(window_size):\n                src[j] = prices[i-j]\n            \n            yhat1[i] = kernel_regression_numba(src, i, h_param, r_param)\n            yhat2[i] = kernel_regression_numba(src, i, h_param-lag, r_param)\n    \n    return yhat1, yhat2\n\n# JIT-compiled function to detect crossovers\n@njit(cache=True)\ndef detect_crosses(yhat1, yhat2):\n    \"\"\"\n    Detect crossovers between two series\n    \"\"\"\n    n = len(yhat1)\n    bullish_cross = np.zeros(n, dtype=np.bool_)\n    bearish_cross = np.zeros(n, dtype=np.bool_)\n    \n    for i in range(1, n):\n        if not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]) and \\\n           not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]):\n            # Bullish cross (yhat2 crosses above yhat1)\n            if yhat2[i] > yhat1[i] and yhat2[i-1] <= yhat1[i-1]:\n                bullish_cross[i] = True\n            \n            # Bearish cross (yhat2 crosses below yhat1)\n            if yhat2[i] < yhat1[i] and yhat2[i-1] >= yhat1[i-1]:\n                bearish_cross[i] = True\n    \n    return bullish_cross, bearish_cross\n\ndef calculate_nw_rqk(df, src_col='Close', h=8.0, r=8.0, x_0=25, lag=2, smooth_colors=False):\n    \"\"\"\n    Calculate Nadaraya-Watson RQK indicator for a dataframe\n    \"\"\"\n    print(\"Calculating Nadaraya-Watson Regression with Rational Quadratic Kernel...\")\n    \n    # Convert to numpy array for Numba\n    prices = df[src_col].values\n    \n    # Calculate regression values using Numba\n    yhat1, yhat2 = calculate_nw_regression(prices, h, h-lag, r, x_0)\n    \n    # Add regression values to dataframe\n    df['yhat1'] = yhat1\n    df['yhat2'] = yhat2\n    \n    # Calculate rates of change (vectorized)\n    df['wasBearish'] = df['yhat1'].shift(2) > df['yhat1'].shift(1)\n    df['wasBullish'] = df['yhat1'].shift(2) < df['yhat1'].shift(1)\n    df['isBearish'] = df['yhat1'].shift(1) > df['yhat1']\n    df['isBullish'] = df['yhat1'].shift(1) < df['yhat1']\n    df['isBearishChange'] = df['isBearish'] & df['wasBullish']\n    df['isBullishChange'] = df['isBullish'] & df['wasBearish']\n    \n    # Calculate crossovers using Numba\n    bullish_cross, bearish_cross = detect_crosses(yhat1, yhat2)\n    df['isBullishCross'] = bullish_cross\n    df['isBearishCross'] = bearish_cross\n    \n    # Calculate smooth color conditions (vectorized)\n    df['isBullishSmooth'] = df['yhat2'] > df['yhat1']\n    df['isBearishSmooth'] = df['yhat2'] < df['yhat1']\n    \n    # Define colors (matches PineScript)\n    c_bullish = '#3AFF17'  # Green\n    c_bearish = '#FD1707'  # Red\n    \n    # Determine plot colors based on settings (vectorized)\n    df['colorByCross'] = np.where(df['isBullishSmooth'], c_bullish, c_bearish)\n    df['colorByRate'] = np.where(df['isBullish'], c_bullish, c_bearish)\n    df['plotColor'] = df['colorByCross'] if smooth_colors else df['colorByRate']\n    \n    # Calculate alert conditions (vectorized)\n    df['alertBullish'] = df['isBearishCross'] if smooth_colors else df['isBearishChange']\n    df['alertBearish'] = df['isBullishCross'] if smooth_colors else df['isBullishChange']\n    \n    # Generate alert stream (-1 for bearish, 1 for bullish, 0 for no change) (vectorized)\n    df['alertStream'] = np.where(df['alertBearish'], -1,\n                                np.where(df['alertBullish'], 1, 0))\n    \n    # Count signals\n    bullish_changes = df['isBullishChange'].sum()\n    bearish_changes = df['isBearishChange'].sum()\n    bullish_crosses = df['isBullishCross'].sum()\n    bearish_crosses = df['isBearishCross'].sum()\n    \n    print(f\"\\nNW-RQK Signal Summary:\")\n    print(f\"- Bullish Rate Changes: {bullish_changes}\")\n    print(f\"- Bearish Rate Changes: {bearish_changes}\")\n    print(f\"- Bullish Crosses: {bullish_crosses}\")\n    print(f\"- Bearish Crosses: {bearish_crosses}\")\n    \n    return df\n\n# Calculate original NW-RQK\nprint(\"\\nCalculating original NW-RQK with single Rational Quadratic kernel...\")\nlogger.info(\"Starting NW-RQK calculation\")\nstart_time = time.time()\n\ntry:\n    # Apply the calculation to the 30-minute data\n    df_30m = calculate_nw_rqk(df_30m, src_col='Close', h=config.nwrqk_h, r=config.nwrqk_r, \n                              x_0=25, lag=config.nwrqk_lag, smooth_colors=False)\n    \n    nwrqk_time = time.time() - start_time\n    \n    # FIX: Use continuous trend states from NW-RQK\n    # isBullish/isBearish are already state-based (not just changes)\n    df_30m['nwrqk_bull'] = df_30m['isBullish'].fillna(False)  # Current trend is bullish\n    df_30m['nwrqk_bear'] = df_30m['isBearish'].fillna(False)  # Current trend is bearish\n    \n    # Log statistics\n    logger.info(f\"NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n    \n    print(f\"Original NW-RQK calculated in {nwrqk_time:.3f} seconds\")\n    print(f\"Bull signals (trend direction): {df_30m['nwrqk_bull'].sum():,}\")\n    print(f\"Bear signals (trend direction): {df_30m['nwrqk_bear'].sum():,}\")\n    \n    # Memory cleanup\n    if check_memory_usage() > config.max_memory_gb * 0.8:\n        cleanup_memory()\n    \nexcept Exception as e:\n    logger.error(f\"Error calculating NW-RQK: {str(e)}\")\n    # Set default values on error\n    df_30m['nwrqk_bull'] = False\n    df_30m['nwrqk_bear'] = False\n    df_30m['yhat1'] = np.nan\n    df_30m['yhat2'] = np.nan\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Smart Timeframe Alignment\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef create_alignment_map(timestamps_5m: np.ndarray, timestamps_30m: np.ndarray) -> np.ndarray:\n    \"\"\"Create efficient mapping between timeframes\"\"\"\n    n_5m = len(timestamps_5m)\n    mapping = np.zeros(n_5m, dtype=np.int64)\n    \n    j = 0\n    for i in prange(n_5m):\n        # Find the corresponding 30m bar\n        while j < len(timestamps_30m) - 1 and timestamps_30m[j + 1] <= timestamps_5m[i]:\n            j += 1\n        mapping[i] = j\n    \n    return mapping\n\nprint(\"\\nPerforming smart timeframe alignment...\")\nstart_time = time.time()\n\n# Create datetime arrays for mapping\n# Keep full precision by using nanosecond timestamps\ntimestamps_5m = df_5m.index.values.astype(np.int64)\ntimestamps_30m = df_30m.index.values.astype(np.int64)\n\n# Create mapping\nmapping = create_alignment_map(timestamps_5m, timestamps_30m)\n\n# Align all indicators efficiently\ndf_5m_aligned = df_5m.copy()\n\n# MLMI alignment (using the original MLMI values)\ndf_5m_aligned['mlmi'] = df_30m['mlmi'].values[mapping]\ndf_5m_aligned['mlmi_bull'] = df_30m['mlmi_bull'].values[mapping]\ndf_5m_aligned['mlmi_bear'] = df_30m['mlmi_bear'].values[mapping]\n\n# NW-RQK alignment (using the original signals)\ndf_5m_aligned['nwrqk_bull'] = df_30m['nwrqk_bull'].values[mapping]\ndf_5m_aligned['nwrqk_bear'] = df_30m['nwrqk_bear'].values[mapping]\n\n# FVG data\ndf_5m_aligned['fvg_bull'] = fvg_bull\ndf_5m_aligned['fvg_bear'] = fvg_bear\n\n# Add market regime detection\ndf_5m_aligned['volatility'] = df_5m_aligned['Returns'].rolling(20).std()\ndf_5m_aligned['trend_strength'] = abs(df_5m_aligned['Returns'].rolling(50).mean()) / df_5m_aligned['volatility']\n\nalign_time = time.time() - start_time\nprint(f\"Smart alignment completed in {align_time:.3f} seconds\")\nprint(f\"Aligned {len(df_5m_aligned):,} 5-minute bars\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: MLMI → NW-RQK → FVG Synergy Detection (Improved Logic)\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef detect_mlmi_nwrqk_fvg_synergy(mlmi_bull: np.ndarray, mlmi_bear: np.ndarray,\n                                 nwrqk_bull: np.ndarray, nwrqk_bear: np.ndarray,\n                                 fvg_bull: np.ndarray, fvg_bear: np.ndarray,\n                                 window: int = 30) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Improved synergy detection with state-based approach and less restrictive logic\n    \n    The synergy requires:\n    1. MLMI is in a bullish/bearish state (above/below MA)\n    2. NW-RQK confirms the trend direction\n    3. FVG provides the entry zone\n    \n    Key improvements:\n    - Uses state-based signals instead of event crossovers\n    - Doesn't reset states aggressively\n    - Allows re-entry if conditions are met again\n    \"\"\"\n    n = len(mlmi_bull)\n    long_signals = np.zeros(n, dtype=np.bool_)\n    short_signals = np.zeros(n, dtype=np.bool_)\n    \n    # Track last signal bar to prevent immediate re-entry\n    last_long_bar = -window\n    last_short_bar = -window\n    \n    for i in range(2, n):  # Start from 2 to have lookback\n        # Check for bullish synergy\n        if (mlmi_bull[i] and  # MLMI is bullish (state)\n            nwrqk_bull[i] and  # NW-RQK confirms bullish trend (state)\n            fvg_bull[i] and  # FVG bullish zone is active\n            (i - last_long_bar) >= 10):  # Minimum bars since last long\n            \n            long_signals[i] = True\n            last_long_bar = i\n        \n        # Check for bearish synergy\n        elif (mlmi_bear[i] and  # MLMI is bearish (state)\n              nwrqk_bear[i] and  # NW-RQK confirms bearish trend (state)\n              fvg_bear[i] and  # FVG bearish zone is active\n              (i - last_short_bar) >= 10):  # Minimum bars since last short\n            \n            short_signals[i] = True\n            last_short_bar = i\n    \n    return long_signals, short_signals\n\nprint(\"\\nDetecting MLMI → NW-RQK → FVG synergy signals...\")\nstart_time = time.time()\n\n# Extract arrays\nmlmi_bull_arr = df_5m_aligned['mlmi_bull'].values\nmlmi_bear_arr = df_5m_aligned['mlmi_bear'].values\nnwrqk_bull_arr = df_5m_aligned['nwrqk_bull'].values\nnwrqk_bear_arr = df_5m_aligned['nwrqk_bear'].values\nfvg_bull_arr = df_5m_aligned['fvg_bull'].values\nfvg_bear_arr = df_5m_aligned['fvg_bear'].values\n\n# Detect synergy with improved logic\nlong_entries, short_entries = detect_mlmi_nwrqk_fvg_synergy(\n    mlmi_bull_arr, mlmi_bear_arr,\n    nwrqk_bull_arr, nwrqk_bear_arr,\n    fvg_bull_arr, fvg_bear_arr,\n    window=config.synergy_window\n)\n\n# Add to dataframe\ndf_5m_aligned['long_entry'] = long_entries\ndf_5m_aligned['short_entry'] = short_entries\n\n# For compatibility with the backtesting code, add a signal_quality column (set to 1.0 for all signals)\ndf_5m_aligned['signal_quality'] = np.where(long_entries | short_entries, 1.0, 0.0)\n\nsignal_time = time.time() - start_time\nprint(f\"Synergy detection completed in {signal_time:.3f} seconds\")\nprint(f\"Long entries: {long_entries.sum():,}\")\nprint(f\"Short entries: {short_entries.sum():,}\")\n\n# Calculate average time between signals\nall_signals = long_entries | short_entries\nif all_signals.sum() > 1:\n    signal_indices = np.where(all_signals)[0]\n    avg_bars_between = np.mean(np.diff(signal_indices))\n    print(f\"Average bars between signals: {avg_bars_between:.1f} (≈ {avg_bars_between * 5:.0f} minutes)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Ultra-Fast VectorBT Backtesting\n\n@njit(fastmath=True, cache=True)\ndef generate_exit_signals_advanced(entries: np.ndarray, direction: np.ndarray, \n                                  close: np.ndarray, atr: np.ndarray,\n                                  max_bars: int = 100, \n                                  stop_loss_atr: float = 2.0,\n                                  take_profit_atr: float = 4.0) -> np.ndarray:\n    \"\"\"Generate exit signals with ATR-based stops\"\"\"\n    n = len(entries)\n    exits = np.zeros(n, dtype=np.bool_)\n    \n    position_open = False\n    position_dir = 0\n    entry_idx = -1\n    entry_price = 0.0\n    entry_atr = 0.0\n    \n    for i in range(n):\n        if position_open:\n            bars_held = i - entry_idx\n            \n            # Fixed exit levels based on ATR\n            stop_distance = entry_atr * stop_loss_atr\n            target_distance = entry_atr * take_profit_atr\n            \n            # Check exit conditions\n            if position_dir == 1:  # Long position\n                stop_price = entry_price - stop_distance\n                target_price = entry_price + target_distance\n                \n                if (direction[i] == -1 or \n                    bars_held >= max_bars or \n                    close[i] <= stop_price or \n                    close[i] >= target_price):\n                    exits[i] = True\n                    position_open = False\n            \n            elif position_dir == -1:  # Short position\n                stop_price = entry_price + stop_distance\n                target_price = entry_price - target_distance\n                \n                if (direction[i] == 1 or \n                    bars_held >= max_bars or \n                    close[i] >= stop_price or \n                    close[i] <= target_price):\n                    exits[i] = True\n                    position_open = False\n        \n        # Check for new entry\n        if entries[i] and not position_open:\n            position_open = True\n            position_dir = direction[i]\n            entry_idx = i\n            entry_price = close[i]\n            entry_atr = atr[i] if not np.isnan(atr[i]) else close[i] * 0.01\n    \n    return exits\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ULTRA-FAST VECTORBT BACKTESTING\")\nprint(\"=\" * 80)\n\n# Prepare data for vectorbt\nclose_prices = df_5m_aligned['Close'].values\nentries = df_5m_aligned['long_entry'] | df_5m_aligned['short_entry']\nentries_array = entries.values\ndirection = np.where(df_5m_aligned['long_entry'], 1, \n                    np.where(df_5m_aligned['short_entry'], -1, 0))\n\n# Calculate ATR for dynamic stops (approximation for speed)\natr_approx = df_5m_aligned['HL_Range'].rolling(14).mean().values\n\n# Generate exit signals\nprint(\"\\nGenerating exit signals...\")\nexit_start = time.time()\n\nexits = generate_exit_signals_advanced(\n    entries_array, direction, close_prices, atr_approx,\n    max_bars=config.max_holding_bars,\n    stop_loss_atr=config.stop_loss_atr,\n    take_profit_atr=config.stop_loss_atr * 2  # 2:1 reward-risk ratio\n)\n\nexit_time = time.time() - exit_start\nprint(f\"Exit signals generated in {exit_time:.3f} seconds\")\nprint(f\"Total exits: {exits.sum():,}\")\n\n# Fixed position sizing (using base size for all trades)\nposition_sizes = np.where(\n    entries_array,\n    config.position_size_base,\n    0\n)\n\nprint(\"\\nRunning vectorized backtest...\")\nbacktest_start = time.time()\n\ntry:\n    # Run backtest with vectorbt\n    portfolio = vbt.Portfolio.from_signals(\n        close=df_5m_aligned['Close'],\n        entries=entries,\n        exits=exits,\n        direction=direction,\n        size=position_sizes,\n        size_type='amount',\n        init_cash=config.initial_capital,\n        fees=config.fees,\n        slippage=config.slippage,\n        freq='5T',\n        cash_sharing=True,\n        call_seq='auto'\n    )\n    \n    backtest_time = time.time() - backtest_start\n    print(f\"\\nBacktest completed in {backtest_time:.3f} seconds!\")\n    \n    # Calculate comprehensive metrics\n    portfolio_stats = portfolio.stats()\n    returns = portfolio.returns()\n    trades = portfolio.trades.records_readable\n    \n    print(\"\\n\" + \"-\" * 50)\n    print(\"PERFORMANCE METRICS\")\n    print(\"-\" * 50)\n    \n    # Core metrics\n    print(f\"Total Return: {portfolio_stats['Total Return [%]']:.2f}%\")\n    print(f\"Annualized Return: {portfolio_stats['Annualized Return [%]']:.2f}%\")\n    print(f\"Sharpe Ratio: {portfolio_stats['Sharpe Ratio']:.2f}\")\n    print(f\"Sortino Ratio: {portfolio_stats['Sortino Ratio']:.2f}\")\n    print(f\"Calmar Ratio: {portfolio_stats['Calmar Ratio']:.2f}\")\n    print(f\"Max Drawdown: {portfolio_stats['Max Drawdown [%]']:.2f}%\")\n    print(f\"Max Drawdown Duration: {portfolio_stats['Max Drawdown Duration']} days\")\n    \n    # Trade statistics\n    print(\"\\n\" + \"-\" * 50)\n    print(\"TRADE STATISTICS\")\n    print(\"-\" * 50)\n    print(f\"Total Trades: {portfolio_stats['Total Trades']:,.0f}\")\n    print(f\"Win Rate: {portfolio_stats['Win Rate [%]']:.2f}%\")\n    print(f\"Profit Factor: {portfolio_stats['Profit Factor']:.2f}\")\n    print(f\"Expectancy: {portfolio_stats['Expectancy [%]']:.3f}%\")\n    print(f\"Average Win: {portfolio_stats['Avg Winning Trade [%]']:.2f}%\")\n    print(f\"Average Loss: {portfolio_stats['Avg Losing Trade [%]']:.2f}%\")\n    print(f\"Best Trade: {portfolio_stats['Best Trade [%]']:.2f}%\")\n    print(f\"Worst Trade: {portfolio_stats['Worst Trade [%]']:.2f}%\")\n    \n    # Position sizing analysis\n    if len(trades) > 0:\n        avg_position_size = trades['Size'].mean()\n        print(f\"\\nAverage Position Size: {avg_position_size:.2f}\")\n        print(f\"Position Size StdDev: {trades['Size'].std():.2f}\")\n        \n    # Advanced metrics\n    print(\"\\n\" + \"-\" * 50)\n    print(\"ADVANCED METRICS\")\n    print(\"-\" * 50)\n    \n    # Calculate additional metrics\n    daily_returns = returns.resample('D').apply(lambda x: (1 + x).prod() - 1)\n    \n    if len(daily_returns) > 0:\n        # Value at Risk (95%)\n        var_95 = np.percentile(daily_returns.dropna(), 5)\n        print(f\"Daily VaR (95%): {var_95*100:.2f}%\")\n        \n        # Conditional VaR (CVaR)\n        cvar_95 = daily_returns[daily_returns <= var_95].mean()\n        print(f\"Daily CVaR (95%): {cvar_95*100:.2f}%\")\n        \n        # Information Ratio (assuming 0 benchmark)\n        ir = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n        print(f\"Information Ratio: {ir:.2f}\")\n    \n    # Trade analysis by direction\n    if len(trades) > 0:\n        print(\"\\n\" + \"-\" * 50)\n        print(\"DIRECTIONAL ANALYSIS\")\n        print(\"-\" * 50)\n        \n        long_trades = trades[trades['Direction'] == 'Long']\n        short_trades = trades[trades['Direction'] == 'Short']\n        \n        if len(long_trades) > 0:\n            print(f\"\\nLong Trades: {len(long_trades)}\")\n            print(f\"  Win Rate: {(long_trades['PnL'] > 0).mean()*100:.1f}%\")\n            print(f\"  Avg PnL: {long_trades['PnL %'].mean():.2f}%\")\n            \n        if len(short_trades) > 0:\n            print(f\"\\nShort Trades: {len(short_trades)}\")\n            print(f\"  Win Rate: {(short_trades['PnL'] > 0).mean()*100:.1f}%\")\n            print(f\"  Avg PnL: {short_trades['PnL %'].mean():.2f}%\")\n    \n    logger.info(f\"Backtest completed successfully with {len(trades)} trades\")\n    \nexcept Exception as e:\n    logger.error(f\"Error running backtest: {str(e)}\")\n    print(f\"\\nError running backtest: {str(e)}\")\n    portfolio_stats = {}\n    returns = pd.Series(dtype=float)\n    trades = pd.DataFrame()\n    portfolio = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Professional Visualizations\n\nprint(\"\\nGenerating professional visualizations...\")\nlogger.info(\"Creating performance visualizations\")\n\n# Create comprehensive dashboard\nfig = make_subplots(\n    rows=4, cols=2,\n    shared_xaxes=True,\n    vertical_spacing=0.05,\n    horizontal_spacing=0.1,\n    row_heights=[0.3, 0.2, 0.2, 0.3],\n    column_widths=[0.7, 0.3],\n    subplot_titles=(\n        'Cumulative Returns', 'Returns Distribution',\n        'Drawdown', 'Monthly Returns',\n        'Signal Indicators', 'Trade PnL Distribution',\n        'Price & Signals (Sample)', 'Trade Duration Analysis'\n    ),\n    specs=[\n        [{\"secondary_y\": True}, {\"type\": \"histogram\"}],\n        [{\"secondary_y\": False}, {\"type\": \"scatter\"}],\n        [{\"secondary_y\": True}, {\"type\": \"box\"}],\n        [{\"secondary_y\": False}, {\"type\": \"bar\"}]\n    ]\n)\n\n# 1. Cumulative Returns with benchmark\ncumulative_returns = (1 + returns).cumprod() - 1\nbenchmark_returns = df_5m_aligned['Close'].pct_change().fillna(0)\ncumulative_benchmark = (1 + benchmark_returns).cumprod() - 1\n\nfig.add_trace(\n    go.Scatter(\n        x=cumulative_returns.index,\n        y=cumulative_returns.values * 100,\n        mode='lines',\n        name='Strategy',\n        line=dict(color='blue', width=2)\n    ),\n    row=1, col=1, secondary_y=False\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=cumulative_benchmark.index,\n        y=cumulative_benchmark.values * 100,\n        mode='lines',\n        name='Buy & Hold',\n        line=dict(color='gray', width=1, dash='dash')\n    ),\n    row=1, col=1, secondary_y=False\n)\n\n# Add cumulative trades on secondary axis\nif 'portfolio' in globals() and portfolio is not None:\n    cumulative_trades = np.arange(len(trades))\n    trade_times = trades['Entry Timestamp']\n    \n    fig.add_trace(\n        go.Scatter(\n            x=trade_times,\n            y=cumulative_trades,\n            mode='lines',\n            name='Cumulative Trades',\n            line=dict(color='green', width=1),\n            yaxis='y2'\n        ),\n        row=1, col=1, secondary_y=True\n    )\n\n# 2. Returns Distribution\nif len(returns) > 0:\n    fig.add_trace(\n        go.Histogram(\n            x=returns.values * 100,\n            nbinsx=50,\n            name='Returns',\n            marker_color='lightblue',\n            showlegend=False\n        ),\n        row=1, col=2\n    )\n    \n    # Add normal distribution overlay\n    mean_ret = returns.mean() * 100\n    std_ret = returns.std() * 100\n    x_range = np.linspace(returns.min() * 100, returns.max() * 100, 100)\n    normal_dist = stats.norm.pdf(x_range, mean_ret, std_ret) * len(returns) * (returns.max() - returns.min()) * 100 / 50\n    \n    fig.add_trace(\n        go.Scatter(\n            x=x_range,\n            y=normal_dist,\n            mode='lines',\n            name='Normal',\n            line=dict(color='red', width=2)\n        ),\n        row=1, col=2\n    )\n\n# 3. Drawdown\nif 'portfolio' in globals() and portfolio is not None:\n    drawdown = portfolio.drawdown() * 100\n    fig.add_trace(\n        go.Scatter(\n            x=drawdown.index,\n            y=-drawdown.values,\n            mode='lines',\n            name='Drawdown',\n            fill='tozeroy',\n            line=dict(color='red', width=1)\n        ),\n        row=2, col=1\n    )\n\n# 4. Monthly Returns Heatmap\nif len(returns) > 30:\n    monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n    years = monthly_returns.index.year.unique()\n    months = range(1, 13)\n    \n    # Group by year and month\n    monthly_data = []\n    for year in years:\n        year_data = []\n        for month in months:\n            try:\n                ret = monthly_returns[(monthly_returns.index.year == year) & \n                                    (monthly_returns.index.month == month)].iloc[0] * 100\n                year_data.append(ret)\n            except:\n                year_data.append(0)\n        monthly_data.append(year_data)\n    \n    fig.add_trace(\n        go.Scatter(\n            x=list(months) * len(years),\n            y=[val for year_data in monthly_data for val in year_data],\n            mode='markers',\n            marker=dict(\n                size=15,\n                color=[val for year_data in monthly_data for val in year_data],\n                colorscale='RdYlGn',\n                cmin=-10,\n                cmax=10,\n                showscale=True,\n                colorbar=dict(title=\"Return %\", x=1.15)\n            ),\n            showlegend=False\n        ),\n        row=2, col=2\n    )\n\n# 5. Signal Indicators (30m data sample)\nsample_size = min(500, len(df_30m))\nsample_30m = df_30m.tail(sample_size)\n\nfig.add_trace(\n    go.Scatter(\n        x=sample_30m.index,\n        y=sample_30m['Close'],\n        mode='lines',\n        name='Close',\n        line=dict(color='black', width=1)\n    ),\n    row=3, col=1, secondary_y=False\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=sample_30m.index,\n        y=sample_30m['mlmi'],\n        mode='lines',\n        name='MLMI',\n        line=dict(color='blue', width=1),\n        yaxis='y2'\n    ),\n    row=3, col=1, secondary_y=True\n)\n\n# Add NW-RQK lines\nif 'yhat1' in sample_30m.columns:\n    fig.add_trace(\n        go.Scatter(\n            x=sample_30m.index,\n            y=sample_30m['yhat1'],\n            mode='lines',\n            name='NW-RQK',\n            line=dict(color='orange', width=1, dash='dash')\n        ),\n        row=3, col=1, secondary_y=False\n    )\n\n# 6. Trade PnL Distribution\nif len(trades) > 0:\n    fig.add_trace(\n        go.Box(\n            y=trades['PnL %'],\n            name='PnL Distribution',\n            boxpoints='all',\n            jitter=0.3,\n            pointpos=-1.8,\n            marker=dict(\n                color='lightblue',\n                size=4\n            ),\n            showlegend=False\n        ),\n        row=3, col=2\n    )\n\n# 7. Price & Signals Sample\nsample_size_5m = min(1000, len(df_5m_aligned))\nsample_5m = df_5m_aligned.tail(sample_size_5m)\n\nfig.add_trace(\n    go.Candlestick(\n        x=sample_5m.index,\n        open=sample_5m['Open'],\n        high=sample_5m['High'],\n        low=sample_5m['Low'],\n        close=sample_5m['Close'],\n        name='Price',\n        showlegend=False\n    ),\n    row=4, col=1\n)\n\n# Add entry signals\nlong_entries_sample = sample_5m[sample_5m['long_entry']]\nshort_entries_sample = sample_5m[sample_5m['short_entry']]\n\nif len(long_entries_sample) > 0:\n    fig.add_trace(\n        go.Scatter(\n            x=long_entries_sample.index,\n            y=long_entries_sample['Low'] * 0.995,\n            mode='markers',\n            name='Long',\n            marker=dict(symbol='triangle-up', size=10, color='green')\n        ),\n        row=4, col=1\n    )\n\nif len(short_entries_sample) > 0:\n    fig.add_trace(\n        go.Scatter(\n            x=short_entries_sample.index,\n            y=short_entries_sample['High'] * 1.005,\n            mode='markers',\n            name='Short',\n            marker=dict(symbol='triangle-down', size=10, color='red')\n        ),\n        row=4, col=1\n    )\n\n# 8. Trade Duration Analysis\nif len(trades) > 0 and 'Duration' in trades.columns:\n    durations = trades['Duration'].dt.total_seconds() / 3600  # Convert to hours\n    \n    fig.add_trace(\n        go.Bar(\n            x=['< 1h', '1-2h', '2-4h', '4-8h', '8-24h', '> 24h'],\n            y=[\n                len(durations[durations < 1]),\n                len(durations[(durations >= 1) & (durations < 2)]),\n                len(durations[(durations >= 2) & (durations < 4)]),\n                len(durations[(durations >= 4) & (durations < 8)]),\n                len(durations[(durations >= 8) & (durations < 24)]),\n                len(durations[durations >= 24])\n            ],\n            name='Trade Count',\n            marker_color='lightblue'\n        ),\n        row=4, col=2\n    )\n\n# Update layout\nfig.update_layout(\n    title={\n        'text': 'MLMI → NW-RQK → FVG Synergy Strategy Performance Dashboard',\n        'x': 0.5,\n        'xanchor': 'center',\n        'font': {'size': 20}\n    },\n    height=1600,\n    showlegend=True,\n    template='plotly_white',\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1.02,\n        xanchor=\"right\",\n        x=1\n    )\n)\n\n# Update axes\nfig.update_yaxes(title_text=\"Return (%)\", row=1, col=1, secondary_y=False)\nfig.update_yaxes(title_text=\"Trades\", row=1, col=1, secondary_y=True)\nfig.update_xaxes(title_text=\"Return (%)\", row=1, col=2)\nfig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\nfig.update_yaxes(title_text=\"Drawdown (%)\", row=2, col=1)\nfig.update_xaxes(title_text=\"Month\", row=2, col=2)\nfig.update_yaxes(title_text=\"Return (%)\", row=2, col=2)\nfig.update_yaxes(title_text=\"Price\", row=3, col=1, secondary_y=False)\nfig.update_yaxes(title_text=\"MLMI\", row=3, col=1, secondary_y=True)\nfig.update_yaxes(title_text=\"PnL (%)\", row=3, col=2)\nfig.update_yaxes(title_text=\"Price\", row=4, col=1)\nfig.update_xaxes(title_text=\"Duration\", row=4, col=2)\nfig.update_yaxes(title_text=\"Count\", row=4, col=2)\n\n# Show the figure\nfig.show()\n\nprint(\"\\nVisualization complete!\")\n\n# Save the figure if configured\nif config.save_results:\n    fig_path = os.path.join(config.results_path, 'performance_dashboard.html')\n    fig.write_html(fig_path)\n    logger.info(f\"Saved performance dashboard to {fig_path}\")\n    print(f\"Dashboard saved to {fig_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Statistical Validation and Robustness Testing\n",
    "\n",
    "@njit(parallel=True, fastmath=True, cache=True)\n",
    "def bootstrap_confidence_intervals(returns: np.ndarray, n_bootstrap: int = 10000,\n",
    "                                  confidence: float = 0.95) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Bootstrap confidence intervals with robust statistics\"\"\"\n",
    "    n = len(returns)\n",
    "    \n",
    "    # Arrays to store bootstrap results\n",
    "    boot_returns = np.zeros(n_bootstrap)\n",
    "    boot_sharpes = np.zeros(n_bootstrap)\n",
    "    boot_max_dd = np.zeros(n_bootstrap)\n",
    "    boot_win_rates = np.zeros(n_bootstrap)\n",
    "    \n",
    "    # Remove NaN values\n",
    "    clean_returns = returns[~np.isnan(returns)]\n",
    "    n_clean = len(clean_returns)\n",
    "    \n",
    "    if n_clean == 0:\n",
    "        return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n",
    "    \n",
    "    # Bootstrap iterations\n",
    "    for i in prange(n_bootstrap):\n",
    "        # Resample with replacement (without setting seed in parallel loop)\n",
    "        indices = np.random.randint(0, n_clean, size=n_clean)\n",
    "        sample = clean_returns[indices]\n",
    "        \n",
    "        # Calculate metrics with safety checks\n",
    "        boot_returns[i] = np.prod(1 + sample) - 1\n",
    "        \n",
    "        mean_ret = np.mean(sample)\n",
    "        std_ret = np.std(sample)\n",
    "        if std_ret > 1e-10:\n",
    "            boot_sharpes[i] = mean_ret / std_ret * np.sqrt(252 * 78)\n",
    "        else:\n",
    "            boot_sharpes[i] = 0.0\n",
    "        \n",
    "        # Max drawdown\n",
    "        cum_ret = np.cumprod(1 + sample)\n",
    "        running_max = np.maximum.accumulate(cum_ret)\n",
    "        dd = np.where(running_max > 0, (cum_ret - running_max) / running_max, 0)\n",
    "        boot_max_dd[i] = np.min(dd)\n",
    "        \n",
    "        # Win rate\n",
    "        boot_win_rates[i] = np.mean(sample > 0)\n",
    "    \n",
    "    return boot_returns, boot_sharpes, boot_max_dd, boot_win_rates\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL VALIDATION & ROBUSTNESS TESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Bootstrap analysis\n",
    "print(\"\\nRunning bootstrap analysis (10,000 iterations)...\")\n",
    "logger.info(\"Starting bootstrap analysis\")\n",
    "boot_start = time.time()\n",
    "\n",
    "try:\n",
    "    returns_array = returns.values\n",
    "    boot_returns, boot_sharpes, boot_max_dd, boot_win_rates = bootstrap_confidence_intervals(returns_array)\n",
    "    \n",
    "    boot_time = time.time() - boot_start\n",
    "    logger.info(f\"Bootstrap completed in {boot_time:.3f} seconds\")\n",
    "    print(f\"Bootstrap completed in {boot_time:.3f} seconds\")\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    def calculate_ci(data, confidence=0.95):\n",
    "        \"\"\"Calculate confidence interval with safety checks\"\"\"\n",
    "        valid_data = data[~np.isnan(data)]\n",
    "        if len(valid_data) == 0:\n",
    "            return 0.0, 0.0\n",
    "        lower = np.percentile(valid_data, (1 - confidence) / 2 * 100)\n",
    "        upper = np.percentile(valid_data, (1 + confidence) / 2 * 100)\n",
    "        return lower, upper\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n95% Confidence Intervals:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    ret_lower, ret_upper = calculate_ci(boot_returns)\n",
    "    print(f\"Total Return: [{ret_lower*100:.2f}%, {ret_upper*100:.2f}%]\")\n",
    "    \n",
    "    sharpe_lower, sharpe_upper = calculate_ci(boot_sharpes)\n",
    "    print(f\"Sharpe Ratio: [{sharpe_lower:.2f}, {sharpe_upper:.2f}]\")\n",
    "    \n",
    "    dd_lower, dd_upper = calculate_ci(boot_max_dd)\n",
    "    print(f\"Max Drawdown: [{dd_lower*100:.2f}%, {dd_upper*100:.2f}%]\")\n",
    "    \n",
    "    wr_lower, wr_upper = calculate_ci(boot_win_rates)\n",
    "    print(f\"Win Rate: [{wr_lower*100:.2f}%, {wr_upper*100:.2f}%]\")\n",
    "    \n",
    "    # Statistical significance tests\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"STATISTICAL SIGNIFICANCE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Test if returns are significantly different from zero\n",
    "    clean_returns = returns_array[~np.isnan(returns_array)]\n",
    "    if len(clean_returns) > 1:\n",
    "        mean_return = np.mean(clean_returns)\n",
    "        std_return = np.std(clean_returns)\n",
    "        n_returns = len(clean_returns)\n",
    "        \n",
    "        if std_return > 0:\n",
    "            t_stat = mean_return / (std_return / np.sqrt(n_returns))\n",
    "            # Approximate p-value using normal distribution\n",
    "            p_value_approx = 2 * (1 - stats.norm.cdf(abs(t_stat)))\n",
    "            \n",
    "            print(f\"T-statistic: {t_stat:.3f}\")\n",
    "            print(f\"Approx p-value: {p_value_approx:.4f}\")\n",
    "            print(f\"Returns significantly positive: {'Yes' if t_stat > 1.96 else 'No'}\")\n",
    "        else:\n",
    "            print(\"Cannot calculate t-statistic: zero standard deviation\")\n",
    "    \n",
    "    # Risk-adjusted performance percentiles\n",
    "    if 'portfolio_stats' in globals() and portfolio_stats and 'Sharpe Ratio' in portfolio_stats:\n",
    "        actual_sharpe = portfolio_stats['Sharpe Ratio']\n",
    "    else:\n",
    "        actual_sharpe = 0\n",
    "        \n",
    "    sharpe_percentile = np.sum(boot_sharpes <= actual_sharpe) / len(boot_sharpes) * 100\n",
    "    \n",
    "    print(f\"\\nStrategy Sharpe ratio percentile: {sharpe_percentile:.1f}%\")\n",
    "    print(f\"Performance assessment: \", end=\"\")\n",
    "    if sharpe_percentile > 90:\n",
    "        print(\"EXCELLENT - Top 10% performance\")\n",
    "    elif sharpe_percentile > 75:\n",
    "        print(\"VERY GOOD - Top 25% performance\")\n",
    "    elif sharpe_percentile > 50:\n",
    "        print(\"GOOD - Above median performance\")\n",
    "    else:\n",
    "        print(\"NEEDS IMPROVEMENT - Below median performance\")\n",
    "    \n",
    "    # Stability analysis\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"STABILITY ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Rolling performance\n",
    "    window = min(252 * 5, len(returns) // 2)  # 1 year of 5-minute bars or half the data\n",
    "    if window > 100:\n",
    "        rolling_returns = returns.rolling(window).apply(lambda x: (1 + x).prod() - 1)\n",
    "        rolling_sharpe = returns.rolling(window).apply(\n",
    "            lambda x: x.mean() / x.std() * np.sqrt(252 * 78) if x.std() > 0 else 0\n",
    "        )\n",
    "        \n",
    "        print(f\"Rolling 1-year return volatility: {rolling_returns.std()*100:.2f}%\")\n",
    "        print(f\"Rolling Sharpe stability: {rolling_sharpe.std():.2f}\")\n",
    "        print(f\"Minimum rolling Sharpe: {rolling_sharpe.min():.2f}\")\n",
    "        print(f\"Maximum rolling Sharpe: {rolling_sharpe.max():.2f}\")\n",
    "    else:\n",
    "        print(\"Insufficient data for rolling analysis\")\n",
    "    \n",
    "    # Save validation results\n",
    "    if config.save_results:\n",
    "        validation_results = {\n",
    "            'confidence_intervals': {\n",
    "                'return': (ret_lower, ret_upper),\n",
    "                'sharpe': (sharpe_lower, sharpe_upper),\n",
    "                'max_dd': (dd_lower, dd_upper),\n",
    "                'win_rate': (wr_lower, wr_upper)\n",
    "            },\n",
    "            'significance': {\n",
    "                't_stat': t_stat if 't_stat' in locals() else None,\n",
    "                'significant': t_stat > 1.96 if 't_stat' in locals() else False\n",
    "            },\n",
    "            'percentiles': {\n",
    "                'sharpe_percentile': sharpe_percentile\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        validation_path = os.path.join(config.results_path, 'validation_results.json')\n",
    "        with open(validation_path, 'w') as f:\n",
    "            json.dump(validation_results, f, indent=2, default=str)\n",
    "        logger.info(f\"Saved validation results to {validation_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in statistical validation: {str(e)}\")\n",
    "    print(f\"\\nError in statistical validation: {str(e)}\")\n",
    "    print(\"Continuing with limited validation...\")\n",
    "\n",
    "# Initialize boot_time if bootstrap failed\n",
    "if 'boot_time' not in locals():\n",
    "    boot_time = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}