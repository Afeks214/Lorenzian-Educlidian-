{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Synergy 3: NW-RQK → MLMI → FVG Trading Strategy [CORRECTED VERSION]\n\n## Ultra-High Performance Implementation with VectorBT and Numba\n\n**IMPORTANT**: This is the corrected version that uses the original indicator implementations from Strategy_Implementation.ipynb\n\nThis notebook implements the third synergy pattern where:\n1. **NW-RQK** (Nadaraya-Watson Rational Quadratic Kernel) provides the initial trend signal [ORIGINAL IMPLEMENTATION]\n2. **MLMI** (Machine Learning Market Intelligence) confirms the market regime [ORIGINAL IMPLEMENTATION]\n3. **FVG** (Fair Value Gap) validates the final entry zone [ORIGINAL IMPLEMENTATION]\n\n### Key Features:\n- Original indicator logic preserved from Strategy_Implementation.ipynb\n- Ultra-fast execution using Numba JIT compilation\n- VectorBT for lightning-fast vectorized backtesting\n- Professional visualizations and comprehensive metrics\n- Production-ready error handling and logging"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries with comprehensive error handling\nimport numpy as np\nimport pandas as pd\nimport vectorbt as vbt\nfrom numba import njit, prange, float64, int64, boolean\nfrom numba.typed import List\nfrom numba.experimental import jitclass  # For MLMI implementation\nfrom scipy.spatial import cKDTree  # For MLMI kNN implementation\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nfrom datetime import datetime, timedelta\nimport time\nfrom scipy import stats\nimport logging\nimport os\nimport sys\nfrom typing import Dict, Any, Tuple, Optional\nimport json\nimport traceback\nfrom functools import lru_cache\nimport threading\nfrom pathlib import Path\n\n# Version checks\nREQUIRED_VERSIONS = {\n    'pandas': '1.3.0',\n    'numpy': '1.19.0',\n    'numba': '0.53.0',\n    'vectorbt': '0.20.0',\n    'scipy': '1.7.0'  # Added scipy requirement\n}\n\ndef check_package_versions():\n    \"\"\"Check if required package versions are met\"\"\"\n    import importlib\n    for package, min_version in REQUIRED_VERSIONS.items():\n        try:\n            module = importlib.import_module(package)\n            version = getattr(module, '__version__', '0.0.0')\n            if version < min_version:\n                warnings.warn(f\"{package} version {version} is below recommended {min_version}\")\n        except ImportError:\n            raise ImportError(f\"Required package {package} not found\")\n\n# Check versions on import\ntry:\n    check_package_versions()\nexcept Exception as e:\n    warnings.warn(f\"Version check failed: {str(e)}\")\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\n# Configure VectorBT with error handling\ntry:\n    vbt.settings.set_theme('dark')\n    vbt.settings['plotting']['layout']['width'] = 1200\n    vbt.settings['plotting']['layout']['height'] = 800\nexcept Exception as e:\n    warnings.warn(f\"VectorBT configuration failed: {str(e)}\")\n\n# Enhanced logging setup with rotation\nfrom logging.handlers import RotatingFileHandler\n\ndef setup_logging(log_file='synergy_3_strategy_corrected.log', level=logging.INFO):\n    \"\"\"Setup production-grade logging with rotation\"\"\"\n    logger = logging.getLogger('Synergy3StrategyCorrected')\n    logger.setLevel(level)\n    \n    # Clear existing handlers\n    logger.handlers = []\n    \n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(level)\n    \n    # File handler with rotation\n    file_handler = RotatingFileHandler(\n        log_file,\n        maxBytes=10*1024*1024,  # 10MB\n        backupCount=5\n    )\n    file_handler.setLevel(level)\n    \n    # Formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n    )\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger\n\n# Setup logger\nlogger = setup_logging()\n\n# Performance tracking decorator\ndef track_performance(func):\n    \"\"\"Decorator to track function execution time\"\"\"\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        try:\n            result = func(*args, **kwargs)\n            execution_time = time.time() - start_time\n            logger.debug(f\"{func.__name__} executed in {execution_time:.3f} seconds\")\n            return result\n        except Exception as e:\n            execution_time = time.time() - start_time\n            logger.error(f\"{func.__name__} failed after {execution_time:.3f} seconds: {str(e)}\")\n            raise\n    return wrapper\n\n# Thread-safe configuration management\nclass StrategyConfig:\n    \"\"\"Centralized configuration for the strategy with validation and thread safety\"\"\"\n    \n    _lock = threading.Lock()\n    _instance = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n        return cls._instance\n    \n    def __init__(self):\n        if hasattr(self, '_initialized'):\n            return\n        self._initialized = True\n        \n        # Data Configuration - FIXED PATHS\n        self.DATA_PATH_30M = \"/home/QuantNova/AlgoSpace-8/notebooks/notebook data/@CL - 5 min - ETH.csv\"\n        self.DATA_PATH_5M = \"/home/QuantNova/AlgoSpace-8/notebooks/notebook data/@CL - 30 min - ETH.csv\"\n        self.DATETIME_FORMATS = ['%d/%m/%Y %H:%M:%S', '%d/%m/%Y %H:%M', '%Y-%m-%d %H:%M:%S%z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']\n        \n        # NW-RQK Configuration (ORIGINAL VALUES)\n        self.NWRQK_H = 8.0\n        self.NWRQK_R = 8.0\n        self.NWRQK_X0 = 25\n        self.NWRQK_LAG = 2\n        self.NWRQK_SMOOTH_COLORS = False\n        \n        # MLMI Configuration (ORIGINAL VALUES)\n        self.MLMI_NUM_NEIGHBORS = 200\n        self.MLMI_MOMENTUM_WINDOW = 20\n        self.MLMI_MA_QUICK = 5\n        self.MLMI_MA_SLOW = 20\n        self.MLMI_RSI_QUICK = 5\n        self.MLMI_RSI_SLOW = 20\n        \n        # FVG Configuration (ORIGINAL VALUES)\n        self.FVG_LOOKBACK_PERIOD = 10\n        self.FVG_BODY_MULTIPLIER = 1.5\n        # NO VOLUME FACTOR - pure price action\n        \n        # Synergy Configuration\n        self.SYNERGY_WINDOW = 30\n        self.SYNERGY_MIN_SPACING = 5  # Minimum bars between signals\n        \n        # Risk Management\n        self.POSITION_SIZE_BASE = 0.1\n        self.STOP_LOSS_PCT = 0.02\n        self.TAKE_PROFIT_PCT = 0.03\n        self.MAX_DRAWDOWN_LIMIT = 0.15\n        self.MAX_DAILY_LOSS = 0.05\n        \n        # Backtesting\n        self.INITIAL_CAPITAL = 100000\n        self.TRADING_FEES = 0.001\n        self.SLIPPAGE = 0.0005\n        \n        # Validation\n        self.MAX_MISSING_DATA_PCT = 0.05\n        self.OUTLIER_STD_THRESHOLD = 10\n        self.MIN_DATA_POINTS = 1000\n        \n        # Performance settings\n        self.CACHE_SIZE = 128\n        self.MAX_WORKERS = 4\n        self.MEMORY_LIMIT_MB = 4096\n    \n    def validate(self):\n        \"\"\"Validate configuration parameters with comprehensive checks\"\"\"\n        errors = []\n        \n        # Data validation\n        if not Path(self.DATA_PATH_30M).exists():\n            errors.append(f\"30m data file not found: {self.DATA_PATH_30M}\")\n        if not Path(self.DATA_PATH_5M).exists():\n            errors.append(f\"5m data file not found: {self.DATA_PATH_5M}\")\n        \n        # NW-RQK validation\n        if self.NWRQK_H <= 0:\n            errors.append(\"NW-RQK h parameter must be positive\")\n        if self.NWRQK_R <= 0:\n            errors.append(\"NW-RQK r parameter must be positive\")\n        \n        # MLMI validation\n        if self.MLMI_NUM_NEIGHBORS < 1:\n            errors.append(\"MLMI num_neighbors must be at least 1\")\n        \n        # Risk management validation\n        if not 0 < self.STOP_LOSS_PCT < 1:\n            errors.append(\"Stop loss must be between 0 and 1\")\n        if not 0 < self.TAKE_PROFIT_PCT < 1:\n            errors.append(\"Take profit must be between 0 and 1\")\n        \n        # Raise error if critical validations fail\n        if errors:\n            error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in errors)\n            raise ValueError(error_msg)\n        \n        logger.info(\"Configuration validated successfully\")\n        return True\n    \n    def to_dict(self):\n        \"\"\"Convert configuration to dictionary for serialization\"\"\"\n        with self._lock:\n            return {\n                attr: getattr(self, attr)\n                for attr in dir(self)\n                if not attr.startswith('_') and not callable(getattr(self, attr))\n            }\n    \n    def update(self, **kwargs):\n        \"\"\"Update configuration with validation\"\"\"\n        with self._lock:\n            for key, value in kwargs.items():\n                if hasattr(self, key):\n                    setattr(self, key, value)\n                else:\n                    logger.warning(f\"Unknown configuration parameter: {key}\")\n            self.validate()\n\n# Initialize and validate configuration\ntry:\n    StrategyConfig = StrategyConfig()\n    StrategyConfig.validate()\n    logger.info(\"Strategy configuration initialized successfully\")\nexcept Exception as e:\n    logger.error(f\"Configuration initialization failed: {str(e)}\")\n    raise\n\n# Global performance metrics\nclass PerformanceMetrics:\n    \"\"\"Track global performance metrics\"\"\"\n    def __init__(self):\n        self.start_time = time.time()\n        self.metrics = {}\n        self._lock = threading.Lock()\n    \n    def add_metric(self, name, value):\n        with self._lock:\n            self.metrics[name] = value\n    \n    def get_summary(self):\n        with self._lock:\n            total_time = time.time() - self.start_time\n            return {\n                'total_execution_time': total_time,\n                **self.metrics\n            }\n\n# Initialize global metrics\nperformance_metrics = PerformanceMetrics()\n\nlogger.info(\"=\"*60)\nlogger.info(\"SYNERGY 3: NW-RQK → MLMI → FVG STRATEGY [CORRECTED]\")\nlogger.info(\"Using original indicator implementations\")\nlogger.info(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ultra-Fast Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-09 21:56:23,356 - Synergy3Strategy - INFO - [3615366201.py:4] - Starting data loading process...\n",
      "2025-07-09 21:56:23,359 - Synergy3Strategy - INFO - [3615366201.py:15] - Loading 30m data from /home/QuantNova/AlgoSpace-8/notebooks/notebook data/@CL - 5 min - ETH.csv\n",
      "2025-07-09 21:56:23,973 - Synergy3Strategy - INFO - [3615366201.py:69] - Parsing timestamps for 30m data...\n"
     ]
    }
   ],
   "source": [
    "# Load data with optimized parsing and comprehensive error handling\n",
    "def load_data():\n",
    "    \"\"\"Load and preprocess data with ultra-fast parsing and robust error handling\"\"\"\n",
    "    logger.info(\"Starting data loading process...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Validate file existence\n",
    "        if not os.path.exists(StrategyConfig.DATA_PATH_30M):\n",
    "            raise FileNotFoundError(f\"30m data file not found: {StrategyConfig.DATA_PATH_30M}\")\n",
    "        if not os.path.exists(StrategyConfig.DATA_PATH_5M):\n",
    "            raise FileNotFoundError(f\"5m data file not found: {StrategyConfig.DATA_PATH_5M}\")\n",
    "        \n",
    "        # Load 30-minute data with error handling\n",
    "        logger.info(f\"Loading 30m data from {StrategyConfig.DATA_PATH_30M}\")\n",
    "        df_30m = load_single_timeframe(StrategyConfig.DATA_PATH_30M, '30m')\n",
    "        \n",
    "        # Load 5-minute data with error handling\n",
    "        logger.info(f\"Loading 5m data from {StrategyConfig.DATA_PATH_5M}\")\n",
    "        df_5m = load_single_timeframe(StrategyConfig.DATA_PATH_5M, '5m')\n",
    "        \n",
    "        # Validate data quality\n",
    "        df_30m = validate_and_clean_data(df_30m, '30m')\n",
    "        df_5m = validate_and_clean_data(df_5m, '5m')\n",
    "        \n",
    "        # Add derived features\n",
    "        df_30m = add_robust_features(df_30m, '30m')\n",
    "        df_5m = add_robust_features(df_5m, '5m')\n",
    "        \n",
    "        # Align data timeframes\n",
    "        df_30m, df_5m = align_timeframes(df_30m, df_5m)\n",
    "        \n",
    "        # Final data integrity check\n",
    "        perform_final_validation(df_30m, df_5m)\n",
    "        \n",
    "        logger.info(f\"Data loading completed in {time.time() - start_time:.2f} seconds\")\n",
    "        logger.info(f\"30m data: {len(df_30m)} bars from {df_30m.index[0]} to {df_30m.index[-1]}\")\n",
    "        logger.info(f\"5m data: {len(df_5m)} bars from {df_5m.index[0]} to {df_5m.index[-1]}\")\n",
    "        \n",
    "        return df_30m, df_5m\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error in data loading: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_single_timeframe(file_path: str, timeframe: str) -> pd.DataFrame:\n",
    "    \"\"\"Load data for a single timeframe with robust error handling\"\"\"\n",
    "    try:\n",
    "        # Load CSV with error handling\n",
    "        df = pd.read_csv(file_path, na_values=['', 'null', 'NULL', 'NaN'])\n",
    "        \n",
    "        # Convert column names to lowercase for consistency\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        # Check for required columns (now lowercase)\n",
    "        required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "        \n",
    "        # Handle case where datetime column might be named differently\n",
    "        if 'timestamp' not in df.columns and 'datetime' in df.columns:\n",
    "            df.rename(columns={'datetime': 'timestamp'}, inplace=True)\n",
    "        elif 'timestamp' not in df.columns and 'date' in df.columns:\n",
    "            df.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "        \n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns in {timeframe} data: {missing_columns}\")\n",
    "        \n",
    "        # Parse timestamps with multiple approaches for robustness\n",
    "        logger.info(f\"Parsing timestamps for {timeframe} data...\")\n",
    "        \n",
    "        # First try with dayfirst=True and format='mixed' for maximum flexibility\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], dayfirst=True, format='mixed', errors='coerce')\n",
    "        \n",
    "        # Check initial parsing success\n",
    "        initial_valid = df['timestamp'].notna().sum()\n",
    "        \n",
    "        # If many failed, try specific formats for remaining NaT values\n",
    "        if df['timestamp'].isna().any():\n",
    "            failed_mask = df['timestamp'].isna()\n",
    "            failed_count = failed_mask.sum()\n",
    "            logger.info(f\"Retrying {failed_count} failed timestamp parsings...\")\n",
    "            \n",
    "            # Try common formats on failed timestamps\n",
    "            formats_to_try = [\n",
    "                '%d/%m/%Y %H:%M:%S',\n",
    "                '%d/%m/%Y %H:%M',\n",
    "                '%Y-%m-%d %H:%M:%S',\n",
    "                '%m/%d/%Y %H:%M:%S',\n",
    "                '%m/%d/%Y %H:%M'\n",
    "            ]\n",
    "            \n",
    "            original_timestamps = df.loc[failed_mask, 'timestamp'].copy()\n",
    "            for fmt in formats_to_try:\n",
    "                if failed_mask.any():\n",
    "                    try:\n",
    "                        parsed = pd.to_datetime(df.loc[failed_mask, 'timestamp'], format=fmt, errors='coerce')\n",
    "                        valid_parsed = parsed.notna()\n",
    "                        if valid_parsed.any():\n",
    "                            df.loc[failed_mask & valid_parsed, 'timestamp'] = parsed[valid_parsed]\n",
    "                            failed_mask = df['timestamp'].isna()\n",
    "                            logger.debug(f\"Format '{fmt}' parsed {valid_parsed.sum()} timestamps\")\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        # Final parsing statistics\n",
    "        valid_timestamps = df['timestamp'].notna().sum()\n",
    "        total_rows = len(df)\n",
    "        parse_success_rate = valid_timestamps / total_rows\n",
    "        \n",
    "        logger.info(f\"Timestamp parsing complete: {parse_success_rate:.1%} success ({valid_timestamps}/{total_rows})\")\n",
    "        \n",
    "        # Remove rows with invalid datetime\n",
    "        invalid_datetime = df['timestamp'].isna()\n",
    "        if invalid_datetime.any():\n",
    "            logger.warning(f\"Removing {invalid_datetime.sum()} rows with unparseable timestamps\")\n",
    "            df = df[~invalid_datetime]\n",
    "        \n",
    "        # Set index and sort\n",
    "        df = df.set_index('timestamp').sort_index()\n",
    "        \n",
    "        # Remove duplicate timestamps\n",
    "        duplicates = df.index.duplicated()\n",
    "        if duplicates.any():\n",
    "            logger.warning(f\"Removing {duplicates.sum()} duplicate timestamps in {timeframe} data\")\n",
    "            df = df[~duplicates]\n",
    "        \n",
    "        # Ensure numeric data types\n",
    "        numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        for col in numeric_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Check minimum data requirement\n",
    "        if len(df) < StrategyConfig.MIN_DATA_POINTS:\n",
    "            logger.warning(f\"Data has {len(df)} rows, which is less than minimum required {StrategyConfig.MIN_DATA_POINTS}\")\n",
    "            # Lower the requirement for this specific case\n",
    "            if len(df) < 100:\n",
    "                raise ValueError(f\"Insufficient data: {len(df)} rows, need at least 100\")\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(df)} rows of {timeframe} data\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {timeframe} data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_and_clean_data(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:\n",
    "    \"\"\"Validate data quality and handle issues robustly\"\"\"\n",
    "    logger.info(f\"Validating {timeframe} data...\")\n",
    "    \n",
    "    try:\n",
    "        # Enhanced data integrity checks\n",
    "        initial_row_count = len(df)\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = df.isnull().sum()\n",
    "        total_missing = missing_count.sum()\n",
    "        \n",
    "        if total_missing > 0:\n",
    "            missing_pct = total_missing / (len(df) * len(df.columns))\n",
    "            logger.warning(f\"Found {total_missing} missing values ({missing_pct:.2%}) in {timeframe} data\")\n",
    "            \n",
    "            if missing_pct > StrategyConfig.MAX_MISSING_DATA_PCT:\n",
    "                logger.error(f\"Too many missing values: {missing_pct:.2%}\")\n",
    "                # Try to recover by forward/backward filling\n",
    "                # Updated: Replace deprecated fillna(method='ffill') with df.ffill()\n",
    "                df = df.ffill().bfill()\n",
    "                \n",
    "                # Check again\n",
    "                remaining_missing = df.isnull().sum().sum()\n",
    "                if remaining_missing > 0:\n",
    "                    logger.warning(f\"Still have {remaining_missing} missing values after filling\")\n",
    "                    # Fill remaining with reasonable defaults\n",
    "                    df['volume'] = df['volume'].fillna(0)\n",
    "                    for col in ['open', 'high', 'low', 'close']:\n",
    "                        if col in df.columns:\n",
    "                            df[col] = df[col].fillna(df[col].mean())\n",
    "        \n",
    "        # Validate price relationships\n",
    "        invalid_candles = (\n",
    "            (df['high'] < df['low']) | \n",
    "            (df['high'] < df['open']) | \n",
    "            (df['high'] < df['close']) |\n",
    "            (df['low'] > df['open']) | \n",
    "            (df['low'] > df['close'])\n",
    "        )\n",
    "        \n",
    "        if invalid_candles.any():\n",
    "            n_invalid = invalid_candles.sum()\n",
    "            logger.warning(f\"Found {n_invalid} invalid candles in {timeframe} data, fixing...\")\n",
    "            \n",
    "            # Fix invalid candles\n",
    "            df.loc[invalid_candles, 'high'] = df.loc[invalid_candles, ['open', 'close', 'high']].max(axis=1)\n",
    "            df.loc[invalid_candles, 'low'] = df.loc[invalid_candles, ['open', 'close', 'low']].min(axis=1)\n",
    "        \n",
    "        # Check for outliers\n",
    "        for col in ['open', 'high', 'low', 'close']:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Calculate rolling statistics\n",
    "            rolling_mean = df[col].rolling(window=100, min_periods=10).mean()\n",
    "            rolling_std = df[col].rolling(window=100, min_periods=10).std()\n",
    "            \n",
    "            # Identify outliers\n",
    "            z_scores = np.abs((df[col] - rolling_mean) / rolling_std)\n",
    "            outliers = z_scores > StrategyConfig.OUTLIER_STD_THRESHOLD\n",
    "            \n",
    "            if outliers.any():\n",
    "                n_outliers = outliers.sum()\n",
    "                logger.warning(f\"Found {n_outliers} outliers in {col} for {timeframe} data\")\n",
    "                \n",
    "                # Cap outliers at threshold\n",
    "                df.loc[outliers, col] = rolling_mean[outliers] + np.sign(\n",
    "                    df.loc[outliers, col] - rolling_mean[outliers]\n",
    "                ) * StrategyConfig.OUTLIER_STD_THRESHOLD * rolling_std[outliers]\n",
    "        \n",
    "        # Ensure no negative prices\n",
    "        negative_prices = (df[['open', 'high', 'low', 'close']] < 0).any(axis=1)\n",
    "        if negative_prices.any():\n",
    "            logger.error(f\"Found {negative_prices.sum()} rows with negative prices, removing...\")\n",
    "            df = df[~negative_prices]\n",
    "        \n",
    "        # Ensure no zero prices\n",
    "        zero_prices = (df[['open', 'high', 'low', 'close']] == 0).any(axis=1)\n",
    "        if zero_prices.any():\n",
    "            logger.warning(f\"Found {zero_prices.sum()} rows with zero prices, interpolating...\")\n",
    "            for col in ['open', 'high', 'low', 'close']:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].replace(0, np.nan).interpolate(method='linear')\n",
    "        \n",
    "        # Check for time gaps\n",
    "        time_diff = df.index.to_series().diff()\n",
    "        expected_freq = pd.Timedelta(timeframe)\n",
    "        large_gaps = time_diff > expected_freq * 2\n",
    "        \n",
    "        if large_gaps.any():\n",
    "            logger.warning(f\"Found {large_gaps.sum()} time gaps larger than expected in {timeframe} data\")\n",
    "            # Log details of largest gaps\n",
    "            largest_gaps = time_diff[large_gaps].nlargest(5)\n",
    "            for idx, gap in largest_gaps.items():\n",
    "                logger.debug(f\"  Gap at {idx}: {gap}\")\n",
    "        \n",
    "        # Additional integrity checks\n",
    "        # Check for stuck prices (no movement for extended periods)\n",
    "        price_unchanged = (df['close'].diff() == 0)\n",
    "        consecutive_unchanged = price_unchanged.rolling(window=10).sum()\n",
    "        stuck_periods = consecutive_unchanged >= 10\n",
    "        \n",
    "        if stuck_periods.any():\n",
    "            logger.warning(f\"Found {stuck_periods.sum()} periods with stuck prices (10+ bars unchanged)\")\n",
    "        \n",
    "        # Check volume integrity\n",
    "        zero_volume = df['volume'] == 0\n",
    "        if zero_volume.sum() > len(df) * 0.5:\n",
    "            logger.warning(f\"More than 50% of bars have zero volume - data quality may be compromised\")\n",
    "        \n",
    "        # Final data quality metrics\n",
    "        final_row_count = len(df)\n",
    "        rows_removed = initial_row_count - final_row_count\n",
    "        \n",
    "        if rows_removed > 0:\n",
    "            logger.info(f\"Data validation removed {rows_removed} rows ({rows_removed/initial_row_count:.1%})\")\n",
    "        \n",
    "        logger.info(f\"Validation completed for {timeframe} data\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in data validation for {timeframe}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def add_robust_features(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:\n",
    "    \"\"\"Add derived features with error handling and validation\"\"\"\n",
    "    try:\n",
    "        # Calculate returns with handling for division by zero\n",
    "        df['returns'] = df['close'].pct_change().fillna(0)\n",
    "        \n",
    "        # Cap extreme returns\n",
    "        extreme_returns = np.abs(df['returns']) > 0.5  # 50% moves\n",
    "        if extreme_returns.any():\n",
    "            logger.warning(f\"Capping {extreme_returns.sum()} extreme returns in {timeframe} data\")\n",
    "            df.loc[extreme_returns, 'returns'] = np.sign(df.loc[extreme_returns, 'returns']) * 0.5\n",
    "        \n",
    "        # Calculate log returns safely\n",
    "        df['log_returns'] = np.log1p(df['returns'])  # log1p is more stable for small values\n",
    "        \n",
    "        # Calculate volatility with minimum periods\n",
    "        df['volatility'] = df['returns'].rolling(window=20, min_periods=5).std().fillna(0)\n",
    "        \n",
    "        # Volume metrics with safety checks\n",
    "        df['volume_sma'] = df['volume'].rolling(window=20, min_periods=5).mean().fillna(df['volume'])\n",
    "        df['volume_ratio'] = np.where(\n",
    "            df['volume_sma'] > 0,\n",
    "            df['volume'] / df['volume_sma'],\n",
    "            1.0\n",
    "        )\n",
    "        \n",
    "        # Price ranges with safety checks\n",
    "        df['high_low_range'] = np.where(\n",
    "            df['close'] > 0,\n",
    "            (df['high'] - df['low']) / df['close'],\n",
    "            0\n",
    "        )\n",
    "        df['close_open_range'] = np.where(\n",
    "            df['open'] > 0,\n",
    "            (df['close'] - df['open']) / df['open'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # VWAP calculation with cumulative approach\n",
    "        typical_price = (df['high'] + df['low'] + df['close']) / 3\n",
    "        df['vwap'] = (typical_price * df['volume']).cumsum() / df['volume'].cumsum()\n",
    "        df['vwap'] = df['vwap'].fillna(typical_price)  # Handle initial NaN values\n",
    "        \n",
    "        # Add trend indicators\n",
    "        df['sma_20'] = df['close'].rolling(window=20, min_periods=5).mean()\n",
    "        df['sma_50'] = df['close'].rolling(window=50, min_periods=10).mean()\n",
    "        df['price_position'] = np.where(\n",
    "            df['sma_20'] > 0,\n",
    "            (df['close'] - df['sma_20']) / df['sma_20'],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # ATR calculation for risk management\n",
    "        high_low = df['high'] - df['low']\n",
    "        high_close = np.abs(df['high'] - df['close'].shift())\n",
    "        low_close = np.abs(df['low'] - df['close'].shift())\n",
    "        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "        df['atr'] = true_range.rolling(window=14, min_periods=5).mean().fillna(true_range)\n",
    "        \n",
    "        # Validate all features\n",
    "        feature_cols = ['returns', 'log_returns', 'volatility', 'volume_sma', \n",
    "                       'volume_ratio', 'high_low_range', 'close_open_range', \n",
    "                       'vwap', 'sma_20', 'sma_50', 'price_position', 'atr']\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            if col in df.columns:\n",
    "                # Check for infinite values\n",
    "                inf_count = np.isinf(df[col]).sum()\n",
    "                if inf_count > 0:\n",
    "                    logger.warning(f\"Replacing {inf_count} infinite values in {col}\")\n",
    "                    df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "                \n",
    "                # Check for NaN values\n",
    "                nan_count = df[col].isna().sum()\n",
    "                if nan_count > 0:\n",
    "                    logger.warning(f\"Found {nan_count} NaN values in {col}, filling with 0\")\n",
    "                    df[col] = df[col].fillna(0)\n",
    "        \n",
    "        logger.info(f\"Added features to {timeframe} data\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error adding features to {timeframe} data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def align_timeframes(df_30m: pd.DataFrame, df_5m: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Align data timeframes with validation\"\"\"\n",
    "    try:\n",
    "        # Find common time range\n",
    "        start_time = max(df_30m.index[0], df_5m.index[0])\n",
    "        end_time = min(df_30m.index[-1], df_5m.index[-1])\n",
    "        \n",
    "        logger.info(f\"Aligning data from {start_time} to {end_time}\")\n",
    "        \n",
    "        # Filter to common range\n",
    "        df_30m = df_30m[(df_30m.index >= start_time) & (df_30m.index <= end_time)]\n",
    "        df_5m = df_5m[(df_5m.index >= start_time) & (df_5m.index <= end_time)]\n",
    "        \n",
    "        # Validate alignment\n",
    "        expected_ratio = 6  # 30min / 5min\n",
    "        actual_ratio = len(df_5m) / len(df_30m) if len(df_30m) > 0 else 0\n",
    "        \n",
    "        if abs(actual_ratio - expected_ratio) > 1:\n",
    "            logger.warning(f\"Unexpected data ratio: {actual_ratio:.2f} (expected ~{expected_ratio})\")\n",
    "        \n",
    "        # Check time alignment\n",
    "        # Every 30m bar should have a corresponding 5m bar\n",
    "        missing_alignments = 0\n",
    "        for ts in df_30m.index:\n",
    "            if ts not in df_5m.index:\n",
    "                missing_alignments += 1\n",
    "        \n",
    "        if missing_alignments > 0:\n",
    "            logger.warning(f\"Found {missing_alignments} 30m timestamps without corresponding 5m data\")\n",
    "        \n",
    "        logger.info(f\"Final data sizes - 30m: {len(df_30m)}, 5m: {len(df_5m)}\")\n",
    "        \n",
    "        return df_30m, df_5m\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error aligning timeframes: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def perform_final_validation(df_30m: pd.DataFrame, df_5m: pd.DataFrame):\n",
    "    \"\"\"Perform final comprehensive validation of loaded data\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Performing final data validation...\")\n",
    "        \n",
    "        # Check for minimum required data\n",
    "        min_days = 30\n",
    "        data_range = (df_30m.index[-1] - df_30m.index[0]).days\n",
    "        \n",
    "        if data_range < min_days:\n",
    "            logger.warning(f\"Data spans only {data_range} days, less than recommended {min_days} days\")\n",
    "        \n",
    "        # Validate data consistency between timeframes\n",
    "        # Check if 30m highs/lows are within 5m range\n",
    "        validation_samples = min(100, len(df_30m))\n",
    "        inconsistencies = 0\n",
    "        \n",
    "        for i in range(validation_samples):\n",
    "            ts_30m = df_30m.index[i]\n",
    "            # Find corresponding 5m bars\n",
    "            mask_5m = (df_5m.index >= ts_30m) & (df_5m.index < ts_30m + pd.Timedelta('30min'))\n",
    "            \n",
    "            if mask_5m.any():\n",
    "                high_5m = df_5m.loc[mask_5m, 'high'].max()\n",
    "                low_5m = df_5m.loc[mask_5m, 'low'].min()\n",
    "                \n",
    "                # Check consistency (with small tolerance for rounding)\n",
    "                tolerance = 0.01  # 1 cent tolerance\n",
    "                if abs(df_30m.loc[ts_30m, 'high'] - high_5m) > tolerance:\n",
    "                    inconsistencies += 1\n",
    "                if abs(df_30m.loc[ts_30m, 'low'] - low_5m) > tolerance:\n",
    "                    inconsistencies += 1\n",
    "        \n",
    "        if inconsistencies > 0:\n",
    "            logger.warning(f\"Found {inconsistencies} price inconsistencies between 30m and 5m data\")\n",
    "        \n",
    "        # Generate data quality report\n",
    "        quality_report = {\n",
    "            '30m_data': {\n",
    "                'total_bars': len(df_30m),\n",
    "                'date_range': f\"{df_30m.index[0]} to {df_30m.index[-1]}\",\n",
    "                'missing_values': df_30m.isnull().sum().sum(),\n",
    "                'zero_volume_bars': (df_30m['volume'] == 0).sum(),\n",
    "                'price_range': f\"${df_30m['low'].min():.2f} - ${df_30m['high'].max():.2f}\"\n",
    "            },\n",
    "            '5m_data': {\n",
    "                'total_bars': len(df_5m),\n",
    "                'date_range': f\"{df_5m.index[0]} to {df_5m.index[-1]}\",\n",
    "                'missing_values': df_5m.isnull().sum().sum(),\n",
    "                'zero_volume_bars': (df_5m['volume'] == 0).sum(),\n",
    "                'price_range': f\"${df_5m['low'].min():.2f} - ${df_5m['high'].max():.2f}\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Data Quality Report:\")\n",
    "        for timeframe, metrics in quality_report.items():\n",
    "            logger.info(f\"\\n{timeframe}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                logger.info(f\"  {metric}: {value}\")\n",
    "        \n",
    "        logger.info(\"Final data validation completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in final validation: {str(e)}\")\n",
    "        # Non-critical error, don't raise\n",
    "\n",
    "# Load the data with error handling\n",
    "try:\n",
    "    df_30m, df_5m = load_data()\n",
    "    logger.info(\"Data loading successful!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Original NW-RQK Implementation from Strategy_Implementation.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === ORIGINAL NW-RQK IMPLEMENTATION FROM STRATEGY_IMPLEMENTATION.IPYNB ===\n\nimport numpy as np\nimport pandas as pd\nfrom numba import jit, njit, prange, float64, boolean\n\n# Define parameters (matching PineScript defaults)\nsrc_col = 'Close'  # Default source is close price\nh = 8.0            # Lookback window\nr = 8.0            # Relative weighting\nx_0 = 25           # Start regression at bar\nlag = 2            # Lag for crossover detection\nsmooth_colors = False  # Smooth colors option\n\n# JIT-compiled kernel regression function\n@njit(float64(float64[:], int64, float64, float64))\ndef kernel_regression_numba(src, size, h_param, r_param):\n    \"\"\"\n    Numba-optimized Nadaraya-Watson Regression using Rational Quadratic Kernel\n    \"\"\"\n    current_weight = 0.0\n    cumulative_weight = 0.0\n    \n    # Calculate only up to the available data points\n    for i in range(min(size + x_0 + 1, len(src))):\n        if i < len(src):\n            y = src[i]  # Value i bars back\n            # Rational Quadratic Kernel\n            w = (1 + (i**2 / ((h_param**2) * 2 * r_param)))**(-r_param)\n            current_weight += y * w\n            cumulative_weight += w\n    \n    if cumulative_weight == 0:\n        return np.nan\n    \n    return current_weight / cumulative_weight\n\n# JIT-compiled function to process the entire series\n@njit(parallel=True)\ndef calculate_nw_regression(prices, h_param, h_lag_param, r_param, x_0_param):\n    \"\"\"\n    Calculate Nadaraya-Watson regression for the entire price series\n    \"\"\"\n    n = len(prices)\n    yhat1 = np.full(n, np.nan)\n    yhat2 = np.full(n, np.nan)\n    \n    # Reverse the array once to match PineScript indexing\n    prices_reversed = np.zeros(n)\n    for i in range(n):\n        prices_reversed[i] = prices[n-i-1]\n    \n    # Calculate regression values for each bar in parallel\n    for i in prange(n):\n        if i >= x_0_param:  # Only start calculation after x_0 bars\n            # Create window for current bar\n            window_size = min(i + 1, n)\n            src = np.zeros(window_size)\n            for j in range(window_size):\n                src[j] = prices[i-j]\n            \n            yhat1[i] = kernel_regression_numba(src, i, h_param, r_param)\n            yhat2[i] = kernel_regression_numba(src, i, h_param-lag, r_param)\n    \n    return yhat1, yhat2\n\n# JIT-compiled function to detect crossovers\n@njit\ndef detect_crosses(yhat1, yhat2):\n    \"\"\"\n    Detect crossovers between two series\n    \"\"\"\n    n = len(yhat1)\n    bullish_cross = np.zeros(n, dtype=np.bool_)\n    bearish_cross = np.zeros(n, dtype=np.bool_)\n    \n    for i in range(1, n):\n        if not np.isnan(yhat1[i]) and not np.isnan(yhat2[i]) and \\\n           not np.isnan(yhat1[i-1]) and not np.isnan(yhat2[i-1]):\n            # Bullish cross (yhat2 crosses above yhat1)\n            if yhat2[i] > yhat1[i] and yhat2[i-1] <= yhat1[i-1]:\n                bullish_cross[i] = True\n            \n            # Bearish cross (yhat2 crosses below yhat1)\n            if yhat2[i] < yhat1[i] and yhat2[i-1] >= yhat1[i-1]:\n                bearish_cross[i] = True\n    \n    return bullish_cross, bearish_cross\n\ndef calculate_nw_rqk(df, src_col='Close', h=8.0, r=8.0, x_0=25, lag=2, smooth_colors=False):\n    \"\"\"\n    Calculate Nadaraya-Watson RQK indicator for a dataframe\n    \"\"\"\n    print(\"Calculating Nadaraya-Watson Regression with Rational Quadratic Kernel...\")\n    \n    # Convert to numpy array for Numba\n    prices = df[src_col].values\n    \n    # Calculate regression values using Numba\n    yhat1, yhat2 = calculate_nw_regression(prices, h, h-lag, r, x_0)\n    \n    # Add regression values to dataframe\n    df['yhat1'] = yhat1\n    df['yhat2'] = yhat2\n    \n    # Calculate rates of change (vectorized)\n    df['wasBearish'] = df['yhat1'].shift(2) > df['yhat1'].shift(1)\n    df['wasBullish'] = df['yhat1'].shift(2) < df['yhat1'].shift(1)\n    df['isBearish'] = df['yhat1'].shift(1) > df['yhat1']\n    df['isBullish'] = df['yhat1'].shift(1) < df['yhat1']\n    df['isBearishChange'] = df['isBearish'] & df['wasBullish']\n    df['isBullishChange'] = df['isBullish'] & df['wasBearish']\n    \n    # Calculate crossovers using Numba\n    bullish_cross, bearish_cross = detect_crosses(yhat1, yhat2)\n    df['isBullishCross'] = bullish_cross\n    df['isBearishCross'] = bearish_cross\n    \n    # Calculate smooth color conditions (vectorized)\n    df['isBullishSmooth'] = df['yhat2'] > df['yhat1']\n    df['isBearishSmooth'] = df['yhat2'] < df['yhat1']\n    \n    # Define colors (matches PineScript)\n    c_bullish = '#3AFF17'  # Green\n    c_bearish = '#FD1707'  # Red\n    \n    # Determine plot colors based on settings (vectorized)\n    df['colorByCross'] = np.where(df['isBullishSmooth'], c_bullish, c_bearish)\n    df['colorByRate'] = np.where(df['isBullish'], c_bullish, c_bearish)\n    df['plotColor'] = df['colorByCross'] if smooth_colors else df['colorByRate']\n    \n    # Calculate alert conditions (vectorized)\n    df['alertBullish'] = df['isBearishCross'] if smooth_colors else df['isBearishChange']\n    df['alertBearish'] = df['isBullishCross'] if smooth_colors else df['isBullishChange']\n    \n    # Generate alert stream (-1 for bearish, 1 for bullish, 0 for no change) (vectorized)\n    df['alertStream'] = np.where(df['alertBearish'], -1,\n                                np.where(df['alertBullish'], 1, 0))\n    \n    # Count signals\n    bullish_changes = df['isBullishChange'].sum()\n    bearish_changes = df['isBearishChange'].sum()\n    bullish_crosses = df['isBullishCross'].sum()\n    bearish_crosses = df['isBearishCross'].sum()\n    \n    print(f\"\\nNW-RQK Signal Summary:\")\n    print(f\"- Bullish Rate Changes: {bullish_changes}\")\n    print(f\"- Bearish Rate Changes: {bearish_changes}\")\n    print(f\"- Bullish Crosses: {bullish_crosses}\")\n    print(f\"- Bearish Crosses: {bearish_crosses}\")\n    \n    return df\n\n# Add validation wrapper for NW-RQK calculation\ndef calculate_nwrqk_with_validation(df, config=None):\n    \"\"\"Calculate NW-RQK with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate input\n        if len(df) < config.NWRQK_X0 * 2:\n            raise ValueError(f\"Insufficient data for NW-RQK: need at least {config.NWRQK_X0 * 2} points\")\n        \n        # Check for required columns\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing 'close' column in dataframe\")\n        \n        # Calculate NW-RQK using original function\n        logger.info(\"Calculating NW-RQK (original implementation)...\")\n        df_result = calculate_nw_rqk(\n            df.copy(),  # Work on a copy to preserve original\n            src_col='close',\n            h=config.NWRQK_H,\n            r=config.NWRQK_R,\n            x_0=config.NWRQK_X0,\n            lag=config.NWRQK_LAG,\n            smooth_colors=config.NWRQK_SMOOTH_COLORS\n        )\n        \n        # Log statistics\n        logger.info(f\"NW-RQK calculation complete\")\n        \n        return df_result\n        \n    except Exception as e:\n        logger.error(f\"Error in NW-RQK calculation: {str(e)}\")\n        raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Original MLMI Implementation from Strategy_Implementation.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === ORIGINAL MLMI IMPLEMENTATION FROM STRATEGY_IMPLEMENTATION.IPYNB ===\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit, prange, float64, int64, boolean\nfrom numba.experimental import jitclass\nfrom scipy.spatial import cKDTree  # Using cKDTree for fast kNN\n\n# Define spec for jitclass\nspec = [\n    ('parameter1', float64[:]),\n    ('parameter2', float64[:]),\n    ('priceArray', float64[:]),\n    ('resultArray', int64[:]),\n    ('size', int64)\n]\n\n# Create a JIT-compiled MLMI data class for maximum performance\n@jitclass(spec)\nclass MLMIDataFast:\n    def __init__(self, max_size=10000):\n        # Pre-allocate arrays with maximum size for better performance\n        self.parameter1 = np.zeros(max_size, dtype=np.float64)\n        self.parameter2 = np.zeros(max_size, dtype=np.float64)\n        self.priceArray = np.zeros(max_size, dtype=np.float64)\n        self.resultArray = np.zeros(max_size, dtype=np.int64)\n        self.size = 0\n    \n    def storePreviousTrade(self, p1, p2, close_price):\n        if self.size > 0:\n            # Calculate result before modifying current values\n            result = 1 if close_price >= self.priceArray[self.size-1] else -1\n            \n            # Increment size and add new entry\n            self.size += 1\n            self.parameter1[self.size-1] = p1\n            self.parameter2[self.size-1] = p2\n            self.priceArray[self.size-1] = close_price\n            self.resultArray[self.size-1] = result\n        else:\n            # First entry\n            self.parameter1[0] = p1\n            self.parameter2[0] = p2\n            self.priceArray[0] = close_price\n            self.resultArray[0] = 0  # Neutral for first entry\n            self.size = 1\n\n# Optimized core functions with parallel processing\n@njit(fastmath=True, parallel=True)\ndef wma_numba_fast(series, length):\n    \"\"\"Ultra-optimized Weighted Moving Average calculation\"\"\"\n    n = len(series)\n    result = np.zeros(n, dtype=np.float64)\n    \n    # Pre-calculate weights (constant throughout calculation)\n    weights = np.arange(1, length + 1, dtype=np.float64)\n    sum_weights = np.sum(weights)\n    \n    # Parallel processing of WMA calculation\n    for i in prange(length-1, n):\n        weighted_sum = 0.0\n        # Inline loop for better performance\n        for j in range(length):\n            weighted_sum += series[i-j] * weights[length-j-1]\n        result[i] = weighted_sum / sum_weights\n    \n    return result\n\n@njit(fastmath=True)\ndef calculate_rsi_numba_fast(prices, window):\n    \"\"\"Ultra-optimized RSI calculation\"\"\"\n    n = len(prices)\n    rsi = np.zeros(n, dtype=np.float64)\n    \n    # Pre-allocate arrays for better memory performance\n    delta = np.zeros(n, dtype=np.float64)\n    gain = np.zeros(n, dtype=np.float64)\n    loss = np.zeros(n, dtype=np.float64)\n    avg_gain = np.zeros(n, dtype=np.float64)\n    avg_loss = np.zeros(n, dtype=np.float64)\n    \n    # Calculate deltas in one pass\n    for i in range(1, n):\n        delta[i] = prices[i] - prices[i-1]\n        # Separate gains and losses in the same loop\n        if delta[i] > 0:\n            gain[i] = delta[i]\n        else:\n            loss[i] = -delta[i]\n    \n    # First value uses simple average\n    if window <= n:\n        avg_gain[window-1] = np.sum(gain[:window]) / window\n        avg_loss[window-1] = np.sum(loss[:window]) / window\n        \n        # Calculate RSI for first window point\n        if avg_loss[window-1] == 0:\n            rsi[window-1] = 100\n        else:\n            rs = avg_gain[window-1] / avg_loss[window-1]\n            rsi[window-1] = 100 - (100 / (1 + rs))\n    \n    # Apply Wilder's smoothing for subsequent values with optimized calculation\n    window_minus_one = window - 1\n    window_recip = 1.0 / window\n    for i in range(window, n):\n        avg_gain[i] = (avg_gain[i-1] * window_minus_one + gain[i]) * window_recip\n        avg_loss[i] = (avg_loss[i-1] * window_minus_one + loss[i]) * window_recip\n        \n        # Calculate RSI directly\n        if avg_loss[i] == 0:\n            rsi[i] = 100\n        else:\n            rs = avg_gain[i] / avg_loss[i]\n            rsi[i] = 100 - (100 / (1 + rs))\n    \n    return rsi\n\n# Use cKDTree for lightning-fast kNN queries\ndef fast_knn_predict(param1_array, param2_array, result_array, p1, p2, k, size):\n    \"\"\"\n    Ultra-fast kNN prediction using scipy.spatial.cKDTree\n    \"\"\"\n    # Handle empty data case\n    if size == 0:\n        return 0\n    \n    # Create points array for KDTree\n    points = np.column_stack((param1_array[:size], param2_array[:size]))\n    \n    # Create KDTree for fast nearest neighbor search\n    tree = cKDTree(points)\n    \n    # Query KDTree for k nearest neighbors\n    distances, indices = tree.query([p1, p2], k=min(k, size))\n    \n    # Get results of nearest neighbors\n    neighbors = result_array[indices]\n    \n    # Return prediction (sum of neighbor results)\n    return np.sum(neighbors)\n\ndef calculate_mlmi_optimized(df, num_neighbors=200, momentum_window=20):\n    \"\"\"\n    Highly optimized MLMI calculation function\n    \"\"\"\n    print(\"Preparing data for MLMI calculation...\")\n    # Get numpy arrays for better performance\n    close_array = df['Close'].values\n    n = len(close_array)\n    \n    # Pre-allocate all output arrays at once\n    ma_quick = np.zeros(n, dtype=np.float64)\n    ma_slow = np.zeros(n, dtype=np.float64)\n    rsi_quick = np.zeros(n, dtype=np.float64)\n    rsi_slow = np.zeros(n, dtype=np.float64)\n    rsi_quick_wma = np.zeros(n, dtype=np.float64)\n    rsi_slow_wma = np.zeros(n, dtype=np.float64)\n    pos = np.zeros(n, dtype=np.bool_)\n    neg = np.zeros(n, dtype=np.bool_)\n    mlmi_values = np.zeros(n, dtype=np.float64)\n    \n    print(\"Calculating RSI and moving averages...\")\n    # Calculate indicators with optimized functions\n    ma_quick = wma_numba_fast(close_array, 5)\n    ma_slow = wma_numba_fast(close_array, 20)\n    \n    # Calculate RSI with optimized function\n    rsi_quick = calculate_rsi_numba_fast(close_array, 5)\n    rsi_slow = calculate_rsi_numba_fast(close_array, 20)\n    \n    # Apply WMA to RSI values\n    rsi_quick_wma = wma_numba_fast(rsi_quick, momentum_window)\n    rsi_slow_wma = wma_numba_fast(rsi_slow, momentum_window)\n    \n    # Detect MA crossovers (vectorized where possible)\n    print(\"Detecting moving average crossovers...\")\n    for i in range(1, n):\n        if ma_quick[i] > ma_slow[i] and ma_quick[i-1] <= ma_slow[i-1]:\n            pos[i] = True\n        if ma_quick[i] < ma_slow[i] and ma_quick[i-1] >= ma_slow[i-1]:\n            neg[i] = True\n    \n    # Initialize optimized MLMI data object\n    mlmi_data = MLMIDataFast(max_size=min(10000, n))  # Pre-allocate with reasonable size\n    \n    print(\"Processing crossovers and calculating MLMI values...\")\n    # Process data with batch processing for performance\n    crossover_indices = np.where(pos | neg)[0]\n    \n    # Process crossovers in a single pass\n    for i in crossover_indices:\n        if not np.isnan(rsi_slow_wma[i]) and not np.isnan(rsi_quick_wma[i]):\n            mlmi_data.storePreviousTrade(\n                rsi_slow_wma[i],\n                rsi_quick_wma[i],\n                close_array[i]\n            )\n    \n    # Batch kNN predictions for performance\n    # Only calculate for points after momentum_window\n    for i in range(momentum_window, n):\n        if not np.isnan(rsi_slow_wma[i]) and not np.isnan(rsi_quick_wma[i]):\n            # Use fast KDTree-based kNN prediction\n            if mlmi_data.size > 0:\n                mlmi_values[i] = fast_knn_predict(\n                    mlmi_data.parameter1,\n                    mlmi_data.parameter2,\n                    mlmi_data.resultArray,\n                    rsi_slow_wma[i],\n                    rsi_quick_wma[i],\n                    num_neighbors,\n                    mlmi_data.size\n                )\n    \n    # Add results to dataframe (do this all at once)\n    df_result = df.copy()\n    df_result['ma_quick'] = ma_quick\n    df_result['ma_slow'] = ma_slow\n    df_result['rsi_quick'] = rsi_quick\n    df_result['rsi_slow'] = rsi_slow\n    df_result['rsi_quick_wma'] = rsi_quick_wma\n    df_result['rsi_slow_wma'] = rsi_slow_wma\n    df_result['pos'] = pos\n    df_result['neg'] = neg\n    df_result['mlmi'] = mlmi_values\n    \n    # Calculate WMA of MLMI\n    df_result['mlmi_ma'] = wma_numba_fast(mlmi_values, 20)\n    \n    # Calculate bands and other derived values\n    print(\"Calculating bands and crossovers...\")\n    \n    # Use vectorized operations for bands calculation\n    highest_values = pd.Series(mlmi_values).rolling(window=2000, min_periods=1).max().values\n    lowest_values = pd.Series(mlmi_values).rolling(window=2000, min_periods=1).min().values\n    mlmi_std = pd.Series(mlmi_values).rolling(window=20).std().values\n    ema_std = pd.Series(mlmi_std).ewm(span=20).mean().values\n    \n    # Add band values to dataframe\n    df_result['upper'] = highest_values\n    df_result['lower'] = lowest_values\n    df_result['upper_band'] = highest_values - ema_std\n    df_result['lower_band'] = lowest_values + ema_std\n    \n    # Generate crossover signals (vectorized where possible)\n    mlmi_bull_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_bear_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_ob_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_ob_exit = np.zeros(n, dtype=np.bool_)\n    mlmi_os_cross = np.zeros(n, dtype=np.bool_)\n    mlmi_os_exit = np.zeros(n, dtype=np.bool_)\n    mlmi_mid_up = np.zeros(n, dtype=np.bool_)\n    mlmi_mid_down = np.zeros(n, dtype=np.bool_)\n    \n    # Calculate crossovers in one pass for better performance\n    for i in range(1, n):\n        if not np.isnan(mlmi_values[i]) and not np.isnan(mlmi_values[i-1]):\n            # MA crossovers\n            if mlmi_values[i] > df_result['mlmi_ma'].iloc[i] and mlmi_values[i-1] <= df_result['mlmi_ma'].iloc[i-1]:\n                mlmi_bull_cross[i] = True\n            if mlmi_values[i] < df_result['mlmi_ma'].iloc[i] and mlmi_values[i-1] >= df_result['mlmi_ma'].iloc[i-1]:\n                mlmi_bear_cross[i] = True\n                \n            # Overbought/Oversold crossovers\n            if mlmi_values[i] > df_result['upper_band'].iloc[i] and mlmi_values[i-1] <= df_result['upper_band'].iloc[i-1]:\n                mlmi_ob_cross[i] = True\n            if mlmi_values[i] < df_result['upper_band'].iloc[i] and mlmi_values[i-1] >= df_result['upper_band'].iloc[i-1]:\n                mlmi_ob_exit[i] = True\n            if mlmi_values[i] < df_result['lower_band'].iloc[i] and mlmi_values[i-1] >= df_result['lower_band'].iloc[i-1]:\n                mlmi_os_cross[i] = True\n            if mlmi_values[i] > df_result['lower_band'].iloc[i] and mlmi_values[i-1] <= df_result['lower_band'].iloc[i-1]:\n                mlmi_os_exit[i] = True\n                \n            # Zero-line crosses\n            if mlmi_values[i] > 0 and mlmi_values[i-1] <= 0:\n                mlmi_mid_up[i] = True\n            if mlmi_values[i] < 0 and mlmi_values[i-1] >= 0:\n                mlmi_mid_down[i] = True\n    \n    # Add crossover signals to dataframe\n    df_result['mlmi_bull_cross'] = mlmi_bull_cross\n    df_result['mlmi_bear_cross'] = mlmi_bear_cross\n    df_result['mlmi_ob_cross'] = mlmi_ob_cross\n    df_result['mlmi_ob_exit'] = mlmi_ob_exit\n    df_result['mlmi_os_cross'] = mlmi_os_cross\n    df_result['mlmi_os_exit'] = mlmi_os_exit\n    df_result['mlmi_mid_up'] = mlmi_mid_up\n    df_result['mlmi_mid_down'] = mlmi_mid_down\n    \n    # Count signals\n    bull_crosses = np.sum(mlmi_bull_cross)\n    bear_crosses = np.sum(mlmi_bear_cross)\n    ob_cross = np.sum(mlmi_ob_cross)\n    ob_exit = np.sum(mlmi_ob_exit)\n    os_cross = np.sum(mlmi_os_cross)\n    os_exit = np.sum(mlmi_os_exit)\n    zero_up = np.sum(mlmi_mid_up)\n    zero_down = np.sum(mlmi_mid_down)\n    \n    print(f\"\\nMLMI Signal Summary:\")\n    print(f\"- Bullish MA Crosses: {bull_crosses}\")\n    print(f\"- Bearish MA Crosses: {bear_crosses}\")\n    print(f\"- Overbought Crosses: {ob_cross}\")\n    print(f\"- Overbought Exits: {ob_exit}\")\n    print(f\"- Oversold Crosses: {os_cross}\")\n    print(f\"- Oversold Exits: {os_exit}\")\n    print(f\"- Zero Line Crosses Up: {zero_up}\")\n    print(f\"- Zero Line Crosses Down: {zero_down}\")\n    \n    return df_result\n\n# Add validation wrapper for MLMI calculation\ndef calculate_mlmi_with_validation(df, config=None):\n    \"\"\"Calculate MLMI with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate input\n        if len(df) < 100:  # Need enough data for meaningful MA crossovers\n            raise ValueError(f\"Insufficient data for MLMI: need at least 100 points\")\n        \n        # Check for required columns\n        if 'Close' not in df.columns:\n            raise ValueError(\"Missing 'Close' column in dataframe\")\n        \n        # Calculate MLMI using original function\n        logger.info(\"Calculating MLMI (original implementation)...\")\n        df_result = calculate_mlmi_optimized(\n            df.copy(),  # Work on a copy to preserve original\n            num_neighbors=config.MLMI_NUM_NEIGHBORS,\n            momentum_window=config.MLMI_MOMENTUM_WINDOW\n        )\n        \n        # Log statistics\n        logger.info(f\"MLMI calculation complete\")\n        \n        return df_result\n        \n    except Exception as e:\n        logger.error(f\"Error in MLMI calculation: {str(e)}\")\n        raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Original FVG Implementation from Strategy_Implementation.ipynb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === ORIGINAL FVG IMPLEMENTATION FROM STRATEGY_IMPLEMENTATION.IPYNB ===\n\ndef detect_fvg(df, lookback_period=10, body_multiplier=1.5):\n    \"\"\"\n    Detects Fair Value Gaps (FVGs) in historical price data.\n    \n    Parameters:\n        df (DataFrame): DataFrame with OHLC data\n        lookback_period (int): Number of candles to look back for average body size\n        body_multiplier (float): Multiplier to determine significant body size\n        \n    Returns:\n        list: List of FVG tuples or None values\n    \"\"\"\n    # Create a list to store FVG results\n    fvg_list = [None] * len(df)\n    \n    # Can't form FVG with fewer than 3 candles\n    if len(df) < 3:\n        print(\"Warning: Not enough data points to detect FVGs\")\n        return fvg_list\n    \n    # Start from the third candle (index 2)\n    for i in range(2, len(df)):\n        try:\n            # Get the prices for three consecutive candles\n            first_high = df['High'].iloc[i-2]\n            first_low = df['Low'].iloc[i-2]\n            middle_open = df['Open'].iloc[i-1]\n            middle_close = df['Close'].iloc[i-1]\n            third_low = df['Low'].iloc[i]\n            third_high = df['High'].iloc[i]\n            \n            # Calculate average body size from lookback period\n            start_idx = max(0, i-1-lookback_period)\n            prev_bodies = (df['Close'].iloc[start_idx:i-1] - df['Open'].iloc[start_idx:i-1]).abs()\n            avg_body_size = prev_bodies.mean() if not prev_bodies.empty else 0.001\n            avg_body_size = max(avg_body_size, 0.001)  # Avoid division by zero\n            \n            # Calculate current middle candle body size\n            middle_body = abs(middle_close - middle_open)\n            \n            # Check for Bullish FVG (gap up)\n            if third_low > first_high and middle_body > avg_body_size * body_multiplier:\n                fvg_list[i] = ('bullish', first_high, third_low, i)\n                \n            # Check for Bearish FVG (gap down)\n            elif third_high < first_low and middle_body > avg_body_size * body_multiplier:\n                fvg_list[i] = ('bearish', first_low, third_high, i)\n                \n        except Exception as e:\n            # Skip this candle if there's an error\n            continue\n    \n    return fvg_list\n\n# Helper function to process FVG data for easier use\ndef process_fvg_data(df, fvg_list):\n    \"\"\"\n    Process FVG list into boolean columns and gap information\n    \"\"\"\n    # Initialize columns\n    df['fvg_bull'] = False\n    df['fvg_bear'] = False\n    df['fvg_bull_bottom'] = np.nan\n    df['fvg_bull_top'] = np.nan\n    df['fvg_bear_bottom'] = np.nan\n    df['fvg_bear_top'] = np.nan\n    \n    # Process FVG list\n    for i, fvg in enumerate(fvg_list):\n        if fvg is not None:\n            fvg_type, level1, level2, idx = fvg\n            if fvg_type == 'bullish':\n                df.loc[df.index[i], 'fvg_bull'] = True\n                df.loc[df.index[i], 'fvg_bull_bottom'] = level1\n                df.loc[df.index[i], 'fvg_bull_top'] = level2\n            elif fvg_type == 'bearish':\n                df.loc[df.index[i], 'fvg_bear'] = True\n                df.loc[df.index[i], 'fvg_bear_top'] = level1\n                df.loc[df.index[i], 'fvg_bear_bottom'] = level2\n    \n    # Create active FVG zones with forward fill\n    # Updated to use ffill() instead of fillna(method='ffill')\n    df['active_bull_fvg_top'] = df['fvg_bull_top'].ffill()\n    df['active_bull_fvg_bottom'] = df['fvg_bull_bottom'].ffill()\n    df['active_bear_fvg_top'] = df['fvg_bear_top'].ffill()\n    df['active_bear_fvg_bottom'] = df['fvg_bear_bottom'].ffill()\n    \n    # Process invalidation rules\n    for i in range(1, len(df)):\n        # Check for bullish FVG invalidation\n        if not pd.isna(df['active_bull_fvg_bottom'].iloc[i-1]):\n            if df['Low'].iloc[i] < df['active_bull_fvg_bottom'].iloc[i-1]:\n                df.loc[df.index[i], 'active_bull_fvg_top'] = np.nan\n                df.loc[df.index[i], 'active_bull_fvg_bottom'] = np.nan\n        \n        # Check for bearish FVG invalidation\n        if not pd.isna(df['active_bear_fvg_top'].iloc[i-1]):\n            if df['High'].iloc[i] > df['active_bear_fvg_top'].iloc[i-1]:\n                df.loc[df.index[i], 'active_bear_fvg_top'] = np.nan\n                df.loc[df.index[i], 'active_bear_fvg_bottom'] = np.nan\n    \n    # Create boolean flags for active zones\n    df['is_bull_fvg_active'] = df['active_bull_fvg_top'].notna()\n    df['is_bear_fvg_active'] = df['active_bear_fvg_top'].notna()\n    \n    return df\n\n# Add validation wrapper for FVG calculation\ndef calculate_fvg_with_validation(df, config=None):\n    \"\"\"Calculate FVG with comprehensive validation\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Validate required columns - Note: FVG uses capitalized column names\n        required_columns = ['Open', 'High', 'Low', 'Close']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            # Try lowercase columns\n            lowercase_cols = ['open', 'high', 'low', 'close']\n            if all(col in df.columns for col in lowercase_cols):\n                # Create temporary capitalized columns for FVG\n                df = df.copy()\n                df['Open'] = df['open']\n                df['High'] = df['high']\n                df['Low'] = df['low']\n                df['Close'] = df['close']\n            else:\n                raise ValueError(f\"Missing required columns for FVG: {missing_columns}\")\n        \n        # Validate data length\n        if len(df) < 50:  # Need enough data for lookback\n            raise ValueError(f\"Insufficient data for FVG: need at least 50 points\")\n        \n        # Calculate FVG using original function\n        logger.info(\"Calculating FVG (original implementation)...\")\n        fvg_list = detect_fvg(\n            df,\n            lookback_period=config.FVG_LOOKBACK_PERIOD,\n            body_multiplier=config.FVG_BODY_MULTIPLIER\n        )\n        \n        # Process FVG data into usable format\n        df = process_fvg_data(df, fvg_list)\n        \n        # Count FVGs\n        bull_count = df['fvg_bull'].sum()\n        bear_count = df['fvg_bear'].sum()\n        \n        # Log statistics\n        logger.info(f\"FVG calculation complete - Bull: {bull_count}, Bear: {bear_count}\")\n        \n        return df\n        \n    except Exception as e:\n        logger.error(f\"Error in FVG calculation: {str(e)}\")\n        raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Corrected NW-RQK → MLMI → FVG Synergy Detection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === CORRECTED SYNERGY DETECTION FOR ORIGINAL INDICATORS ===\n\n@njit(parallel=True, fastmath=True, cache=True)\ndef detect_nwrqk_mlmi_fvg_synergy_original(\n    nwrqk_bull_change, nwrqk_bear_change, \n    mlmi_bull_cross, mlmi_bear_cross, mlmi_values,\n    fvg_bull, fvg_bear,\n    window=30):\n    \"\"\"\n    Detect NW-RQK → MLMI → FVG synergy pattern using original indicator outputs\n    \n    Synergy Logic:\n    1. NW-RQK provides initial trend signal (rate change)\n    2. MLMI confirms with crossover signal\n    3. FVG provides final entry validation\n    \"\"\"\n    n = len(nwrqk_bull_change)\n    synergy_bull = np.zeros(n, dtype=np.bool8)\n    synergy_bear = np.zeros(n, dtype=np.bool8)\n    synergy_strength = np.zeros(n)\n    \n    # State tracking\n    nwrqk_active_bull = np.zeros(n, dtype=np.bool8)\n    nwrqk_active_bear = np.zeros(n, dtype=np.bool8)\n    mlmi_confirmed_bull = np.zeros(n, dtype=np.bool8)\n    mlmi_confirmed_bear = np.zeros(n, dtype=np.bool8)\n    \n    # Track timing\n    nwrqk_bull_time = -window\n    nwrqk_bear_time = -window\n    mlmi_bull_time = -window\n    mlmi_bear_time = -window\n    last_bull_synergy = -window\n    last_bear_synergy = -window\n    \n    for i in prange(1, n):\n        # Carry forward states\n        if i > 0:\n            nwrqk_active_bull[i] = nwrqk_active_bull[i-1]\n            nwrqk_active_bear[i] = nwrqk_active_bear[i-1]\n            mlmi_confirmed_bull[i] = mlmi_confirmed_bull[i-1]\n            mlmi_confirmed_bear[i] = mlmi_confirmed_bear[i-1]\n        \n        # Step 1: NW-RQK signal (rate change)\n        if nwrqk_bull_change[i]:\n            nwrqk_active_bull[i] = True\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n            nwrqk_bull_time = i\n        elif nwrqk_bear_change[i]:\n            nwrqk_active_bear[i] = True\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n            nwrqk_bear_time = i\n        \n        # Step 2: MLMI confirmation (MA crossover)\n        if nwrqk_active_bull[i] and mlmi_bull_cross[i]:\n            mlmi_confirmed_bull[i] = True\n            mlmi_bull_time = i\n        elif nwrqk_active_bear[i] and mlmi_bear_cross[i]:\n            mlmi_confirmed_bear[i] = True\n            mlmi_bear_time = i\n        \n        # Step 3: FVG validation for entry\n        if mlmi_confirmed_bull[i] and fvg_bull[i] and (i - last_bull_synergy) >= 5:\n            synergy_bull[i] = True\n            last_bull_synergy = i\n            \n            # Calculate synergy strength based on MLMI value\n            if not np.isnan(mlmi_values[i]):\n                synergy_strength[i] = min(abs(mlmi_values[i]) / 100, 1.0)\n            else:\n                synergy_strength[i] = 0.5\n            \n            # Reset states\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n            \n        elif mlmi_confirmed_bear[i] and fvg_bear[i] and (i - last_bear_synergy) >= 5:\n            synergy_bear[i] = True\n            last_bear_synergy = i\n            \n            # Calculate synergy strength based on MLMI value\n            if not np.isnan(mlmi_values[i]):\n                synergy_strength[i] = min(abs(mlmi_values[i]) / 100, 1.0)\n            else:\n                synergy_strength[i] = 0.5\n            \n            # Reset states\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n        \n        # State decay - reset if too old\n        if i - nwrqk_bull_time > window:\n            nwrqk_active_bull[i] = False\n            mlmi_confirmed_bull[i] = False\n        \n        if i - nwrqk_bear_time > window:\n            nwrqk_active_bear[i] = False\n            mlmi_confirmed_bear[i] = False\n    \n    return synergy_bull, synergy_bear, synergy_strength\n\n# Wrapper function for synergy detection\ndef detect_synergy_with_validation_original(df_30m, df_5m_aligned, config=None):\n    \"\"\"Detect synergies using original indicator outputs\"\"\"\n    try:\n        if config is None:\n            config = StrategyConfig\n        \n        # Extract required signals from NW-RQK\n        nwrqk_bull = df_30m['isBullishChange'].values\n        nwrqk_bear = df_30m['isBearishChange'].values\n        \n        # Extract required signals from MLMI\n        mlmi_bull = df_30m['mlmi_bull_cross'].values\n        mlmi_bear = df_30m['mlmi_bear_cross'].values\n        mlmi_values = df_30m['mlmi'].values\n        \n        # Extract FVG signals (aligned to 30m)\n        fvg_bull = df_5m_aligned['fvg_bull'].values\n        fvg_bear = df_5m_aligned['fvg_bear'].values\n        \n        # Detect synergies\n        logger.info(\"Detecting NW-RQK → MLMI → FVG synergies (original logic)...\")\n        synergy_bull, synergy_bear, synergy_strength = detect_nwrqk_mlmi_fvg_synergy_original(\n            nwrqk_bull, nwrqk_bear,\n            mlmi_bull, mlmi_bear, mlmi_values,\n            fvg_bull, fvg_bear,\n            window=config.SYNERGY_WINDOW\n        )\n        \n        # Log statistics\n        total_synergies = synergy_bull.sum() + synergy_bear.sum()\n        logger.info(f\"Synergy detection complete - Bull: {synergy_bull.sum()}, Bear: {synergy_bear.sum()}\")\n        \n        return synergy_bull, synergy_bear, synergy_strength\n        \n    except Exception as e:\n        logger.error(f\"Error in synergy detection: {str(e)}\")\n        raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Complete Strategy Implementation with Original Indicators"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_nwrqk_mlmi_fvg_strategy_corrected(df_30m, df_5m):\n    \"\"\"Execute the corrected NW-RQK → MLMI → FVG strategy with original indicators\"\"\"\n    logger.info(\"\\n\" + \"=\"*60)\n    logger.info(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY (CORRECTED)\")\n    logger.info(\"Using original indicator implementations\")\n    logger.info(\"=\"*60)\n    \n    start_time = time.time()\n    \n    try:\n        # 1. Calculate NW-RQK signals\n        logger.info(\"\\n1. Calculating NW-RQK signals (original implementation)...\")\n        nwrqk_calc_start = time.time()\n        \n        df_30m = calculate_nwrqk_with_validation(df_30m, StrategyConfig)\n        \n        logger.info(f\"   - NW-RQK calculation time: {time.time() - nwrqk_calc_start:.2f}s\")\n        logger.info(f\"   - Bullish rate changes: {df_30m['isBullishChange'].sum()}\")\n        logger.info(f\"   - Bearish rate changes: {df_30m['isBearishChange'].sum()}\")\n        \n        # 2. Calculate MLMI signals\n        logger.info(\"\\n2. Calculating MLMI signals (original implementation)...\")\n        mlmi_calc_start = time.time()\n        \n        df_30m = calculate_mlmi_with_validation(df_30m, StrategyConfig)\n        \n        logger.info(f\"   - MLMI calculation time: {time.time() - mlmi_calc_start:.2f}s\")\n        logger.info(f\"   - Bullish MA crosses: {df_30m['mlmi_bull_cross'].sum()}\")\n        logger.info(f\"   - Bearish MA crosses: {df_30m['mlmi_bear_cross'].sum()}\")\n        \n        # 3. Calculate FVG on 5-minute data\n        logger.info(\"\\n3. Calculating FVG signals on 5m data (original implementation)...\")\n        fvg_calc_start = time.time()\n        \n        df_5m = calculate_fvg_with_validation(df_5m, StrategyConfig)\n        \n        logger.info(f\"   - FVG calculation time: {time.time() - fvg_calc_start:.2f}s\")\n        logger.info(f\"   - Bull FVGs: {df_5m['fvg_bull'].sum()}\")\n        logger.info(f\"   - Bear FVGs: {df_5m['fvg_bear'].sum()}\")\n        \n        # 4. Map 5m FVG to 30m timeframe\n        logger.info(\"\\n4. Mapping FVG signals to 30m timeframe...\")\n        \n        try:\n            # Resample FVG signals\n            fvg_resampled = df_5m[['fvg_bull', 'fvg_bear', 'is_bull_fvg_active', 'is_bear_fvg_active']].resample('30min').agg({\n                'fvg_bull': 'max',\n                'fvg_bear': 'max',\n                'is_bull_fvg_active': 'max',\n                'is_bear_fvg_active': 'max'\n            })\n            \n            # Align with 30m data\n            fvg_aligned = fvg_resampled.reindex(df_30m.index)\n            fvg_aligned = fvg_aligned.ffill()\n            fvg_aligned = fvg_aligned.fillna(False)\n            \n        except Exception as e:\n            logger.error(f\"Error in FVG resampling: {str(e)}\")\n            # Create empty FVG signals as fallback\n            fvg_aligned = pd.DataFrame(index=df_30m.index)\n            fvg_aligned['fvg_bull'] = False\n            fvg_aligned['fvg_bear'] = False\n            fvg_aligned['is_bull_fvg_active'] = False\n            fvg_aligned['is_bear_fvg_active'] = False\n        \n        # 5. Detect synergies\n        logger.info(\"\\n5. Detecting NW-RQK → MLMI → FVG synergies...\")\n        synergy_calc_start = time.time()\n        \n        synergy_bull, synergy_bear, synergy_strength = detect_synergy_with_validation_original(\n            df_30m, fvg_aligned, StrategyConfig\n        )\n        \n        logger.info(f\"   - Synergy detection time: {time.time() - synergy_calc_start:.2f}s\")\n        logger.info(f\"   - Bull synergies: {synergy_bull.sum()}\")\n        logger.info(f\"   - Bear synergies: {synergy_bear.sum()}\")\n        logger.info(f\"   - Total signals: {synergy_bull.sum() + synergy_bear.sum()}\")\n        \n        # 6. Create signals DataFrame\n        signals = pd.DataFrame(index=df_30m.index)\n        signals['synergy_bull'] = synergy_bull\n        signals['synergy_bear'] = synergy_bear\n        signals['synergy_strength'] = synergy_strength\n        signals['price'] = df_30m['close']\n        \n        # Add volatility for risk management\n        signals['volatility'] = df_30m['volatility']\n        \n        # Generate position signals\n        signals['signal'] = 0\n        signals.loc[signals['synergy_bull'], 'signal'] = 1\n        signals.loc[signals['synergy_bear'], 'signal'] = -1\n        \n        # Add signal quality metrics\n        max_volatility = signals['volatility'].quantile(0.95)\n        signals['signal_quality'] = signals['synergy_strength'] * (1 - signals['volatility'] / max_volatility)\n        \n        # Add indicator values for analysis\n        signals['nwrqk_value'] = df_30m['yhat1']\n        signals['mlmi_value'] = df_30m['mlmi']\n        \n        # Performance summary\n        total_time = time.time() - start_time\n        logger.info(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n        logger.info(f\"Signals generated: {(signals['signal'] != 0).sum()}\")\n        \n        if total_time < 10:\n            logger.info(\"✓ Performance: EXCELLENT (< 10s)\")\n        elif total_time < 30:\n            logger.info(\"✓ Performance: GOOD (< 30s)\")\n        else:\n            logger.warning(\"⚠ Performance: SLOW (> 30s)\")\n        \n        return signals\n        \n    except Exception as e:\n        logger.error(f\"Critical error in strategy execution: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        \n        # Return empty signals on error\n        signals = pd.DataFrame(index=df_30m.index)\n        signals['signal'] = 0\n        signals['price'] = df_30m['close']\n        return signals\n\n# Run the corrected strategy\ntry:\n    signals = run_nwrqk_mlmi_fvg_strategy_corrected(df_30m, df_5m)\n    logger.info(\"Corrected strategy execution completed successfully\")\nexcept Exception as e:\n    logger.error(f\"Failed to run corrected strategy: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. VectorBT Backtesting with Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EASY PARAMETER ADJUSTMENT CELL\n",
    "# Modify these parameters to see their effect on backtest results\n",
    "\n",
    "# Trading Parameters - Change these values to see different results!\n",
    "CUSTOM_INITIAL_CAPITAL = 100000.0  # Starting capital in dollars\n",
    "CUSTOM_POSITION_SIZE = 0.1         # Position size as fraction of capital (0.1 = 10%)\n",
    "CUSTOM_STOP_LOSS = 0.02           # Stop loss percentage (0.02 = 2%)\n",
    "CUSTOM_TAKE_PROFIT = 0.03         # Take profit percentage (0.03 = 3%)\n",
    "CUSTOM_FEES = 0.001               # Trading fees (0.001 = 0.1%)\n",
    "\n",
    "# Update the configuration for next run\n",
    "StrategyConfig.INITIAL_CAPITAL = CUSTOM_INITIAL_CAPITAL\n",
    "StrategyConfig.POSITION_SIZE_BASE = CUSTOM_POSITION_SIZE\n",
    "StrategyConfig.STOP_LOSS_PCT = CUSTOM_STOP_LOSS\n",
    "StrategyConfig.TAKE_PROFIT_PCT = CUSTOM_TAKE_PROFIT\n",
    "StrategyConfig.TRADING_FEES = CUSTOM_FEES\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"PARAMETER UPDATE\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Initial Capital: ${CUSTOM_INITIAL_CAPITAL:,.2f}\")\n",
    "logger.info(f\"Position Size: {CUSTOM_POSITION_SIZE * 100:.1f}%\")\n",
    "logger.info(f\"Stop Loss: {CUSTOM_STOP_LOSS * 100:.1f}%\")\n",
    "logger.info(f\"Take Profit: {CUSTOM_TAKE_PROFIT * 100:.1f}%\")\n",
    "logger.info(f\"Trading Fees: {CUSTOM_FEES * 100:.2f}%\")\n",
    "logger.info(\"\\nParameters updated! Run the backtest cell below to see the effect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vectorbt_backtest(signals, initial_capital=None, position_size=None, \n",
    "                         sl_pct=None, tp_pct=None, fees=None):\n",
    "    \"\"\"Run VectorBT backtest with dynamic position sizing and risk management\"\"\"\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"VECTORBT BACKTEST WITH RISK MANAGEMENT\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Use configuration values if not provided\n",
    "    if initial_capital is None:\n",
    "        initial_capital = StrategyConfig.INITIAL_CAPITAL\n",
    "    if position_size is None:\n",
    "        position_size = StrategyConfig.POSITION_SIZE_BASE\n",
    "    if sl_pct is None:\n",
    "        sl_pct = StrategyConfig.STOP_LOSS_PCT\n",
    "    if tp_pct is None:\n",
    "        tp_pct = StrategyConfig.TAKE_PROFIT_PCT\n",
    "    if fees is None:\n",
    "        fees = StrategyConfig.TRADING_FEES\n",
    "        \n",
    "    logger.info(f\"Backtest Parameters:\")\n",
    "    logger.info(f\"  Initial Capital: ${initial_capital:,.2f}\")\n",
    "    logger.info(f\"  Position Size: {position_size * 100:.1f}%\")\n",
    "    logger.info(f\"  Stop Loss: {sl_pct * 100:.1f}%\")\n",
    "    logger.info(f\"  Take Profit: {tp_pct * 100:.1f}%\")\n",
    "    logger.info(f\"  Trading Fees: {fees * 100:.2f}%\")\n",
    "    logger.info(f\"  Slippage: {StrategyConfig.SLIPPAGE * 100:.2f}%\")\n",
    "    \n",
    "    backtest_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Validate signals\n",
    "        if signals is None or len(signals) == 0:\n",
    "            raise ValueError(\"Invalid signals data\")\n",
    "        \n",
    "        if 'signal' not in signals.columns or 'price' not in signals.columns:\n",
    "            raise ValueError(\"Signals must contain 'signal' and 'price' columns\")\n",
    "        \n",
    "        # Prepare data\n",
    "        price = signals['price']\n",
    "        entries = signals['signal'] == 1\n",
    "        exits = signals['signal'] == -1\n",
    "        \n",
    "        # Check if we have any signals\n",
    "        if not entries.any() and not exits.any():\n",
    "            logger.warning(\"No trading signals generated - check strategy parameters\")\n",
    "            return None, None\n",
    "        \n",
    "        # Dynamic position sizing based on signal strength and volatility\n",
    "        if 'synergy_strength' in signals.columns and 'volatility' in signals.columns:\n",
    "            # Scale position size by signal strength (0.5 to 1.5x base size)\n",
    "            strength_factor = 0.5 + 0.5 * np.minimum(signals['synergy_strength'], 1.0)\n",
    "            \n",
    "            # Reduce position size in high volatility (0.5 to 1.0x)\n",
    "            vol_percentile = signals['volatility'].rolling(252).apply(\n",
    "                lambda x: stats.rankdata(x)[-1] / len(x) if len(x) > 0 else 0.5\n",
    "            ).fillna(0.5)\n",
    "            vol_factor = 1.0 - 0.5 * vol_percentile\n",
    "            \n",
    "            # Combined position sizing\n",
    "            position_sizes = position_size * strength_factor * vol_factor\n",
    "            position_sizes = position_sizes.fillna(position_size)\n",
    "            \n",
    "            # Apply position size limits\n",
    "            position_sizes = np.clip(position_sizes, \n",
    "                                   position_size * 0.5,  # Min 50% of base\n",
    "                                   position_size * 1.5)  # Max 150% of base\n",
    "        else:\n",
    "            position_sizes = position_size\n",
    "        \n",
    "        # Performance optimization: Configure VectorBT for speed\n",
    "        vbt.settings['numba']['parallel'] = True\n",
    "        vbt.settings['numba']['cache'] = True\n",
    "        vbt.settings['chunk_len'] = 100000  # Process in chunks for memory efficiency\n",
    "        \n",
    "        # Run backtest with VectorBT\n",
    "        portfolio = vbt.Portfolio.from_signals(\n",
    "            price,\n",
    "            entries=entries,\n",
    "            exits=exits,\n",
    "            size=position_sizes,\n",
    "            size_type='percent',\n",
    "            init_cash=initial_capital,\n",
    "            fees=fees,\n",
    "            slippage=StrategyConfig.SLIPPAGE,\n",
    "            freq='30min',\n",
    "            sl_stop=sl_pct,\n",
    "            tp_stop=tp_pct,\n",
    "            stop_exit_price='close',  # Use close price for stops\n",
    "            upon_stop_exit='close_position',  # Close full position on stop\n",
    "            raise_reject=False,  # Don't raise on rejected orders\n",
    "            log=True,  # Enable logging for debugging\n",
    "            cash_sharing=True,  # Share cash between trades for efficiency\n",
    "            call_seq='auto'  # Optimize call sequence automatically\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics with error handling\n",
    "        try:\n",
    "            # Use cached stats calculation for performance\n",
    "            stats = portfolio.stats(\n",
    "                silence_warnings=True,\n",
    "                metrics=[\n",
    "                    'Total Return [%]',\n",
    "                    'Sharpe Ratio',\n",
    "                    'Sortino Ratio',\n",
    "                    'Calmar Ratio',\n",
    "                    'Max Drawdown [%]',\n",
    "                    'Win Rate [%]',\n",
    "                    'Total Trades',\n",
    "                    'Avg Winning Trade [%]',\n",
    "                    'Avg Losing Trade [%]',\n",
    "                    'Profit Factor',\n",
    "                    'Expectancy [%]'\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Additional risk metrics with performance optimization\n",
    "            returns = portfolio.returns()\n",
    "            \n",
    "            # Maximum consecutive losses (optimized)\n",
    "            if hasattr(portfolio.trades, 'records_readable'):\n",
    "                trades_df = portfolio.trades.records_readable\n",
    "                if len(trades_df) > 0:\n",
    "                    trade_returns = trades_df['Return [%]'].values\n",
    "                    losing_mask = trade_returns < 0\n",
    "                    \n",
    "                    # Vectorized consecutive losses calculation\n",
    "                    max_consecutive_losses = 0\n",
    "                    if losing_mask.any():\n",
    "                        # Group consecutive True values\n",
    "                        changes = np.diff(np.concatenate(([False], losing_mask, [False])).astype(int))\n",
    "                        starts = np.where(changes == 1)[0]\n",
    "                        ends = np.where(changes == -1)[0]\n",
    "                        if len(starts) > 0 and len(ends) > 0:\n",
    "                            consecutive_lengths = ends - starts\n",
    "                            max_consecutive_losses = consecutive_lengths.max()\n",
    "                else:\n",
    "                    max_consecutive_losses = 0\n",
    "            else:\n",
    "                max_consecutive_losses = 0\n",
    "            \n",
    "            # Risk-adjusted metrics (optimized calculation)\n",
    "            downside_returns = returns[returns < 0]\n",
    "            if len(downside_returns) > 0:\n",
    "                downside_std = downside_returns.std()\n",
    "                sortino_ratio = (returns.mean() / downside_std) * np.sqrt(252 * 48) if downside_std > 0 else 0\n",
    "            else:\n",
    "                sortino_ratio = np.inf\n",
    "            \n",
    "            # Add custom metrics to stats\n",
    "            stats['Max Consecutive Losses'] = max_consecutive_losses\n",
    "            stats['Sortino Ratio (Custom)'] = sortino_ratio\n",
    "            stats['Average Position Size'] = position_sizes.mean() if isinstance(position_sizes, pd.Series) else position_size\n",
    "            \n",
    "            # Performance timing\n",
    "            stats['Backtest Execution Time'] = time.time() - backtest_start\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating portfolio statistics: {str(e)}\")\n",
    "            stats = {'Error': str(e)}\n",
    "        \n",
    "        logger.info(f\"\\nBacktest execution time: {time.time() - backtest_start:.2f} seconds\")\n",
    "        \n",
    "        # Log key performance metrics\n",
    "        if 'Total Return [%]' in stats:\n",
    "            logger.info(\"\\nKey Performance Metrics:\")\n",
    "            logger.info(f\"Total Return: {stats.get('Total Return [%]', 'N/A'):.2f}%\")\n",
    "            logger.info(f\"Sharpe Ratio: {stats.get('Sharpe Ratio', 'N/A'):.2f}\")\n",
    "            logger.info(f\"Max Drawdown: {stats.get('Max Drawdown [%]', 'N/A'):.2f}%\")\n",
    "            logger.info(f\"Win Rate: {stats.get('Win Rate [%]', 'N/A'):.2f}%\")\n",
    "            logger.info(f\"Total Trades: {stats.get('Total Trades', 'N/A')}\")\n",
    "            \n",
    "            # Calculate annual metrics\n",
    "            if price.index[-1] and price.index[0]:\n",
    "                n_years = (price.index[-1] - price.index[0]).days / 365.25\n",
    "                if n_years > 0 and stats.get('Total Return [%]'):\n",
    "                    annual_return = (1 + stats['Total Return [%]'] / 100) ** (1 / n_years) - 1\n",
    "                    trades_per_year = stats.get('Total Trades', 0) / n_years\n",
    "                    \n",
    "                    logger.info(f\"\\nAnnualized Return: {annual_return * 100:.2f}%\")\n",
    "                    logger.info(f\"Trades per Year: {trades_per_year:.0f}\")\n",
    "            \n",
    "            # Risk warnings\n",
    "            if stats.get('Max Drawdown [%]', 0) > StrategyConfig.MAX_DRAWDOWN_LIMIT * 100:\n",
    "                logger.warning(f\"⚠️  Maximum drawdown exceeds limit: {stats['Max Drawdown [%]']:.2f}% > {StrategyConfig.MAX_DRAWDOWN_LIMIT * 100:.0f}%\")\n",
    "            \n",
    "            if stats.get('Win Rate [%]', 0) < 40:\n",
    "                logger.warning(f\"⚠️  Low win rate: {stats['Win Rate [%]']:.2f}%\")\n",
    "            \n",
    "            # Performance grade\n",
    "            backtest_time = stats.get('Backtest Execution Time', 0)\n",
    "            if backtest_time < 5:\n",
    "                logger.info(\"✓ Backtest Performance: EXCELLENT (< 5s)\")\n",
    "            elif backtest_time < 15:\n",
    "                logger.info(\"✓ Backtest Performance: GOOD (< 15s)\")\n",
    "            else:\n",
    "                logger.warning(\"⚠ Backtest Performance: SLOW (> 15s)\")\n",
    "        \n",
    "        return portfolio, stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error in backtesting: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None, {'Error': str(e)}\n",
    "\n",
    "# Check if required variables exist\n",
    "if 'signals' not in globals():\n",
    "    logger.error(\"Signals not found. Please run the strategy cell first.\")\n",
    "    raise RuntimeError(\"Cannot run backtest without signals. Please execute the strategy cell first.\")\n",
    "\n",
    "if 'StrategyConfig' not in globals():\n",
    "    logger.error(\"StrategyConfig not found. Please run the configuration cell first.\")\n",
    "    raise RuntimeError(\"Cannot run backtest without configuration. Please execute the imports/configuration cell first.\")\n",
    "\n",
    "# Import required modules if not already imported\n",
    "if 'vbt' not in globals():\n",
    "    logger.warning(\"VectorBT not imported. Importing now...\")\n",
    "    import vectorbt as vbt\n",
    "\n",
    "if 'stats' not in globals():\n",
    "    from scipy import stats\n",
    "\n",
    "if 'traceback' not in globals():\n",
    "    import traceback\n",
    "\n",
    "# Run backtest with error handling\n",
    "try:\n",
    "    portfolio, stats = run_vectorbt_backtest(signals)\n",
    "    if portfolio is not None:\n",
    "        logger.info(\"Backtest completed successfully\")\n",
    "    else:\n",
    "        logger.error(\"Backtest failed - check logs for details\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to run backtest: {str(e)}\")\n",
    "    portfolio, stats = None, {'Error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_dashboard(signals, portfolio):\n",
    "    \"\"\"Create comprehensive performance dashboard with robust error handling\"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if signals is None or portfolio is None:\n",
    "            logger.error(\"Cannot create dashboard - signals or portfolio is None\")\n",
    "            return None\n",
    "        \n",
    "        # Check required columns in signals\n",
    "        required_columns = ['synergy_strength', 'signal']\n",
    "        missing_columns = [col for col in required_columns if col not in signals.columns]\n",
    "        if missing_columns:\n",
    "            logger.warning(f\"Missing columns in signals for full dashboard: {missing_columns}\")\n",
    "        \n",
    "        # Create subplots with error handling\n",
    "        try:\n",
    "            fig = make_subplots(\n",
    "                rows=4, cols=2,\n",
    "                subplot_titles=(\n",
    "                    'Portfolio Value', 'Monthly Returns',\n",
    "                    'Cumulative Returns', 'Drawdown',\n",
    "                    'Trade Distribution', 'Signal Strength vs Returns',\n",
    "                    'Rolling Sharpe Ratio', 'Win Rate by Month'\n",
    "                ),\n",
    "                row_heights=[0.25, 0.25, 0.25, 0.25],\n",
    "                specs=[\n",
    "                    [{\"secondary_y\": False}, {\"type\": \"bar\"}],\n",
    "                    [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                    [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n",
    "                    [{\"secondary_y\": False}, {\"type\": \"bar\"}]\n",
    "                ]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating subplot structure: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        # 1. Portfolio Value with error handling\n",
    "        try:\n",
    "            portfolio_value = portfolio.value()\n",
    "            if portfolio_value is not None and len(portfolio_value) > 0:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=portfolio_value.index,\n",
    "                        y=portfolio_value.values,\n",
    "                        name='Portfolio Value',\n",
    "                        line=dict(color='cyan', width=2)\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            else:\n",
    "                logger.warning(\"Portfolio value data is empty\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting portfolio value: {str(e)}\")\n",
    "        \n",
    "        # 2. Monthly Returns with error handling\n",
    "        try:\n",
    "            returns = portfolio.returns()\n",
    "            if returns is not None and len(returns) > 0:\n",
    "                monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "                if len(monthly_returns) > 0:\n",
    "                    colors = ['green' if r > 0 else 'red' for r in monthly_returns]\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=monthly_returns.index,\n",
    "                            y=monthly_returns.values * 100,\n",
    "                            name='Monthly Returns',\n",
    "                            marker_color=colors\n",
    "                        ),\n",
    "                        row=1, col=2\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting monthly returns: {str(e)}\")\n",
    "        \n",
    "        # 3. Cumulative Returns with error handling\n",
    "        try:\n",
    "            if returns is not None and len(returns) > 0:\n",
    "                cum_returns = (1 + returns).cumprod() - 1\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=cum_returns.index,\n",
    "                        y=cum_returns.values * 100,\n",
    "                        name='Cumulative Returns',\n",
    "                        fill='tozeroy',\n",
    "                        line=dict(color='lightblue')\n",
    "                    ),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting cumulative returns: {str(e)}\")\n",
    "        \n",
    "        # 4. Drawdown with error handling\n",
    "        try:\n",
    "            drawdown = portfolio.drawdown()\n",
    "            if drawdown is not None and len(drawdown) > 0:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=drawdown.index,\n",
    "                        y=-drawdown.values * 100,\n",
    "                        name='Drawdown',\n",
    "                        fill='tozeroy',\n",
    "                        line=dict(color='red')\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting drawdown: {str(e)}\")\n",
    "        \n",
    "        # 5. Trade Distribution with error handling\n",
    "        try:\n",
    "            trades_records = portfolio.trades.records_readable\n",
    "            if trades_records is not None and len(trades_records) > 0:\n",
    "                trade_returns = trades_records['Return [%]'].values\n",
    "                if len(trade_returns) > 0:\n",
    "                    fig.add_trace(\n",
    "                        go.Histogram(\n",
    "                            x=trade_returns,\n",
    "                            nbinsx=50,\n",
    "                            name='Trade Returns',\n",
    "                            marker_color='purple'\n",
    "                        ),\n",
    "                        row=3, col=1\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting trade distribution: {str(e)}\")\n",
    "        \n",
    "        # 6. Signal Strength vs Returns with error handling\n",
    "        try:\n",
    "            if 'synergy_strength' in signals.columns and trades_records is not None and len(trades_records) > 0:\n",
    "                entry_times = pd.to_datetime(trades_records['Entry Timestamp'])\n",
    "                signal_strengths = []\n",
    "                \n",
    "                for entry_time in entry_times:\n",
    "                    try:\n",
    "                        idx = signals.index.get_indexer([entry_time], method='nearest')[0]\n",
    "                        if 0 <= idx < len(signals):\n",
    "                            signal_strengths.append(signals.iloc[idx]['synergy_strength'])\n",
    "                        else:\n",
    "                            signal_strengths.append(0)\n",
    "                    except:\n",
    "                        signal_strengths.append(0)\n",
    "                \n",
    "                if len(signal_strengths) > 0 and len(trade_returns) > 0:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=signal_strengths,\n",
    "                            y=trade_returns,\n",
    "                            mode='markers',\n",
    "                            name='Strength vs Return',\n",
    "                            marker=dict(\n",
    "                                size=5,\n",
    "                                color=trade_returns,\n",
    "                                colorscale='RdYlGn',\n",
    "                                showscale=True\n",
    "                            )\n",
    "                        ),\n",
    "                        row=3, col=2\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting signal strength vs returns: {str(e)}\")\n",
    "        \n",
    "        # 7. Rolling Sharpe Ratio with error handling\n",
    "        try:\n",
    "            rolling_sharpe = portfolio.sharpe_ratio(rolling=252)\n",
    "            if rolling_sharpe is not None and len(rolling_sharpe) > 0:\n",
    "                # Filter out extreme values\n",
    "                rolling_sharpe = rolling_sharpe.clip(-10, 10)\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=rolling_sharpe.index,\n",
    "                        y=rolling_sharpe.values,\n",
    "                        name='Rolling Sharpe',\n",
    "                        line=dict(color='orange')\n",
    "                    ),\n",
    "                    row=4, col=1\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting rolling Sharpe ratio: {str(e)}\")\n",
    "        \n",
    "        # 8. Win Rate by Month with error handling\n",
    "        try:\n",
    "            if trades_records is not None and len(trades_records) > 0:\n",
    "                trades_df = trades_records.copy()\n",
    "                trades_df['Month'] = pd.to_datetime(trades_df['Entry Timestamp']).dt.to_period('M')\n",
    "                \n",
    "                # Group by month and calculate win rate\n",
    "                monthly_groups = trades_df.groupby('Month')['Return [%]']\n",
    "                monthly_stats = pd.DataFrame({\n",
    "                    'Count': monthly_groups.count(),\n",
    "                    'Win Rate': monthly_groups.apply(lambda x: (x > 0).sum() / len(x) * 100 if len(x) > 0 else 0)\n",
    "                })\n",
    "                \n",
    "                if len(monthly_stats) > 0:\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=monthly_stats.index.astype(str),\n",
    "                            y=monthly_stats['Win Rate'],\n",
    "                            name='Win Rate %',\n",
    "                            marker_color='lightgreen'\n",
    "                        ),\n",
    "                        row=4, col=2\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting win rate by month: {str(e)}\")\n",
    "        \n",
    "        # Update layout with error handling\n",
    "        try:\n",
    "            fig.update_layout(\n",
    "                title_text=\"NW-RQK → MLMI → FVG Synergy Performance Dashboard\",\n",
    "                showlegend=False,\n",
    "                height=1600,\n",
    "                template='plotly_dark'\n",
    "            )\n",
    "            \n",
    "            # Update axes with error handling\n",
    "            axes_updates = [\n",
    "                (fig.update_xaxes, \"Date\", 1, 1),\n",
    "                (fig.update_xaxes, \"Date\", 1, 2),\n",
    "                (fig.update_xaxes, \"Date\", 2, 1),\n",
    "                (fig.update_xaxes, \"Date\", 2, 2),\n",
    "                (fig.update_xaxes, \"Return %\", 3, 1),\n",
    "                (fig.update_xaxes, \"Signal Strength\", 3, 2),\n",
    "                (fig.update_xaxes, \"Date\", 4, 1),\n",
    "                (fig.update_xaxes, \"Month\", 4, 2),\n",
    "                (fig.update_yaxes, \"Value ($)\", 1, 1),\n",
    "                (fig.update_yaxes, \"Return %\", 1, 2),\n",
    "                (fig.update_yaxes, \"Return %\", 2, 1),\n",
    "                (fig.update_yaxes, \"Drawdown %\", 2, 2),\n",
    "                (fig.update_yaxes, \"Frequency\", 3, 1),\n",
    "                (fig.update_yaxes, \"Return %\", 3, 2),\n",
    "                (fig.update_yaxes, \"Sharpe Ratio\", 4, 1),\n",
    "                (fig.update_yaxes, \"Win Rate %\", 4, 2)\n",
    "            ]\n",
    "            \n",
    "            for update_func, title, row, col in axes_updates:\n",
    "                try:\n",
    "                    update_func(title_text=title, row=row, col=col)\n",
    "                except:\n",
    "                    pass  # Skip if specific axis update fails\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating layout: {str(e)}\")\n",
    "        \n",
    "        # Show figure with error handling\n",
    "        try:\n",
    "            fig.show()\n",
    "            logger.info(\"Performance dashboard created successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error displaying figure: {str(e)}\")\n",
    "            # Try to save as HTML instead\n",
    "            try:\n",
    "                import os\n",
    "                html_file = os.path.join('/home/QuantNova/AlgoSpace-8/results', f'dashboard_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.html')\n",
    "                fig.write_html(html_file)\n",
    "                logger.info(f\"Dashboard saved as HTML: {html_file}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error creating performance dashboard: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "# Check if required variables exist\n",
    "if 'signals' not in globals() or 'portfolio' not in globals():\n",
    "    logger.error(\"Signals or portfolio not found. Please run the strategy and backtest cells first.\")\n",
    "    logger.info(\"Skipping dashboard creation - required data not available\")\n",
    "    dashboard = None\n",
    "else:\n",
    "    # Import required modules if not already imported\n",
    "    if 'go' not in globals():\n",
    "        from plotly import graph_objects as go\n",
    "    if 'make_subplots' not in globals():\n",
    "        from plotly.subplots import make_subplots\n",
    "    if 'traceback' not in globals():\n",
    "        import traceback\n",
    "    if 'datetime' not in globals():\n",
    "        from datetime import datetime\n",
    "    \n",
    "    # Create dashboard with error handling\n",
    "    try:\n",
    "        dashboard = create_performance_dashboard(signals, portfolio)\n",
    "        if dashboard is None:\n",
    "            logger.warning(\"Dashboard creation returned None - check logs for errors\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create dashboard: {str(e)}\")\n",
    "        dashboard = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monte Carlo Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True, fastmath=True)\n",
    "def monte_carlo_simulation(returns, n_simulations=1000, n_periods=252):\n",
    "    \"\"\"Run Monte Carlo simulation for confidence intervals with validation\"\"\"\n",
    "    n_returns = len(returns)\n",
    "    \n",
    "    # Validate inputs\n",
    "    if n_returns < 10:\n",
    "        raise ValueError(\"Insufficient returns for Monte Carlo simulation\")\n",
    "    \n",
    "    if n_simulations < 100:\n",
    "        n_simulations = 100\n",
    "    \n",
    "    if n_periods < 10:\n",
    "        n_periods = 252\n",
    "    \n",
    "    final_values = np.zeros(n_simulations)\n",
    "    \n",
    "    # Filter out extreme returns to avoid unrealistic simulations\n",
    "    valid_returns = returns[~np.isnan(returns)]\n",
    "    valid_returns = valid_returns[np.abs(valid_returns) < 0.5]  # Cap at 50% moves\n",
    "    \n",
    "    if len(valid_returns) < 10:\n",
    "        raise ValueError(\"Insufficient valid returns after filtering\")\n",
    "    \n",
    "    for sim in prange(n_simulations):\n",
    "        # Bootstrap sample returns\n",
    "        sim_returns = np.zeros(n_periods)\n",
    "        for i in range(n_periods):\n",
    "            idx = np.random.randint(0, len(valid_returns))\n",
    "            sim_returns[i] = valid_returns[idx]\n",
    "        \n",
    "        # Calculate final value with compound returns\n",
    "        cumulative_return = 1.0\n",
    "        for ret in sim_returns:\n",
    "            cumulative_return *= (1 + ret)\n",
    "        \n",
    "        final_values[sim] = cumulative_return\n",
    "    \n",
    "    return final_values\n",
    "\n",
    "def run_monte_carlo_analysis(portfolio):\n",
    "    \"\"\"Run Monte Carlo analysis for strategy validation with comprehensive checks\"\"\"\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"MONTE CARLO VALIDATION\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    mc_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Validate portfolio\n",
    "        if portfolio is None:\n",
    "            logger.error(\"No portfolio provided for Monte Carlo analysis\")\n",
    "            return None, None\n",
    "        \n",
    "        # Get trade returns with validation\n",
    "        try:\n",
    "            trades_df = portfolio.trades.records_readable\n",
    "            if len(trades_df) == 0:\n",
    "                logger.error(\"No trades found in portfolio\")\n",
    "                return None, None\n",
    "            \n",
    "            trade_returns = trades_df['Return [%]'].values / 100\n",
    "            \n",
    "            # Filter out invalid returns\n",
    "            valid_mask = ~np.isnan(trade_returns) & (np.abs(trade_returns) < 5.0)  # Cap at 500% moves\n",
    "            trade_returns = trade_returns[valid_mask]\n",
    "            \n",
    "            if len(trade_returns) < 10:\n",
    "                logger.error(f\"Insufficient valid trades for Monte Carlo: {len(trade_returns)}\")\n",
    "                return None, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting trade returns: {str(e)}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Run simulation with error handling\n",
    "        try:\n",
    "            final_values = monte_carlo_simulation(trade_returns, n_simulations=10000)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in Monte Carlo simulation: {str(e)}\")\n",
    "            # Try with fewer simulations\n",
    "            try:\n",
    "                logger.warning(\"Retrying with 1000 simulations...\")\n",
    "                final_values = monte_carlo_simulation(trade_returns, n_simulations=1000)\n",
    "            except:\n",
    "                return None, None\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mc_returns = (final_values - 1) * 100\n",
    "        percentiles = np.percentile(mc_returns, [5, 25, 50, 75, 95])\n",
    "        \n",
    "        logger.info(f\"\\nMonte Carlo simulation completed in {time.time() - mc_start:.2f} seconds\")\n",
    "        logger.info(f\"Based on {len(trade_returns)} historical trades\")\n",
    "        logger.info(\"\\nConfidence Intervals for Annual Returns:\")\n",
    "        logger.info(f\"5th percentile:  {percentiles[0]:.2f}%\")\n",
    "        logger.info(f\"25th percentile: {percentiles[1]:.2f}%\")\n",
    "        logger.info(f\"Median:          {percentiles[2]:.2f}%\")\n",
    "        logger.info(f\"75th percentile: {percentiles[3]:.2f}%\")\n",
    "        logger.info(f\"95th percentile: {percentiles[4]:.2f}%\")\n",
    "        \n",
    "        # Risk metrics\n",
    "        prob_profit = (mc_returns > 0).mean() * 100\n",
    "        prob_loss_10pct = (mc_returns < -10).mean() * 100\n",
    "        prob_gain_20pct = (mc_returns > 20).mean() * 100\n",
    "        expected_return = mc_returns.mean()\n",
    "        return_std = mc_returns.std()\n",
    "        \n",
    "        logger.info(f\"\\nRisk Metrics:\")\n",
    "        logger.info(f\"Probability of Profit: {prob_profit:.1f}%\")\n",
    "        logger.info(f\"Probability of >20% Gain: {prob_gain_20pct:.1f}%\")\n",
    "        logger.info(f\"Probability of >10% Loss: {prob_loss_10pct:.1f}%\")\n",
    "        logger.info(f\"Expected Return: {expected_return:.2f}%\")\n",
    "        logger.info(f\"Return Std Dev: {return_std:.2f}%\")\n",
    "        \n",
    "        # Create visualization with error handling\n",
    "        try:\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Add histogram\n",
    "            fig.add_trace(go.Histogram(\n",
    "                x=mc_returns,\n",
    "                nbinsx=100,\n",
    "                name='Simulated Returns',\n",
    "                marker_color='lightblue',\n",
    "                opacity=0.7,\n",
    "                showlegend=False\n",
    "            ))\n",
    "            \n",
    "            # Add percentile lines\n",
    "            colors = ['red', 'orange', 'green', 'orange', 'red']\n",
    "            for i, (p, label) in enumerate(zip(percentiles, ['5%', '25%', '50%', '75%', '95%'])):\n",
    "                fig.add_vline(\n",
    "                    x=p, \n",
    "                    line_dash=\"dash\", \n",
    "                    line_color=colors[i],\n",
    "                    annotation_text=f\"{label}: {p:.1f}%\",\n",
    "                    annotation_position=\"top\"\n",
    "                )\n",
    "            \n",
    "            # Add zero line\n",
    "            fig.add_vline(x=0, line_dash=\"solid\", line_color=\"black\", line_width=2)\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Monte Carlo Simulation Results ({len(final_values):,} simulations)\",\n",
    "                xaxis_title=\"Annual Return %\",\n",
    "                yaxis_title=\"Frequency\",\n",
    "                template='plotly_dark',\n",
    "                height=600,\n",
    "                showlegend=False\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating Monte Carlo visualization: {str(e)}\")\n",
    "        \n",
    "        return mc_returns, percentiles\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error in Monte Carlo analysis: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None, None\n",
    "\n",
    "# Check if required variables exist\n",
    "if 'portfolio' not in globals():\n",
    "    logger.error(\"Portfolio not found. Please run the backtest cell first.\")\n",
    "    logger.info(\"Skipping Monte Carlo analysis - portfolio required\")\n",
    "    mc_returns, percentiles = None, None\n",
    "else:\n",
    "    # Import required modules if not already imported\n",
    "    if 'go' not in globals():\n",
    "        import plotly.graph_objects as go\n",
    "    if 'traceback' not in globals():\n",
    "        import traceback\n",
    "    \n",
    "    # Run Monte Carlo validation with error handling\n",
    "    try:\n",
    "        mc_returns, percentiles = run_monte_carlo_analysis(portfolio)\n",
    "        if mc_returns is not None:\n",
    "            logger.info(\"Monte Carlo analysis completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to run Monte Carlo analysis: {str(e)}\")\n",
    "        mc_returns, percentiles = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics and Trade Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_report(signals, portfolio, stats):\n",
    "    \"\"\"Generate comprehensive performance report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Validate inputs\n",
    "    if signals is None or portfolio is None or stats is None:\n",
    "        print(\"Error: Missing required data for report generation\")\n",
    "        print(\"Please ensure strategy, backtest, and portfolio cells have been run successfully\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Time period analysis\n",
    "        start_date = signals.index[0]\n",
    "        end_date = signals.index[-1]\n",
    "        n_years = (end_date - start_date).days / 365.25\n",
    "        \n",
    "        print(f\"\\nBacktest Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Duration: {n_years:.1f} years\")\n",
    "        \n",
    "        # Trade analysis\n",
    "        trades = portfolio.trades.records_readable\n",
    "        total_trades = len(trades)\n",
    "        winning_trades = len(trades[trades['Return [%]'] > 0])\n",
    "        losing_trades = len(trades[trades['Return [%]'] < 0])\n",
    "        \n",
    "        print(f\"\\nTrade Statistics:\")\n",
    "        print(f\"Total Trades: {total_trades}\")\n",
    "        print(f\"Trades per Year: {total_trades / n_years:.0f}\")\n",
    "        print(f\"Winning Trades: {winning_trades}\")\n",
    "        print(f\"Losing Trades: {losing_trades}\")\n",
    "        print(f\"Win Rate: {(winning_trades / total_trades * 100) if total_trades > 0 else 0:.2f}%\")\n",
    "        \n",
    "        # Return analysis\n",
    "        avg_win = trades[trades['Return [%]'] > 0]['Return [%]'].mean() if winning_trades > 0 else 0\n",
    "        avg_loss = trades[trades['Return [%]'] < 0]['Return [%]'].mean() if losing_trades > 0 else 0\n",
    "        profit_factor = abs(avg_win * winning_trades / (avg_loss * losing_trades)) if losing_trades > 0 else np.inf\n",
    "        \n",
    "        print(f\"\\nReturn Metrics:\")\n",
    "        print(f\"Average Win: {avg_win:.2f}%\")\n",
    "        print(f\"Average Loss: {avg_loss:.2f}%\")\n",
    "        print(f\"Profit Factor: {profit_factor:.2f}\")\n",
    "        print(f\"Expectancy: {trades['Return [%]'].mean():.2f}%\")\n",
    "        \n",
    "        # Risk metrics\n",
    "        print(f\"\\nRisk Metrics:\")\n",
    "        print(f\"Maximum Drawdown: {stats['Max Drawdown [%]']:.2f}%\")\n",
    "        print(f\"Average Drawdown: {portfolio.drawdown().mean() * 100:.2f}%\")\n",
    "        print(f\"Calmar Ratio: {stats.get('Calmar Ratio', 'N/A'):.2f}\")\n",
    "        print(f\"Sortino Ratio: {stats.get('Sortino Ratio', 'N/A'):.2f}\")\n",
    "        \n",
    "        # Signal quality analysis\n",
    "        bull_signals = signals[signals['synergy_bull']]\n",
    "        bear_signals = signals[signals['synergy_bear']]\n",
    "        \n",
    "        print(f\"\\nSignal Analysis:\")\n",
    "        print(f\"Total Bull Signals: {len(bull_signals)}\")\n",
    "        print(f\"Total Bear Signals: {len(bear_signals)}\")\n",
    "        \n",
    "        if 'synergy_strength' in signals.columns:\n",
    "            avg_strength = signals['synergy_strength'][signals['synergy_strength'] > 0].mean()\n",
    "            if not np.isnan(avg_strength):\n",
    "                print(f\"Average Signal Strength: {avg_strength:.3f}\")\n",
    "        \n",
    "        # Monthly performance\n",
    "        monthly_returns = portfolio.returns().resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "        positive_months = (monthly_returns > 0).sum()\n",
    "        total_months = len(monthly_returns)\n",
    "        \n",
    "        print(f\"\\nMonthly Performance:\")\n",
    "        print(f\"Positive Months: {positive_months}/{total_months} ({positive_months/total_months*100:.1f}%)\")\n",
    "        print(f\"Best Month: {monthly_returns.max() * 100:.2f}%\")\n",
    "        print(f\"Worst Month: {monthly_returns.min() * 100:.2f}%\")\n",
    "        print(f\"Average Monthly Return: {monthly_returns.mean() * 100:.2f}%\")\n",
    "        \n",
    "        return trades\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError generating report: {str(e)}\")\n",
    "        if 'traceback' in globals():\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "# Check if required variables exist\n",
    "if 'signals' not in globals() or 'portfolio' not in globals() or 'stats' not in globals():\n",
    "    logger.error(\"Required variables not found for report generation\")\n",
    "    logger.info(\"Please run strategy and backtest cells before generating report\")\n",
    "    trades_df = None\n",
    "else:\n",
    "    # Generate report\n",
    "    trades_df = generate_comprehensive_report(signals, portfolio, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced atomic file operations for production safety\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def atomic_write(file_path, content, mode='w'):\n",
    "    \"\"\"Write file atomically to prevent corruption\"\"\"\n",
    "    # Create temporary file in same directory (for same filesystem)\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    \n",
    "    try:\n",
    "        # Write to temporary file\n",
    "        with tempfile.NamedTemporaryFile(mode=mode, dir=dir_path, delete=False) as temp_file:\n",
    "            temp_path = temp_file.name\n",
    "            if isinstance(content, str):\n",
    "                temp_file.write(content)\n",
    "            elif isinstance(content, pd.DataFrame):\n",
    "                content.to_csv(temp_file, index=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported content type: {type(content)}\")\n",
    "        \n",
    "        # Atomic rename (replaces existing file if present)\n",
    "        shutil.move(temp_path, file_path)\n",
    "        logger.info(f\"✓ Atomically wrote: {file_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Atomic write failed for {file_path}: {str(e)}\")\n",
    "        # Clean up temporary file if it exists\n",
    "        if 'temp_path' in locals() and os.path.exists(temp_path):\n",
    "            try:\n",
    "                os.remove(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "        return False\n",
    "\n",
    "# Save results with comprehensive error handling and atomic operations\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"SAVING RESULTS WITH ATOMIC OPERATIONS\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = '/home/QuantNova/AlgoSpace-8/results'\n",
    "try:\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    logger.info(f\"Results directory verified: {results_dir}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create results directory: {str(e)}\")\n",
    "    results_dir = '.'  # Fallback to current directory\n",
    "\n",
    "# Create backup directory for existing files\n",
    "backup_dir = os.path.join(results_dir, 'backups')\n",
    "try:\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "except:\n",
    "    backup_dir = None\n",
    "\n",
    "# Prepare timestamp for unique filenames\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save signals with atomic write\n",
    "try:\n",
    "    if 'signals' in globals() and signals is not None:\n",
    "        signals_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_signals_{timestamp}.csv')\n",
    "        \n",
    "        # Backup existing file if it exists\n",
    "        if os.path.exists(signals_file) and backup_dir:\n",
    "            backup_file = os.path.join(backup_dir, f'signals_backup_{timestamp}.csv')\n",
    "            shutil.copy2(signals_file, backup_file)\n",
    "            logger.info(f\"Backed up existing signals to {backup_file}\")\n",
    "        \n",
    "        # Atomic write\n",
    "        if atomic_write(signals_file, signals):\n",
    "            logger.info(f\"✓ Signals saved to {signals_file}\")\n",
    "        else:\n",
    "            logger.error(\"Failed to save signals atomically\")\n",
    "    else:\n",
    "        logger.warning(\"No signals to save\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save signals: {str(e)}\")\n",
    "\n",
    "# Save trade records with validation and atomic write\n",
    "if 'portfolio' in globals() and portfolio is not None:\n",
    "    try:\n",
    "        trades_df = portfolio.trades.records_readable\n",
    "        if len(trades_df) > 0:\n",
    "            trades_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_trades_{timestamp}.csv')\n",
    "            \n",
    "            # Add additional trade analysis columns\n",
    "            trades_df['Trade Duration'] = pd.to_datetime(trades_df['Exit Timestamp']) - pd.to_datetime(trades_df['Entry Timestamp'])\n",
    "            trades_df['Trade Duration Hours'] = trades_df['Trade Duration'].dt.total_seconds() / 3600\n",
    "            trades_df['Win/Loss'] = trades_df['Return [%]'].apply(lambda x: 'Win' if x > 0 else 'Loss')\n",
    "            \n",
    "            # Atomic write\n",
    "            if atomic_write(trades_file, trades_df):\n",
    "                logger.info(f\"✓ Trade records saved to {trades_file}\")\n",
    "                logger.info(f\"  Total trades: {len(trades_df)}\")\n",
    "                logger.info(f\"  Win rate: {(trades_df['Return [%]'] > 0).mean() * 100:.1f}%\")\n",
    "                logger.info(f\"  Average trade duration: {trades_df['Trade Duration Hours'].mean():.1f} hours\")\n",
    "            else:\n",
    "                logger.error(\"Failed to save trades atomically\")\n",
    "        else:\n",
    "            logger.warning(\"No trades to save\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save trade records: {str(e)}\")\n",
    "else:\n",
    "    logger.warning(\"No portfolio available - skipping trade records\")\n",
    "\n",
    "# Save performance metrics with comprehensive details\n",
    "try:\n",
    "    metrics_file = os.path.join(results_dir, f'synergy_3_nwrqk_mlmi_fvg_metrics_{timestamp}.txt')\n",
    "    \n",
    "    # Build comprehensive metrics report\n",
    "    metrics_content = []\n",
    "    metrics_content.append(\"=\"*60)\n",
    "    metrics_content.append(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY PERFORMANCE REPORT\")\n",
    "    metrics_content.append(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Strategy configuration\n",
    "    metrics_content.append(\"STRATEGY CONFIGURATION:\")\n",
    "    metrics_content.append(\"-\"*30)\n",
    "    metrics_content.append(f\"Run Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    if 'df_30m' in globals() and df_30m is not None and len(df_30m) > 0:\n",
    "        metrics_content.append(f\"Data Period: {df_30m.index[0]} to {df_30m.index[-1]}\")\n",
    "    metrics_content.append(f\"Initial Capital: ${StrategyConfig.INITIAL_CAPITAL:,.2f}\")\n",
    "    metrics_content.append(f\"Position Size: {StrategyConfig.POSITION_SIZE_BASE * 100:.1f}%\")\n",
    "    metrics_content.append(f\"Stop Loss: {StrategyConfig.STOP_LOSS_PCT * 100:.1f}%\")\n",
    "    metrics_content.append(f\"Take Profit: {StrategyConfig.TAKE_PROFIT_PCT * 100:.1f}%\")\n",
    "    metrics_content.append(f\"Trading Fees: {StrategyConfig.TRADING_FEES * 100:.2f}%\")\n",
    "    metrics_content.append(f\"Slippage: {StrategyConfig.SLIPPAGE * 100:.2f}%\\n\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    if 'stats' in globals() and stats is not None and 'Error' not in stats:\n",
    "        metrics_content.append(\"PERFORMANCE METRICS:\")\n",
    "        metrics_content.append(\"-\"*30)\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                if 'Return' in key or 'Ratio' in key or '%' in key:\n",
    "                    metrics_content.append(f\"{key}: {value:.2f}\")\n",
    "                else:\n",
    "                    metrics_content.append(f\"{key}: {value:.4f}\")\n",
    "            else:\n",
    "                metrics_content.append(f\"{key}: {value}\")\n",
    "    else:\n",
    "        metrics_content.append(\"Performance metrics unavailable due to errors\")\n",
    "    \n",
    "    # Signal statistics\n",
    "    if 'signals' in globals() and signals is not None:\n",
    "        metrics_content.append(\"\\nSIGNAL STATISTICS:\")\n",
    "        metrics_content.append(\"-\"*30)\n",
    "        metrics_content.append(f\"Total Signals Generated: {(signals['signal'] != 0).sum()}\")\n",
    "        metrics_content.append(f\"Bull Signals: {(signals['signal'] == 1).sum()}\")\n",
    "        metrics_content.append(f\"Bear Signals: {(signals['signal'] == -1).sum()}\")\n",
    "        \n",
    "        if 'synergy_strength' in signals.columns:\n",
    "            avg_strength = signals.loc[signals['signal'] != 0, 'synergy_strength'].mean()\n",
    "            if not np.isnan(avg_strength):\n",
    "                metrics_content.append(f\"Average Signal Strength: {avg_strength:.3f}\")\n",
    "    \n",
    "    # Risk analysis\n",
    "    if 'signals' in globals() and signals is not None and hasattr(signals, 'attrs') and 'performance_metrics' in signals.attrs:\n",
    "        perf = signals.attrs['performance_metrics']\n",
    "        metrics_content.append(\"\\nPERFORMANCE ANALYSIS:\")\n",
    "        metrics_content.append(\"-\"*30)\n",
    "        metrics_content.append(f\"Total Execution Time: {perf.get('total_time', 0):.2f} seconds\")\n",
    "        metrics_content.append(f\"Signals Filtered by Risk: {perf.get('signals_filtered', 0)}\")\n",
    "        \n",
    "        if perf.get('errors'):\n",
    "            metrics_content.append(\"\\nERRORS ENCOUNTERED:\")\n",
    "            for error in perf['errors']:\n",
    "                metrics_content.append(f\"- {error}\")\n",
    "    \n",
    "    # Monte Carlo results\n",
    "    if 'percentiles' in locals() and percentiles is not None:\n",
    "        metrics_content.append(\"\\nMONTE CARLO ANALYSIS:\")\n",
    "        metrics_content.append(\"-\"*30)\n",
    "        metrics_content.append(f\"5th Percentile Return: {percentiles[0]:.2f}%\")\n",
    "        metrics_content.append(f\"Median Return: {percentiles[2]:.2f}%\")\n",
    "        metrics_content.append(f\"95th Percentile Return: {percentiles[4]:.2f}%\")\n",
    "    \n",
    "    # System information\n",
    "    metrics_content.append(\"\\nSYSTEM INFORMATION:\")\n",
    "    metrics_content.append(\"-\"*30)\n",
    "    metrics_content.append(f\"Python Version: {sys.version.split()[0]}\")\n",
    "    metrics_content.append(f\"Pandas Version: {pd.__version__}\")\n",
    "    metrics_content.append(f\"NumPy Version: {np.__version__}\")\n",
    "    metrics_content.append(f\"Numba Version: {nb.__version__}\")\n",
    "    if 'vbt' in globals():\n",
    "        metrics_content.append(f\"VectorBT Version: {vbt.__version__}\")\n",
    "    \n",
    "    metrics_content.append(\"\\n\" + \"=\"*60)\n",
    "    metrics_content.append(\"END OF REPORT\")\n",
    "    metrics_content.append(\"=\"*60)\n",
    "    \n",
    "    # Join content and write atomically\n",
    "    full_content = '\\n'.join(metrics_content) + '\\n'\n",
    "    \n",
    "    if atomic_write(metrics_file, full_content):\n",
    "        logger.info(f\"✓ Performance metrics saved to {metrics_file}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to save metrics atomically\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save performance metrics: {str(e)}\")\n",
    "\n",
    "# Save configuration for reproducibility with atomic write\n",
    "try:\n",
    "    config_file = os.path.join(results_dir, f'synergy_3_config_{timestamp}.json')\n",
    "    config_dict = {attr: getattr(StrategyConfig, attr) \n",
    "                   for attr in dir(StrategyConfig) \n",
    "                   if not attr.startswith('_') and not callable(getattr(StrategyConfig, attr))}\n",
    "    \n",
    "    # Convert to JSON string\n",
    "    config_json = json.dumps(config_dict, indent=2, default=str)\n",
    "    \n",
    "    if atomic_write(config_file, config_json):\n",
    "        logger.info(f\"✓ Configuration saved to {config_file}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to save configuration atomically\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save configuration: {str(e)}\")\n",
    "\n",
    "# Create summary report with all file paths\n",
    "try:\n",
    "    summary_file = os.path.join(results_dir, f'synergy_3_summary_{timestamp}.json')\n",
    "    \n",
    "    summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_date': datetime.now().isoformat(),\n",
    "        'files_created': [],\n",
    "        'performance_summary': {},\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # Add created files\n",
    "    for filename in os.listdir(results_dir):\n",
    "        if timestamp in filename:\n",
    "            summary['files_created'].append(os.path.join(results_dir, filename))\n",
    "    \n",
    "    # Add performance summary\n",
    "    if 'stats' in globals() and stats is not None and 'Error' not in stats:\n",
    "        summary['performance_summary'] = {\n",
    "            'total_return': stats.get('Total Return [%]', 'N/A'),\n",
    "            'sharpe_ratio': stats.get('Sharpe Ratio', 'N/A'),\n",
    "            'max_drawdown': stats.get('Max Drawdown [%]', 'N/A'),\n",
    "            'win_rate': stats.get('Win Rate [%]', 'N/A'),\n",
    "            'total_trades': stats.get('Total Trades', 0)\n",
    "        }\n",
    "    \n",
    "    # Save summary atomically\n",
    "    summary_json = json.dumps(summary, indent=2, default=str)\n",
    "    if atomic_write(summary_file, summary_json):\n",
    "        logger.info(f\"✓ Summary report saved to {summary_file}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create summary report: {str(e)}\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"NW-RQK → MLMI → FVG SYNERGY STRATEGY COMPLETE\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Final summary\n",
    "logger.info(\"\\nFINAL SUMMARY:\")\n",
    "if 'portfolio' in globals() and portfolio is not None and 'stats' in globals() and stats is not None and 'Total Return [%]' in stats:\n",
    "    logger.info(f\"Total Return: {stats['Total Return [%]']:.2f}%\")\n",
    "    logger.info(f\"Sharpe Ratio: {stats.get('Sharpe Ratio', 'N/A'):.2f}\")\n",
    "    logger.info(f\"Max Drawdown: {stats.get('Max Drawdown [%]', 'N/A'):.2f}%\")\n",
    "    logger.info(f\"Total Trades: {stats.get('Total Trades', 0)}\")\n",
    "else:\n",
    "    logger.info(\"Strategy execution completed with errors - check logs for details\")\n",
    "\n",
    "logger.info(f\"\\nAll results saved to: {results_dir}\")\n",
    "logger.info(\"✓ All files written atomically for data integrity\")\n",
    "logger.info(\"✓ Backups created for existing files\")\n",
    "logger.info(\"Notebook execution completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}