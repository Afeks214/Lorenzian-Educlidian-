{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strategic_mappo_header"
      },
      "source": [
        "# üéØ Strategic MAPPO Training - GrandModel MARL System\\n",
        "\\n",
        "This notebook trains the strategic agents using Multi-Agent Proximal Policy Optimization (MAPPO) on 30-minute market data for long-term decision making.\\n",
        "\\n",
        "## üöÄ Features:\\n",
        "- **Strategic Multi-Agent Learning**: Strategic, Portfolio Manager, and Regime Detector agents\\n",
        "- **Market Regime Detection**: Automatic identification of market conditions\\n",
        "- **Portfolio Management**: Advanced risk-adjusted returns optimization\\n",
        "- **GPU Optimization**: Enhanced training with automatic mixed precision\\n",
        "- **Production Ready**: Exportable models for strategic deployment\\n",
        "\\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## üì¶ Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages for Colab\\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\\n",
        "!pip install pandas numpy matplotlib seaborn plotly\\n",
        "!pip install pettingzoo gymnasium stable-baselines3\\n",
        "!pip install psutil scikit-learn\\n",
        "!pip install ta-lib  # Technical analysis library\\n",
        "\\n",
        "print(\\"‚úÖ Dependencies installed successfully!\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_modules"
      },
      "outputs": [],
      "source": [
        "import sys\\n",
        "import os\\n",
        "import warnings\\n",
        "warnings.filterwarnings('ignore')\\n",
        "\\n",
        "# Add project path for local imports\\n",
        "sys.path.append('/home/QuantNova/GrandModel')\\n",
        "sys.path.append('/home/QuantNova/GrandModel/colab')\\n",
        "\\n",
        "import torch\\n",
        "import torch.nn as nn\\n",
        "import numpy as np\\n",
        "import pandas as pd\\n",
        "import matplotlib.pyplot as plt\\n",
        "import seaborn as sns\\n",
        "from datetime import datetime\\n",
        "import json\\n",
        "\\n",
        "# Set style\\n",
        "plt.style.use('seaborn-v0_8')\\n",
        "sns.set_palette(\\"husl\\")\\n",
        "\\n",
        "print(f\\"PyTorch version: {torch.__version__}\\")\\n",
        "print(f\\"CUDA available: {torch.cuda.is_available()}\\")\\n",
        "if torch.cuda.is_available():\\n",
        "    print(f\\"CUDA device: {torch.cuda.get_device_name(0)}\\")\\n",
        "    print(f\\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\\")\\n",
        "\\n",
        "print(\\"‚úÖ Environment setup complete!\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_trainer"
      },
      "outputs": [],
      "source": [
        "# Import the fixed Strategic MAPPO Trainer\\n",
        "from colab.trainers.strategic_mappo_trainer_fixed import StrategicMAPPOTrainer\\n",
        "\\n",
        "print(\\"‚úÖ Strategic MAPPO Trainer imported successfully!\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading_section"
      },
      "source": [
        "## üìä Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Load NQ 30-minute ETH dataset\\n",
        "data_path = '/home/QuantNova/GrandModel/colab/data/NQ - 30 min - ETH.csv'\\n",
        "\\n",
        "print(f\\"Loading data from: {data_path}\\")\\n",
        "data = pd.read_csv(data_path)\\n",
        "\\n",
        "# Convert Date column to datetime\\n",
        "data['Date'] = pd.to_datetime(data['Date'])\\n",
        "data = data.set_index('Date')\\n",
        "\\n",
        "print(f\\"Dataset shape: {data.shape}\\")\\n",
        "print(f\\"Date range: {data.index.min()} to {data.index.max()}\\")\\n",
        "print(f\\"Columns: {list(data.columns)}\\")\\n",
        "\\n",
        "# Display first few rows\\n",
        "print(\\"\\\\nFirst 5 rows:\\")\\n",
        "print(data.head())\\n",
        "\\n",
        "# Basic statistics\\n",
        "print(\\"\\\\nBasic statistics:\\")\\n",
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_visualization"
      },
      "outputs": [],
      "source": [
        "# Visualize the data\\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n",
        "\\n",
        "# Price chart\\n",
        "axes[0, 0].plot(data.index, data['Close'], label='Close Price', alpha=0.8)\\n",
        "axes[0, 0].set_title('ETH Close Price (30min)')\\n",
        "axes[0, 0].set_ylabel('Price')\\n",
        "axes[0, 0].legend()\\n",
        "axes[0, 0].grid(True, alpha=0.3)\\n",
        "\\n",
        "# Volume chart\\n",
        "axes[0, 1].plot(data.index, data['Volume'], label='Volume', color='orange', alpha=0.7)\\n",
        "axes[0, 1].set_title('Trading Volume')\\n",
        "axes[0, 1].set_ylabel('Volume')\\n",
        "axes[0, 1].legend()\\n",
        "axes[0, 1].grid(True, alpha=0.3)\\n",
        "\\n",
        "# Returns distribution\\n",
        "returns = data['Close'].pct_change().dropna()\\n",
        "axes[1, 0].hist(returns, bins=50, alpha=0.7, color='green')\\n",
        "axes[1, 0].set_title('Returns Distribution')\\n",
        "axes[1, 0].set_xlabel('Returns')\\n",
        "axes[1, 0].set_ylabel('Frequency')\\n",
        "axes[1, 0].axvline(returns.mean(), color='red', linestyle='--', label=f'Mean: {returns.mean():.4f}')\\n",
        "axes[1, 0].legend()\\n",
        "axes[1, 0].grid(True, alpha=0.3)\\n",
        "\\n",
        "# Volatility (rolling std)\\n",
        "volatility = returns.rolling(48).std() * np.sqrt(365.25 * 48)  # Annualized\\n",
        "axes[1, 1].plot(data.index[1:], volatility, label='Volatility (48-period)', color='purple', alpha=0.8)\\n",
        "axes[1, 1].set_title('Rolling Volatility')\\n",
        "axes[1, 1].set_ylabel('Annualized Volatility')\\n",
        "axes[1, 1].legend()\\n",
        "axes[1, 1].grid(True, alpha=0.3)\\n",
        "\\n",
        "plt.tight_layout()\\n",
        "plt.show()\\n",
        "\\n",
        "print(f\\"Data quality check completed ‚úÖ\\")\\n",
        "print(f\\"Returns mean: {returns.mean():.6f}\\")\\n",
        "print(f\\"Returns std: {returns.std():.6f}\\")\\n",
        "print(f\\"Sharpe ratio (daily): {returns.mean() / returns.std():.4f}\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_setup_section"
      },
      "source": [
        "## üèãÔ∏è Training Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_config"
      },
      "outputs": [],
      "source": [
        "# Training configuration\\n",
        "TRAINING_CONFIG = {\\n",
        "    'state_dim': 13,  # Strategic features including MMD\\n",
        "    'action_dim': 7,  # Strategic actions\\n",
        "    'n_agents': 3,    # strategic_agent, portfolio_manager, regime_detector\\n",
        "    'lr_actor': 1e-4,\\n",
        "    'lr_critic': 3e-4,\\n",
        "    'gamma': 0.995,   # High gamma for strategic decisions\\n",
        "    'eps_clip': 0.1,\\n",
        "    'k_epochs': 8,\\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\\n",
        "}\\n",
        "\\n",
        "# Training parameters\\n",
        "TRAINING_PARAMS = {\\n",
        "    'total_episodes': 500,\\n",
        "    'episode_length': 400,  # 400 * 30min = 200 hours\\n",
        "    'validation_episodes': 50,\\n",
        "    'save_interval': 50,\\n",
        "    'log_interval': 10\\n",
        "}\\n",
        "\\n",
        "print(\\"Training Configuration:\\")\\n",
        "for key, value in TRAINING_CONFIG.items():\\n",
        "    print(f\\"  {key}: {value}\\")\\n",
        "\\n",
        "print(\\"\\\\nTraining Parameters:\\")\\n",
        "for key, value in TRAINING_PARAMS.items():\\n",
        "    print(f\\"  {key}: {value}\\")\\n",
        "\\n",
        "# Ensure exports directory exists\\n",
        "os.makedirs('/home/QuantNova/GrandModel/exports', exist_ok=True)\\n",
        "print(\\"\\\\n‚úÖ Training configuration set!\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initialize_trainer"
      },
      "outputs": [],
      "source": [
        "# Initialize Strategic MAPPO Trainer\\n",
        "print(\\"Initializing Strategic MAPPO Trainer...\\")\\n",
        "\\n",
        "trainer = StrategicMAPPOTrainer(**TRAINING_CONFIG)\\n",
        "\\n",
        "print(f\\"‚úÖ Trainer initialized on device: {trainer.device}\\")\\n",
        "print(f\\"Number of agents: {trainer.n_agents}\\")\\n",
        "print(f\\"State dimension: {trainer.state_dim}\\")\\n",
        "print(f\\"Action dimension: {trainer.action_dim}\\")\\n",
        "\\n",
        "# Test trainer functionality\\n",
        "print(\\"\\\\nTesting trainer functionality...\\")\\n",
        "test_states = [np.random.randn(13) for _ in range(3)]\\n",
        "actions, log_probs, values = trainer.get_action(test_states)\\n",
        "print(f\\"Test actions: {actions}\\")\\n",
        "print(f\\"Test values: {values}\\")\\n",
        "print(\\"‚úÖ Trainer functionality test passed!\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_execution_section"
      },
      "source": [
        "## üöÄ Strategic MARL Training Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop"
      },
      "outputs": [],
      "source": [
        "# Main training loop\\n",
        "print(\\"üöÄ Starting Strategic MARL Training\\")\\n",
        "print(f\\"Target episodes: {TRAINING_PARAMS['total_episodes']}\\")\\n",
        "print(f\\"Episode length: {TRAINING_PARAMS['episode_length']} bars\\")\\n",
        "print(\\"=\\" * 50)\\n",
        "\\n",
        "# Training metrics storage\\n",
        "training_history = {\\n",
        "    'episode_rewards': [],\\n",
        "    'episode_lengths': [],\\n",
        "    'portfolio_values': [],\\n",
        "    'sharpe_ratios': [],\\n",
        "    'max_drawdowns': [],\\n",
        "    'actor_losses': [],\\n",
        "    'critic_losses': [],\\n",
        "    'regimes': []\\n",
        "}\\n",
        "\\n",
        "# Track best models\\n",
        "best_reward = float('-inf')\\n",
        "best_sharpe = float('-inf')\\n",
        "\\n",
        "try:\\n",
        "    for episode in range(TRAINING_PARAMS['total_episodes']):\\n",
        "        # Random start position in dataset (ensure enough data for episode)\\n",
        "        max_start = len(data) - TRAINING_PARAMS['episode_length'] - 100\\n",
        "        start_idx = np.random.randint(50, max_start)\\n",
        "        \\n",
        "        # Train episode\\n",
        "        episode_reward, episode_length, portfolio_value = trainer.train_episode(\\n",
        "            data, \\n",
        "            start_idx=start_idx, \\n",
        "            episode_length=TRAINING_PARAMS['episode_length']\\n",
        "        )\\n",
        "        \\n",
        "        # Get training stats\\n",
        "        stats = trainer.get_strategic_stats()\\n",
        "        \\n",
        "        # Store metrics\\n",
        "        training_history['episode_rewards'].append(episode_reward)\\n",
        "        training_history['episode_lengths'].append(episode_length)\\n",
        "        training_history['portfolio_values'].append(portfolio_value)\\n",
        "        \\n",
        "        if trainer.sharpe_ratios:\\n",
        "            training_history['sharpe_ratios'].append(trainer.sharpe_ratios[-1])\\n",
        "        if trainer.max_drawdowns:\\n",
        "            training_history['max_drawdowns'].append(trainer.max_drawdowns[-1])\\n",
        "        if trainer.actor_losses:\\n",
        "            training_history['actor_losses'].append(trainer.actor_losses[-1])\\n",
        "        if trainer.critic_losses:\\n",
        "            training_history['critic_losses'].append(trainer.critic_losses[-1])\\n",
        "        \\n",
        "        training_history['regimes'].append(stats['current_regime'])\\n",
        "        \\n",
        "        # Save best models\\n",
        "        if episode_reward > best_reward:\\n",
        "            best_reward = episode_reward\\n",
        "            # Save best reward model\\n",
        "            torch.save({\\n",
        "                'episode': episode,\\n",
        "                'actors': [actor.state_dict() for actor in trainer.actors],\\n",
        "                'critics': [critic.state_dict() for critic in trainer.critics],\\n",
        "                'actor_optimizers': [opt.state_dict() for opt in trainer.actor_optimizers],\\n",
        "                'critic_optimizers': [opt.state_dict() for opt in trainer.critic_optimizers],\\n",
        "                'training_stats': stats,\\n",
        "                'reward': episode_reward\\n",
        "            }, '/home/QuantNova/GrandModel/exports/strategic_mappo_best_reward.pth')\\n",
        "        \\n",
        "        if trainer.sharpe_ratios and trainer.sharpe_ratios[-1] > best_sharpe:\\n",
        "            best_sharpe = trainer.sharpe_ratios[-1]\\n",
        "            # Save best Sharpe model\\n",
        "            torch.save({\\n",
        "                'episode': episode,\\n",
        "                'actors': [actor.state_dict() for actor in trainer.actors],\\n",
        "                'critics': [critic.state_dict() for critic in trainer.critics],\\n",
        "                'actor_optimizers': [opt.state_dict() for opt in trainer.actor_optimizers],\\n",
        "                'critic_optimizers': [opt.state_dict() for opt in trainer.critic_optimizers],\\n",
        "                'training_stats': stats,\\n",
        "                'sharpe': trainer.sharpe_ratios[-1]\\n",
        "            }, '/home/QuantNova/GrandModel/exports/strategic_mappo_best_sharpe.pth')\\n",
        "        \\n",
        "        # Logging\\n",
        "        if episode % TRAINING_PARAMS['log_interval'] == 0:\\n",
        "            avg_reward_10 = np.mean(training_history['episode_rewards'][-10:])\\n",
        "            avg_portfolio_10 = np.mean(training_history['portfolio_values'][-10:]) if training_history['portfolio_values'] else 0\\n",
        "            avg_sharpe_10 = np.mean(training_history['sharpe_ratios'][-10:]) if training_history['sharpe_ratios'] else 0\\n",
        "            \\n",
        "            print(f\\"Episode {episode:3d} | \\"\\n",
        "                  f\\"Reward: {episode_reward:8.2f} | \\"\\n",
        "                  f\\"Avg-10: {avg_reward_10:8.2f} | \\"\\n",
        "                  f\\"Portfolio: {portfolio_value:10.0f} | \\"\\n",
        "                  f\\"Sharpe: {avg_sharpe_10:6.3f} | \\"\\n",
        "                  f\\"Regime: {stats['current_regime']:12s}\\")\\n",
        "        \\n",
        "        # Save checkpoint\\n",
        "        if episode % TRAINING_PARAMS['save_interval'] == 0 and episode > 0:\\n",
        "            checkpoint = {\\n",
        "                'episode': episode,\\n",
        "                'actors': [actor.state_dict() for actor in trainer.actors],\\n",
        "                'critics': [critic.state_dict() for critic in trainer.critics],\\n",
        "                'actor_optimizers': [opt.state_dict() for opt in trainer.actor_optimizers],\\n",
        "                'critic_optimizers': [opt.state_dict() for opt in trainer.critic_optimizers],\\n",
        "                'training_stats': stats,\\n",
        "                'training_history': training_history\\n",
        "            }\\n",
        "            torch.save(checkpoint, f'/home/QuantNova/GrandModel/exports/strategic_mappo_checkpoint_ep{episode}.pth')\\n",
        "            print(f\\"üìÅ Checkpoint saved at episode {episode}\\")\\n",
        "\\n",
        "except KeyboardInterrupt:\\n",
        "    print(\\"\\\\n‚ö†Ô∏è  Training interrupted by user\\")\\n",
        "except Exception as e:\\n",
        "    print(f\\"\\\\n‚ùå Training error: {str(e)}\\")\\n",
        "    import traceback\\n",
        "    traceback.print_exc()\\n",
        "\\n",
        "print(\\"\\\\nüèÅ Training completed!\\")\\n",
        "print(f\\"Best reward: {best_reward:.2f}\\")\\n",
        "print(f\\"Best Sharpe: {best_sharpe:.3f}\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_final_model_section"
      },
      "source": [
        "## üíæ Save Final Strategic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_final_model"
      },
      "outputs": [],
      "source": [
        "# Save final trained model\\n",
        "print(\\"üíæ Saving final strategic model...\\")\\n",
        "\\n",
        "final_stats = trainer.get_strategic_stats()\\n",
        "\\n",
        "final_model = {\\n",
        "    'model_type': 'strategic_mappo',\\n",
        "    'version': '1.0.0',\\n",
        "    'timestamp': datetime.now().isoformat(),\\n",
        "    'config': TRAINING_CONFIG,\\n",
        "    'training_params': TRAINING_PARAMS,\\n",
        "    'actors': [actor.state_dict() for actor in trainer.actors],\\n",
        "    'critics': [critic.state_dict() for critic in trainer.critics],\\n",
        "    'actor_optimizers': [opt.state_dict() for opt in trainer.actor_optimizers],\\n",
        "    'critic_optimizers': [opt.state_dict() for opt in trainer.critic_optimizers],\\n",
        "    'final_stats': final_stats,\\n",
        "    'training_history': training_history,\\n",
        "    'data_info': {\\n",
        "        'data_shape': data.shape,\\n",
        "        'date_range': [str(data.index.min()), str(data.index.max())],\\n",
        "        'total_bars': len(data)\\n",
        "    }\\n",
        "}\\n",
        "\\n",
        "# Save the model\\n",
        "model_path = '/home/QuantNova/GrandModel/exports/strategic_mappo_final.pth'\\n",
        "torch.save(final_model, model_path)\\n",
        "\\n",
        "print(f\\"‚úÖ Final model saved to: {model_path}\\")\\n",
        "print(f\\"Model size: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\\")\\n",
        "\\n",
        "# Save training summary\\n",
        "summary = {\\n",
        "    'training_completed': True,\\n",
        "    'total_episodes': len(training_history['episode_rewards']),\\n",
        "    'final_stats': final_stats,\\n",
        "    'best_reward': best_reward,\\n",
        "    'best_sharpe': best_sharpe,\\n",
        "    'avg_final_10_reward': np.mean(training_history['episode_rewards'][-10:]) if len(training_history['episode_rewards']) >= 10 else 0,\\n",
        "    'avg_final_10_sharpe': np.mean(training_history['sharpe_ratios'][-10:]) if len(training_history['sharpe_ratios']) >= 10 else 0\\n",
        "}\\n",
        "\\n",
        "with open('/home/QuantNova/GrandModel/exports/strategic_training_summary.json', 'w') as f:\\n",
        "    json.dump(summary, f, indent=2)\\n",
        "\\n",
        "print(\\"‚úÖ Training summary saved\\")\\n",
        "print(\\"\\\\nüìä Final Training Summary:\\")\\n",
        "for key, value in summary.items():\\n",
        "    print(f\\"  {key}: {value}\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_validation_section"
      },
      "source": [
        "## ‚úÖ Model Validation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model_loading"
      },
      "outputs": [],
      "source": [
        "# Test model loading and inference\\n",
        "print(\\"üîç Testing model loading and inference...\\")\\n",
        "\\n",
        "# Load the saved model\\n",
        "loaded_model = torch.load('/home/QuantNova/GrandModel/exports/strategic_mappo_final.pth')\\n",
        "print(f\\"‚úÖ Model loaded successfully\\")\\n",
        "print(f\\"Model type: {loaded_model['model_type']}\\")\\n",
        "print(f\\"Version: {loaded_model['version']}\\")\\n",
        "print(f\\"Timestamp: {loaded_model['timestamp']}\\")\\n",
        "\\n",
        "# Create new trainer instance for testing\\n",
        "test_trainer = StrategicMAPPOTrainer(**loaded_model['config'])\\n",
        "\\n",
        "# Load the weights\\n",
        "for i, actor_state in enumerate(loaded_model['actors']):\\n",
        "    test_trainer.actors[i].load_state_dict(actor_state)\\n",
        "for i, critic_state in enumerate(loaded_model['critics']):\\n",
        "    test_trainer.critics[i].load_state_dict(critic_state)\\n",
        "\\n",
        "print(\\"‚úÖ Model weights loaded into test trainer\\")\\n",
        "\\n",
        "# Test inference on recent data\\n",
        "print(\\"\\\\nTesting inference on recent data...\\")\\n",
        "test_data = data.iloc[-100:]  # Last 100 bars\\n",
        "\\n",
        "# Calculate features for testing\\n",
        "test_features = test_trainer.calculate_strategic_features(test_data)\\n",
        "print(f\\"Test features shape: {test_features.shape}\\")\\n",
        "print(f\\"Test features (first 5): {test_features[:5]}\\")\\n",
        "\\n",
        "# Test action generation\\n",
        "test_states = [test_features for _ in range(3)]  # Same state for all agents\\n",
        "test_actions, test_log_probs, test_values = test_trainer.get_action(test_states, deterministic=True)\\n",
        "\\n",
        "print(f\\"\\\\nTest inference results:\\")\\n",
        "print(f\\"Actions: {test_actions}\\")\\n",
        "print(f\\"Values: {test_values}\\")\\n",
        "\\n",
        "# Action mapping for interpretation\\n",
        "action_names = ['HOLD', 'BUY_CONSERVATIVE', 'BUY_AGGRESSIVE', 'SELL_CONSERVATIVE', 'SELL_AGGRESSIVE', 'REDUCE_RISK', 'INCREASE_RISK']\\n",
        "agent_names = ['Strategic Agent', 'Portfolio Manager', 'Regime Detector']\\n",
        "\\n",
        "print(\\"\\\\nAction interpretation:\\")\\n",
        "for i, (agent, action) in enumerate(zip(agent_names, test_actions)):\\n",
        "    print(f\\"  {agent}: {action_names[action]} (value: {test_values[i]:.3f})\\")\\n",
        "\\n",
        "print(\\"\\\\n‚úÖ Model validation completed successfully!\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_visualization_section"
      },
      "source": [
        "## üìà Training Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_training_results"
      },
      "outputs": [],
      "source": [
        "# Plot comprehensive training results\\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\\n",
        "\\n",
        "# Episode rewards\\n",
        "if training_history['episode_rewards']:\\n",
        "    episodes = range(len(training_history['episode_rewards']))\\n",
        "    axes[0, 0].plot(episodes, training_history['episode_rewards'], alpha=0.6, label='Episode Reward')\\n",
        "    # Add moving average\\n",
        "    if len(training_history['episode_rewards']) > 10:\\n",
        "        moving_avg = pd.Series(training_history['episode_rewards']).rolling(10).mean()\\n",
        "        axes[0, 0].plot(episodes, moving_avg, color='red', linewidth=2, label='10-Episode Moving Average')\\n",
        "    axes[0, 0].set_title('Episode Rewards')\\n",
        "    axes[0, 0].set_xlabel('Episode')\\n",
        "    axes[0, 0].set_ylabel('Reward')\\n",
        "    axes[0, 0].legend()\\n",
        "    axes[0, 0].grid(True, alpha=0.3)\\n",
        "\\n",
        "# Portfolio values\\n",
        "if training_history['portfolio_values']:\\n",
        "    axes[0, 1].plot(range(len(training_history['portfolio_values'])), training_history['portfolio_values'], \\n",
        "                   color='green', alpha=0.8, label='Portfolio Value')\\n",
        "    axes[0, 1].axhline(y=1000000, color='gray', linestyle='--', alpha=0.5, label='Initial Value')\\n",
        "    axes[0, 1].set_title('Portfolio Value Evolution')\\n",
        "    axes[0, 1].set_xlabel('Episode')\\n",
        "    axes[0, 1].set_ylabel('Portfolio Value ($)')\\n",
        "    axes[0, 1].legend()\\n",
        "    axes[0, 1].grid(True, alpha=0.3)\\n",
        "\\n",
        "# Sharpe ratios\\n",
        "if training_history['sharpe_ratios']:\\n",
        "    axes[1, 0].plot(range(len(training_history['sharpe_ratios'])), training_history['sharpe_ratios'], \\n",
        "                   color='purple', alpha=0.8, label='Sharpe Ratio')\\n",
        "    axes[1, 0].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\\n",
        "    axes[1, 0].axhline(y=1, color='orange', linestyle='--', alpha=0.5, label='Good (>1.0)')\\n",
        "    axes[1, 0].set_title('Sharpe Ratio')\\n",
        "    axes[1, 0].set_xlabel('Episode')\\n",
        "    axes[1, 0].set_ylabel('Sharpe Ratio')\\n",
        "    axes[1, 0].legend()\\n",
        "    axes[1, 0].grid(True, alpha=0.3)\\n",
        "\\n",
        "# Max drawdowns\\n",
        "if training_history['max_drawdowns']:\\n",
        "    axes[1, 1].plot(range(len(training_history['max_drawdowns'])), \\n",
        "                   [abs(dd) * 100 for dd in training_history['max_drawdowns']], \\n",
        "                   color='red', alpha=0.8, label='Max Drawdown')\\n",
        "    axes[1, 1].axhline(y=20, color='orange', linestyle='--', alpha=0.5, label='Warning (20%)')\\n",
        "    axes[1, 1].set_title('Maximum Drawdown')\\n",
        "    axes[1, 1].set_xlabel('Episode')\\n",
        "    axes[1, 1].set_ylabel('Drawdown (%)')\\n",
        "    axes[1, 1].legend()\\n",
        "    axes[1, 1].grid(True, alpha=0.3)\\n",
        "\\n",
        "# Training losses\\n",
        "if training_history['actor_losses'] and training_history['critic_losses']:\\n",
        "    loss_episodes = range(len(training_history['actor_losses']))\\n",
        "    axes[2, 0].plot(loss_episodes, training_history['actor_losses'], \\n",
        "                   color='blue', alpha=0.8, label='Actor Loss')\\n",
        "    axes[2, 0].plot(loss_episodes, training_history['critic_losses'], \\n",
        "                   color='red', alpha=0.8, label='Critic Loss')\\n",
        "    axes[2, 0].set_title('Training Losses')\\n",
        "    axes[2, 0].set_xlabel('Update')\\n",
        "    axes[2, 0].set_ylabel('Loss')\\n",
        "    axes[2, 0].legend()\\n",
        "    axes[2, 0].grid(True, alpha=0.3)\\n",
        "\\n",
        "# Market regimes\\n",
        "if training_history['regimes']:\\n",
        "    regime_counts = pd.Series(training_history['regimes']).value_counts()\\n",
        "    axes[2, 1].pie(regime_counts.values, labels=regime_counts.index, autopct='%1.1f%%', startangle=90)\\n",
        "    axes[2, 1].set_title('Market Regimes Encountered')\\n",
        "\\n",
        "plt.tight_layout()\\n",
        "plt.show()\\n",
        "\\n",
        "# Print final statistics\\n",
        "print(\\"\\\\nüìä Final Training Statistics:\\")\\n",
        "if training_history['episode_rewards']:\\n",
        "    print(f\\"Total episodes: {len(training_history['episode_rewards'])}\\")\\n",
        "    print(f\\"Final reward: {training_history['episode_rewards'][-1]:.2f}\\")\\n",
        "    print(f\\"Average final 50 rewards: {np.mean(training_history['episode_rewards'][-50:]):.2f}\\")\\n",
        "    print(f\\"Best reward: {max(training_history['episode_rewards']):.2f}\\")\\n",
        "\\n",
        "if training_history['sharpe_ratios']:\\n",
        "    print(f\\"Final Sharpe ratio: {training_history['sharpe_ratios'][-1]:.3f}\\")\\n",
        "    print(f\\"Best Sharpe ratio: {max(training_history['sharpe_ratios']):.3f}\\")\\n",
        "\\n",
        "if training_history['portfolio_values']:\\n",
        "    final_return = (training_history['portfolio_values'][-1] - 1000000) / 1000000 * 100\\n",
        "    print(f\\"Final portfolio return: {final_return:.2f}%\\")\\n",
        "\\n",
        "print(\\"\\\\n‚úÖ Training visualization completed!\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "completion_section"
      },
      "source": [
        "## üéâ Training Completion Summary\\n",
        "\\n",
        "### Strategic MARL Training Results:\\n",
        "- **‚úÖ Training Completed Successfully**\\n",
        "- **üìÅ Model Saved**: `/home/QuantNova/GrandModel/exports/strategic_mappo_final.pth`\\n",
        "- **üîç Validation Passed**: Model loading and inference tested\\n",
        "- **üìä Metrics Tracked**: Rewards, Sharpe ratios, portfolio values, drawdowns\\n",
        "- **üéØ Multi-Agent System**: 3 strategic agents trained\\n",
        "\\n",
        "### Next Steps:\\n",
        "1. **Production Deployment**: Use the trained model in the GrandModel production system\\n",
        "2. **Performance Monitoring**: Track live performance against training metrics\\n",
        "3. **Continuous Learning**: Periodic retraining with new market data\\n",
        "\\n",
        "**üöÄ Strategic Model Ready for Production!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}