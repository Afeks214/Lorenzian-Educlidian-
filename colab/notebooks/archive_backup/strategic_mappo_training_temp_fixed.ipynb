{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategic_mappo_header"
   },
   "source": [
    "# üéØ Strategic MAPPO Training - GrandModel MARL System\n",
    "\n",
    "This notebook trains the strategic agents using Multi-Agent Proximal Policy Optimization (MAPPO) on 30-minute market data for long-term decision making.\n",
    "\n",
    "## üöÄ Features:\n",
    "- **Strategic Multi-Agent Learning**: Strategic, Portfolio Manager, and Regime Detector agents\n",
    "- **Market Regime Detection**: Automatic identification of market conditions\n",
    "- **Portfolio Management**: Advanced risk-adjusted returns optimization\n",
    "- **GPU Optimization**: Enhanced training with automatic mixed precision\n",
    "- **Production Ready**: Exportable models for strategic deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## üì¶ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pandas numpy matplotlib seaborn plotly\n",
    "!pip install pettingzoo gymnasium stable-baselines3\n",
    "!pip install psutil scikit-learn\n",
    "!pip install ta-lib  # Technical analysis library\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional - for saving models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_environment"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to path\n",
    "project_path = '/content/GrandModel'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "# Set up environment for strategic training\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'  # Enhanced CUDA debugging\n",
    "\n",
    "print(\"‚úÖ Strategic training environment configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_section"
   },
   "source": [
    "## üìÅ Clone Project (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the GrandModel repository\n",
    "!git clone https://github.com/Afeks214/GrandModel.git /content/GrandModel\n",
    "!cd /content/GrandModel && git checkout main\n",
    "\n",
    "print(\"‚úÖ Repository cloned and checked out!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports_section"
   },
   "source": [
    "## üìö Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# Set enhanced style for strategic analysis\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Compute Capability: {torch.cuda.get_device_capability()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_project_modules"
   },
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "try:\n",
    "    from colab.trainers.strategic_mappo_trainer import StrategicMAPPOTrainer\n",
    "    from colab.utils.gpu_optimizer import GPUOptimizer, setup_colab_environment, quick_gpu_check, quick_memory_check\n",
    "    print(\"‚úÖ Strategic training modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please ensure the GrandModel project is properly cloned and accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_setup_section"
   },
   "source": [
    "## üñ•Ô∏è Enhanced GPU Setup and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_gpu_optimizer"
   },
   "outputs": [],
   "source": [
    "# Setup enhanced GPU optimizer for strategic training\n",
    "gpu_optimizer = setup_colab_environment()\n",
    "\n",
    "# Enhanced checks for strategic training\n",
    "print(\"üîç Strategic Training System Check:\")\n",
    "quick_gpu_check()\n",
    "quick_memory_check()\n",
    "\n",
    "# Mixed precision support check\n",
    "if torch.cuda.is_available():\n",
    "    scaler = gpu_optimizer.auto_mixed_precision_scaler()\n",
    "    print(f\"‚úÖ Automatic Mixed Precision: {'Enabled' if scaler else 'Disabled'}\")\n",
    "\n",
    "# Get strategic optimization recommendations\n",
    "recommendations = gpu_optimizer.get_optimization_recommendations()\n",
    "if recommendations:\n",
    "    print(\"\\nüîß Strategic Training Recommendations:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"  ‚Ä¢ {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## üìä Load and Analyze Strategic Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load 30-minute data for strategic training\n",
    "data_path = '/content/GrandModel/colab/data/NQ - 30 min - ETH.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    print(f\"‚úÖ Strategic data loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "    print(f\"   Price range: ${df['Close'].min():.2f} - ${df['Close'].max():.2f}\")\n",
    "    print(f\"   Data frequency: 30-minute bars\")\n",
    "    \n",
    "    # Calculate strategic metrics\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Volatility'] = df['Returns'].rolling(20).std()\n",
    "    df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "    df['SMA_50'] = df['Close'].rolling(50).mean()\n",
    "    \n",
    "    # Display enhanced preview\n",
    "    print(\"\\nüìã Strategic Data Preview:\")\n",
    "    display(df.head(10))\n",
    "    \n",
    "    # Strategic data quality check\n",
    "    missing_data = df.isnull().sum().sum()\n",
    "    print(f\"\\nüîç Data Quality: {missing_data} missing values\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Strategic data file not found at {data_path}\")\n",
    "    print(\"Please ensure the 30-minute data file exists in the correct location.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strategic_data_analysis"
   },
   "outputs": [],
   "source": [
    "# Enhanced strategic data visualization\n",
    "if df is not None:\n",
    "    # Create interactive plotly charts for strategic analysis\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Price Chart with Moving Averages', 'Volume Analysis',\n",
    "            'Returns Distribution', 'Rolling Volatility',\n",
    "            'Price vs Volume Correlation', 'Market Regime Indicators',\n",
    "            'Cumulative Returns', 'Risk-Return Analysis'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Price chart with moving averages\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['Date'], y=df['Close'], name='Close Price', line=dict(width=1)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['Date'], y=df['SMA_20'], name='SMA 20', line=dict(width=1, dash='dash')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['Date'], y=df['SMA_50'], name='SMA 50', line=dict(width=1, dash='dot')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Volume analysis\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df['Date'], y=df['Volume'], name='Volume', opacity=0.7),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Returns distribution\n",
    "    returns_clean = df['Returns'].dropna()\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=returns_clean, nbinsx=50, name='Returns Distribution'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Rolling volatility\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['Date'], y=df['Volatility'], name='Rolling Volatility (20)', \n",
    "                  line=dict(width=2, color='red')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Price vs Volume correlation\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['Volume'], y=df['Close'], mode='markers', \n",
    "                  name='Price vs Volume', opacity=0.6),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Market regime indicators (trend strength)\n",
    "    trend_strength = (df['SMA_20'] - df['SMA_50']) / df['SMA_50']\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['Date'], y=trend_strength, name='Trend Strength', \n",
    "                  line=dict(width=2, color='purple')),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Cumulative returns\n",
    "    cumulative_returns = (1 + df['Returns'].fillna(0)).cumprod()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['Date'], y=cumulative_returns, name='Cumulative Returns', \n",
    "                  line=dict(width=2, color='green')),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # Risk-return scatter (rolling windows)\n",
    "    rolling_return = df['Returns'].rolling(20).mean()\n",
    "    rolling_risk = df['Returns'].rolling(20).std()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=rolling_risk, y=rolling_return, mode='markers', \n",
    "                  name='Risk-Return Profile', opacity=0.7),\n",
    "        row=4, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=1200, title=\"Strategic Market Data Analysis - 30min NQ Futures\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Calculate and display strategic statistics\n",
    "    print(\"\\nüìà Strategic Market Statistics:\")\n",
    "    print(f\"   Total Return: {(cumulative_returns.iloc[-1] - 1) * 100:.2f}%\")\n",
    "    print(f\"   Annualized Volatility: {df['Returns'].std() * np.sqrt(252 * 48) * 100:.2f}%\")\n",
    "    print(f\"   Sharpe Ratio: {(df['Returns'].mean() / df['Returns'].std()) * np.sqrt(252 * 48):.2f}\")\n",
    "    print(f\"   Maximum Drawdown: {((cumulative_returns / cumulative_returns.expanding().max()) - 1).min() * 100:.2f}%\")\n",
    "    print(f\"   Skewness: {df['Returns'].skew():.3f}\")\n",
    "    print(f\"   Kurtosis: {df['Returns'].kurtosis():.3f}\")\n",
    "    \n",
    "    # Market regime analysis\n",
    "    high_vol_threshold = df['Volatility'].quantile(0.75)\n",
    "    low_vol_threshold = df['Volatility'].quantile(0.25)\n",
    "    \n",
    "    high_vol_periods = (df['Volatility'] > high_vol_threshold).sum()\n",
    "    low_vol_periods = (df['Volatility'] < low_vol_threshold).sum()\n",
    "    \n",
    "    print(f\"\\nüéØ Market Regime Analysis:\")\n",
    "    print(f\"   High Volatility Periods: {high_vol_periods / len(df) * 100:.1f}% of time\")\n",
    "    print(f\"   Low Volatility Periods: {low_vol_periods / len(df) * 100:.1f}% of time\")\n",
    "    print(f\"   Trending Periods: {(abs(trend_strength) > 0.02).sum() / len(df) * 100:.1f}% of time\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for strategic analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trainer_section"
   },
   "source": [
    "## üéØ Initialize Strategic MAPPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_strategic_trainer"
   },
   "outputs": [],
   "source": [
    "# Initialize strategic trainer with enhanced settings\n",
    "device = gpu_optimizer.device\n",
    "\n",
    "strategic_trainer = StrategicMAPPOTrainer(\n",
    "    state_dim=13,         # 30min matrix features including MMD\n",
    "    action_dim=7,         # HOLD, BUY_CONSERVATIVE, BUY_AGGRESSIVE, SELL_CONSERVATIVE, SELL_AGGRESSIVE, REDUCE_RISK, INCREASE_RISK\n",
    "    n_agents=3,           # strategic_agent, portfolio_manager, regime_detector\n",
    "    lr_actor=1e-4,        # Lower learning rate for stability\n",
    "    lr_critic=3e-4,       # Learning rate for critic networks\n",
    "    gamma=0.995,          # Higher gamma for long-term rewards\n",
    "    eps_clip=0.1,         # Smaller clipping for strategic stability\n",
    "    k_epochs=8,           # More epochs for strategic learning\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Strategic MAPPO Trainer initialized!\")\n",
    "print(f\"   Device: {strategic_trainer.device}\")\n",
    "print(f\"   State dimension: {strategic_trainer.state_dim}\")\n",
    "print(f\"   Action dimension: {strategic_trainer.action_dim}\")\n",
    "print(f\"   Number of strategic agents: {strategic_trainer.n_agents}\")\n",
    "print(f\"   Gamma (discount factor): {strategic_trainer.gamma}\")\n",
    "print(f\"   Learning rates: Actor={1e-4}, Critic={3e-4}\")\n",
    "\n",
    "# Enhanced model profiling for strategic networks\n",
    "print(\"\\nüîç Strategic Model Profiling:\")\n",
    "agent_names = ['Strategic Agent', 'Portfolio Manager', 'Regime Detector']\n",
    "total_params = 0\n",
    "\n",
    "for i, (actor, critic) in enumerate(zip(strategic_trainer.actors, strategic_trainer.critics)):\n",
    "    actor_profile = gpu_optimizer.profile_model(actor, (strategic_trainer.state_dim,), batch_size=16)\n",
    "    critic_profile = gpu_optimizer.profile_model(critic, (strategic_trainer.state_dim,), batch_size=16)\n",
    "    \n",
    "    agent_params = actor_profile['total_parameters'] + critic_profile['total_parameters']\n",
    "    total_params += agent_params\n",
    "    \n",
    "    print(f\"   {agent_names[i]}:\")\n",
    "    print(f\"     Actor: {actor_profile['total_parameters']:,} params, {actor_profile['model_size_mb']:.1f} MB\")\n",
    "    print(f\"     Critic: {critic_profile['total_parameters']:,} params, {critic_profile['model_size_mb']:.1f} MB\")\n",
    "    print(f\"     Total: {agent_params:,} parameters\")\n",
    "\n",
    "print(f\"\\nüìä Total Model Parameters: {total_params:,}\")\n",
    "print(f\"   Estimated Total Memory: {total_params * 4 / 1e6:.1f} MB\")\n",
    "\n",
    "# Find optimal batch size for strategic training\n",
    "strategic_batch_size = gpu_optimizer.optimize_batch_size(\n",
    "    strategic_trainer.actors[0], \n",
    "    (strategic_trainer.state_dim,), \n",
    "    start_batch_size=16,\n",
    "    max_batch_size=128\n",
    ")\n",
    "print(f\"\\n‚ö° Optimal Strategic Batch Size: {strategic_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategic_config_section"
   },
   "source": [
    "## üéØ Strategic Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strategic_training_config"
   },
   "outputs": [],
   "source": [
    "# Strategic training configuration\n",
    "STRATEGIC_CONFIG = {\n",
    "    'num_episodes': 300,        # More episodes for strategic learning\n",
    "    'episode_length': 500,      # Longer episodes for strategic decisions\n",
    "    'save_frequency': 20,       # Save more frequently\n",
    "    'plot_frequency': 30,       # Plot progress more often\n",
    "    'validation_frequency': 50, # Validate strategic performance\n",
    "    'early_stopping_patience': 75,  # More patience for strategic learning\n",
    "    'target_reward': 200.0,     # Higher target for strategic performance\n",
    "    'target_sharpe': 2.0,       # Target Sharpe ratio\n",
    "    'max_drawdown_threshold': -0.15,  # Maximum acceptable drawdown\n",
    "    'use_mixed_precision': torch.cuda.is_available(),\n",
    "    'gradient_accumulation_steps': 2\n",
    "}\n",
    "\n",
    "print(\"üéØ Strategic Training Configuration:\")\n",
    "for key, value in STRATEGIC_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Create enhanced directories for strategic training\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "strategic_save_dir = f'/content/GrandModel/colab/exports/strategic_training_{timestamp}'\n",
    "os.makedirs(strategic_save_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories\n",
    "os.makedirs(os.path.join(strategic_save_dir, 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(strategic_save_dir, 'plots'), exist_ok=True)\n",
    "os.makedirs(os.path.join(strategic_save_dir, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(strategic_save_dir, 'logs'), exist_ok=True)\n",
    "\n",
    "print(f\"\\nüíæ Strategic Save Directory: {strategic_save_dir}\")\n",
    "\n",
    "# Initialize mixed precision scaler if available\n",
    "scaler = None\n",
    "if STRATEGIC_CONFIG['use_mixed_precision'] and torch.cuda.is_available():\n",
    "    scaler = gpu_optimizer.auto_mixed_precision_scaler()\n",
    "    print(f\"‚úÖ Mixed Precision Training: Enabled\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Mixed Precision Training: Disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategic_training_section"
   },
   "source": [
    "## üöÄ Strategic Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strategic_training_loop",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enhanced strategic training loop\n",
    "print(\"üöÄ Starting Strategic MAPPO Training...\\n\")\n",
    "\n",
    "if df is not None:\n",
    "    # Strategic training metrics\n",
    "    training_start_time = time.time()\n",
    "    best_reward = float('-inf')\n",
    "    best_sharpe = float('-inf')\n",
    "    episodes_without_improvement = 0\n",
    "    performance_history = []\n",
    "    \n",
    "    # Enhanced progress tracking\n",
    "    pbar = tqdm(range(STRATEGIC_CONFIG['num_episodes']), desc=\"Strategic Training\")\n",
    "    \n",
    "    for episode in pbar:\n",
    "        episode_start_time = time.time()\n",
    "        \n",
    "        # Enhanced memory monitoring\n",
    "        if episode % 5 == 0:\n",
    "            memory_info = gpu_optimizer.monitor_memory()\n",
    "            \n",
    "        # Strategic episode data selection (ensure enough historical data)\n",
    "        min_start_idx = 100  # Need more history for strategic features\n",
    "        max_start_idx = len(df) - STRATEGIC_CONFIG['episode_length'] - 50\n",
    "        start_idx = np.random.randint(min_start_idx, max_start_idx)\n",
    "        \n",
    "        # Train strategic episode\n",
    "        episode_reward, episode_steps, final_portfolio_value = strategic_trainer.train_episode(\n",
    "            data=df,\n",
    "            start_idx=start_idx,\n",
    "            episode_length=STRATEGIC_CONFIG['episode_length']\n",
    "        )\n",
    "        \n",
    "        episode_time = time.time() - episode_start_time\n",
    "        \n",
    "        # Get enhanced statistics\n",
    "        stats = strategic_trainer.get_strategic_stats()\n",
    "        current_sharpe = stats.get('latest_sharpe', 0.0)\n",
    "        current_drawdown = stats.get('latest_drawdown', 0.0)\n",
    "        current_regime = stats.get('current_regime', 'UNKNOWN')\n",
    "        \n",
    "        # Enhanced progress display\n",
    "        pbar.set_postfix({\n",
    "            'Reward': f\"{episode_reward:.2f}\",\n",
    "            'Sharpe': f\"{current_sharpe:.2f}\",\n",
    "            'Portfolio': f\"{final_portfolio_value/1e6:.2f}M\",\n",
    "            'Regime': current_regime[:4],\n",
    "            'Time': f\"{episode_time:.1f}s\"\n",
    "        })\n",
    "        \n",
    "        # Track performance history\n",
    "        performance_history.append({\n",
    "            'episode': episode,\n",
    "            'reward': episode_reward,\n",
    "            'sharpe': current_sharpe,\n",
    "            'portfolio_value': final_portfolio_value,\n",
    "            'drawdown': current_drawdown,\n",
    "            'regime': current_regime,\n",
    "            'training_time': episode_time\n",
    "        })\n",
    "        \n",
    "        # Multi-criteria improvement tracking\n",
    "        improvement_found = False\n",
    "        \n",
    "        # Check for reward improvement\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            improvement_found = True\n",
    "            \n",
    "        # Check for Sharpe ratio improvement\n",
    "        if current_sharpe > best_sharpe:\n",
    "            best_sharpe = current_sharpe\n",
    "            improvement_found = True\n",
    "            \n",
    "            # Save best Sharpe model\n",
    "            best_sharpe_path = os.path.join(strategic_save_dir, 'models', 'best_sharpe_strategic_model.pth')\n",
    "            strategic_trainer.save_checkpoint(best_sharpe_path)\n",
    "        \n",
    "        if improvement_found:\n",
    "            episodes_without_improvement = 0\n",
    "            \n",
    "            # Save best overall model\n",
    "            best_model_path = os.path.join(strategic_save_dir, 'models', 'best_strategic_model.pth')\n",
    "            strategic_trainer.save_checkpoint(best_model_path)\n",
    "        else:\n",
    "            episodes_without_improvement += 1\n",
    "        \n",
    "        # Enhanced periodic saves and analysis\n",
    "        if (episode + 1) % STRATEGIC_CONFIG['save_frequency'] == 0:\n",
    "            checkpoint_path = os.path.join(strategic_save_dir, 'checkpoints', f'strategic_checkpoint_ep{episode+1}.pth')\n",
    "            strategic_trainer.save_checkpoint(checkpoint_path)\n",
    "            \n",
    "            # Save performance history\n",
    "            performance_file = os.path.join(strategic_save_dir, 'logs', f'performance_history_ep{episode+1}.json')\n",
    "            with open(performance_file, 'w') as f:\n",
    "                json.dump(performance_history, f, indent=2)\n",
    "            \n",
    "        if (episode + 1) % STRATEGIC_CONFIG['plot_frequency'] == 0:\n",
    "            # Strategic progress plots\n",
    "            plot_path = os.path.join(strategic_save_dir, 'plots', f'strategic_progress_ep{episode+1}.png')\n",
    "            strategic_trainer.plot_strategic_progress(save_path=plot_path)\n",
    "            \n",
    "            # Memory usage plots\n",
    "            memory_plot_path = os.path.join(strategic_save_dir, 'plots', f'memory_usage_ep{episode+1}.png')\n",
    "            gpu_optimizer.plot_memory_usage(save_path=memory_plot_path)\n",
    "        \n",
    "        # Strategic validation\n",
    "        if (episode + 1) % STRATEGIC_CONFIG['validation_frequency'] == 0:\n",
    "            print(f\"\\nüß™ Strategic Validation at Episode {episode+1}:\")\n",
    "            print(f\"   Best Reward: {best_reward:.3f}\")\n",
    "            print(f\"   Best Sharpe: {best_sharpe:.3f}\")\n",
    "            print(f\"   Current Regime: {current_regime}\")\n",
    "            print(f\"   Portfolio Value: ${final_portfolio_value:,.0f}\")\n",
    "        \n",
    "        # Enhanced early stopping criteria\n",
    "        stop_training = False\n",
    "        \n",
    "        # Standard patience check\n",
    "        if episodes_without_improvement >= STRATEGIC_CONFIG['early_stopping_patience']:\n",
    "            print(f\"\\nüõë Early stopping: {episodes_without_improvement} episodes without improvement\")\n",
    "            stop_training = True\n",
    "            \n",
    "        # Target achievement checks\n",
    "        if episode_reward >= STRATEGIC_CONFIG['target_reward']:\n",
    "            print(f\"\\nüéâ Target reward {STRATEGIC_CONFIG['target_reward']} achieved!\")\n",
    "            stop_training = True\n",
    "            \n",
    "        if current_sharpe >= STRATEGIC_CONFIG['target_sharpe']:\n",
    "            print(f\"\\nüéâ Target Sharpe ratio {STRATEGIC_CONFIG['target_sharpe']} achieved!\")\n",
    "            stop_training = True\n",
    "        \n",
    "        # Risk management check\n",
    "        if current_drawdown < STRATEGIC_CONFIG['max_drawdown_threshold']:\n",
    "            print(f\"\\n‚ö†Ô∏è Maximum drawdown threshold exceeded: {current_drawdown:.2%}\")\n",
    "            # Don't stop, but log the event\n",
    "        \n",
    "        if stop_training:\n",
    "            break\n",
    "        \n",
    "        # Enhanced memory management\n",
    "        if episode % 10 == 0:\n",
    "            gpu_optimizer.clear_cache()\n",
    "        \n",
    "        # Dynamic learning rate adjustment (optional)\n",
    "        if episode > 0 and episode % 100 == 0:\n",
    "            # Reduce learning rate by 10% every 100 episodes\n",
    "            for optimizer in strategic_trainer.actor_optimizers + strategic_trainer.critic_optimizers:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.9\n",
    "            print(f\"\\nüìâ Learning rate reduced at episode {episode}\")\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Training completion summary\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(f\"\\n‚úÖ Strategic Training Completed!\")\n",
    "    print(f\"   Total Time: {total_training_time/60:.1f} minutes\")\n",
    "    print(f\"   Episodes Completed: {len(strategic_trainer.episode_rewards)}\")\n",
    "    print(f\"   Best Reward: {best_reward:.3f}\")\n",
    "    print(f\"   Best Sharpe Ratio: {best_sharpe:.3f}\")\n",
    "    print(f\"   Final Portfolio Value: ${final_portfolio_value:,.0f}\")\n",
    "    \n",
    "    # Save final comprehensive model\n",
    "    final_model_path = os.path.join(strategic_save_dir, 'models', 'final_strategic_model.pth')\n",
    "    strategic_trainer.save_checkpoint(final_model_path)\n",
    "    \n",
    "    # Save final performance history\n",
    "    final_performance_file = os.path.join(strategic_save_dir, 'logs', 'final_performance_history.json')\n",
    "    with open(final_performance_file, 'w') as f:\n",
    "        json.dump(performance_history, f, indent=2)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot start strategic training - data not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategic_results_section"
   },
   "source": [
    "## üìä Strategic Training Results and Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strategic_results_analysis"
   },
   "outputs": [],
   "source": [
    "# Enhanced strategic results analysis\n",
    "if len(strategic_trainer.episode_rewards) > 0:\n",
    "    # Final comprehensive plots\n",
    "    final_plot_path = os.path.join(strategic_save_dir, 'plots', 'final_strategic_results.png')\n",
    "    strategic_trainer.plot_strategic_progress(save_path=final_plot_path)\n",
    "    \n",
    "    # Final memory analysis\n",
    "    final_memory_plot = os.path.join(strategic_save_dir, 'plots', 'final_memory_analysis.png')\n",
    "    gpu_optimizer.plot_memory_usage(save_path=final_memory_plot)\n",
    "    \n",
    "    # Create interactive performance dashboard\n",
    "    if len(performance_history) > 0:\n",
    "        df_perf = pd.DataFrame(performance_history)\n",
    "        \n",
    "        # Interactive performance dashboard\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Episode Rewards Over Time', 'Sharpe Ratio Evolution',\n",
    "                'Portfolio Value Growth', 'Drawdown Analysis',\n",
    "                'Market Regime Distribution', 'Training Time per Episode'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Episode rewards\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_perf['episode'], y=df_perf['reward'], \n",
    "                      name='Episode Reward', line=dict(width=2)),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Sharpe ratio\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_perf['episode'], y=df_perf['sharpe'], \n",
    "                      name='Sharpe Ratio', line=dict(width=2, color='green')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Portfolio value\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_perf['episode'], y=df_perf['portfolio_value'], \n",
    "                      name='Portfolio Value', line=dict(width=2, color='blue')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Drawdown\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_perf['episode'], y=df_perf['drawdown'], \n",
    "                      name='Drawdown', line=dict(width=2, color='red')),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Market regime distribution\n",
    "        regime_counts = df_perf['regime'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=regime_counts.index, y=regime_counts.values, \n",
    "                   name='Regime Frequency'),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # Training time\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_perf['episode'], y=df_perf['training_time'], \n",
    "                      name='Training Time (s)', line=dict(width=1, color='purple')),\n",
    "            row=3, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=1000, title=\"Strategic MAPPO Training Dashboard\")\n",
    "        fig.show()\n",
    "    \n",
    "    print(\"üìä Strategic training analysis completed!\")\n",
    "else:\n",
    "    print(\"‚ùå No strategic training data to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strategic_statistics"
   },
   "outputs": [],
   "source": [
    "# Comprehensive strategic training statistics\n",
    "if len(strategic_trainer.episode_rewards) > 0:\n",
    "    final_stats = strategic_trainer.get_strategic_stats()\n",
    "    \n",
    "    print(\"üéØ Final Strategic Training Statistics:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic metrics\n",
    "    print(f\"üìà Performance Metrics:\")\n",
    "    print(f\"   Episodes completed: {final_stats['episodes']}\")\n",
    "    print(f\"   Total training steps: {final_stats['total_steps']:,}\")\n",
    "    print(f\"   Best episode reward: {final_stats['best_reward']:.3f}\")\n",
    "    print(f\"   Average reward (last 50): {final_stats['avg_reward_50']:.3f}\")\n",
    "    print(f\"   Latest episode reward: {final_stats['latest_reward']:.3f}\")\n",
    "    \n",
    "    # Financial metrics\n",
    "    print(f\"\\nüí∞ Financial Performance:\")\n",
    "    print(f\"   Best Sharpe ratio: {final_stats['best_sharpe']:.3f}\")\n",
    "    print(f\"   Latest Sharpe ratio: {final_stats['latest_sharpe']:.3f}\")\n",
    "    print(f\"   Latest portfolio value: ${final_stats['latest_portfolio_value']:,.0f}\")\n",
    "    print(f\"   Latest drawdown: {final_stats['latest_drawdown']:.2%}\")\n",
    "    print(f\"   Current market regime: {final_stats['current_regime']}\")\n",
    "    \n",
    "    # Training stability\n",
    "    print(f\"\\nüîß Training Stability:\")\n",
    "    print(f\"   Final actor loss: {final_stats['actor_loss']:.6f}\")\n",
    "    print(f\"   Final critic loss: {final_stats['critic_loss']:.6f}\")\n",
    "    \n",
    "    # Advanced performance analysis\n",
    "    if len(strategic_trainer.episode_rewards) >= 20:\n",
    "        rewards = np.array(strategic_trainer.episode_rewards)\n",
    "        recent_rewards = rewards[-20:]\n",
    "        early_rewards = rewards[:20]\n",
    "        \n",
    "        improvement = np.mean(recent_rewards) - np.mean(early_rewards)\n",
    "        consistency = 1 - (np.std(recent_rewards) / np.mean(recent_rewards))\n",
    "        trend = np.polyfit(range(len(rewards)), rewards, 1)[0]\n",
    "        \n",
    "        print(f\"\\nüìä Advanced Analysis:\")\n",
    "        print(f\"   Performance improvement: {improvement:.3f}\")\n",
    "        print(f\"   Recent consistency: {consistency:.3f}\")\n",
    "        print(f\"   Learning trend: {trend:.6f} per episode\")\n",
    "    \n",
    "    # Portfolio performance metrics\n",
    "    if len(strategic_trainer.portfolio_values) > 1:\n",
    "        portfolio_returns = np.diff(strategic_trainer.portfolio_values) / strategic_trainer.portfolio_values[:-1]\n",
    "        total_return = (strategic_trainer.portfolio_values[-1] / strategic_trainer.portfolio_values[0]) - 1\n",
    "        volatility = np.std(portfolio_returns)\n",
    "        \n",
    "        print(f\"\\nüíº Portfolio Analytics:\")\n",
    "        print(f\"   Total return: {total_return:.2%}\")\n",
    "        print(f\"   Volatility: {volatility:.3f}\")\n",
    "        print(f\"   Return/Risk ratio: {(np.mean(portfolio_returns) / volatility):.3f}\")\n",
    "    \n",
    "    # Save comprehensive statistics\n",
    "    comprehensive_stats = {\n",
    "        'final_stats': final_stats,\n",
    "        'training_summary': {\n",
    "            'total_training_time_minutes': total_training_time / 60 if 'total_training_time' in locals() else 0,\n",
    "            'episodes_completed': len(strategic_trainer.episode_rewards),\n",
    "            'improvement': improvement if 'improvement' in locals() else 0,\n",
    "            'consistency': consistency if 'consistency' in locals() else 0,\n",
    "            'learning_trend': trend if 'trend' in locals() else 0\n",
    "        },\n",
    "        'portfolio_analytics': {\n",
    "            'total_return': total_return if 'total_return' in locals() else 0,\n",
    "            'volatility': volatility if 'volatility' in locals() else 0\n",
    "        } if 'total_return' in locals() else {}\n",
    "    }\n",
    "    \n",
    "    stats_file = os.path.join(strategic_save_dir, 'logs', 'comprehensive_statistics.json')\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(comprehensive_stats, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüíæ Comprehensive statistics saved to: {stats_file}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No strategic training statistics available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategic_validation_section"
   },
   "source": [
    "## üß™ Strategic Model Validation and Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strategic_validation"
   },
   "outputs": [],
   "source": [
    "# Enhanced strategic model validation\n",
    "if len(strategic_trainer.episode_rewards) > 0 and df is not None:\n",
    "    print(\"üß™ Running Strategic Model Validation...\")\n",
    "    \n",
    "    # Use last 30% of data for validation\n",
    "    val_start_idx = int(len(df) * 0.7)\n",
    "    val_data = df.iloc[val_start_idx:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"   Validation data: {len(val_data)} bars ({len(val_data)/48:.1f} days)\")\n",
    "    \n",
    "    # Strategic validation metrics\n",
    "    validation_results = {\n",
    "        'episodes': [],\n",
    "        'rewards': [],\n",
    "        'portfolio_values': [],\n",
    "        'sharpe_ratios': [],\n",
    "        'max_drawdowns': [],\n",
    "        'regimes_detected': []\n",
    "    }\n",
    "    \n",
    "    # Run multiple validation episodes\n",
    "    n_validation_episodes = 10\n",
    "    print(f\"   Running {n_validation_episodes} validation episodes...\")\n",
    "    \n",
    "    for val_episode in tqdm(range(n_validation_episodes), desc=\"Validation\"):\n",
    "        # Ensure enough data for strategic features\n",
    "        max_start = len(val_data) - 200  # Shorter validation episodes\n",
    "        start_idx = np.random.randint(100, max_start)\n",
    "        \n",
    "        # Run validation episode\n",
    "        val_reward, val_steps, val_portfolio = strategic_trainer.train_episode(\n",
    "            data=val_data,\n",
    "            start_idx=start_idx,\n",
    "            episode_length=150  # Shorter for validation\n",
    "        )\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_stats = strategic_trainer.get_strategic_stats()\n",
    "        \n",
    "        validation_results['episodes'].append(val_episode)\n",
    "        validation_results['rewards'].append(val_reward)\n",
    "        validation_results['portfolio_values'].append(val_portfolio)\n",
    "        validation_results['sharpe_ratios'].append(val_stats.get('latest_sharpe', 0))\n",
    "        validation_results['max_drawdowns'].append(val_stats.get('latest_drawdown', 0))\n",
    "        validation_results['regimes_detected'].append(val_stats.get('current_regime', 'UNKNOWN'))\n",
    "    \n",
    "    # Calculate validation statistics\n",
    "    val_rewards = np.array(validation_results['rewards'])\n",
    "    val_sharpes = np.array(validation_results['sharpe_ratios'])\n",
    "    val_portfolios = np.array(validation_results['portfolio_values'])\n",
    "    val_drawdowns = np.array(validation_results['max_drawdowns'])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Strategic Validation Results:\")\n",
    "    print(f\"   Average reward: {np.mean(val_rewards):.3f} ¬± {np.std(val_rewards):.3f}\")\n",
    "    print(f\"   Average Sharpe: {np.mean(val_sharpes):.3f} ¬± {np.std(val_sharpes):.3f}\")\n",
    "    print(f\"   Average portfolio: ${np.mean(val_portfolios):,.0f} ¬± ${np.std(val_portfolios):,.0f}\")\n",
    "    print(f\"   Average drawdown: {np.mean(val_drawdowns):.2%} ¬± {np.std(val_drawdowns):.2%}\")\n",
    "    print(f\"   Consistency (reward): {1 - np.std(val_rewards)/np.mean(val_rewards):.3f}\")\n",
    "    print(f\"   Win rate: {(val_rewards > 0).mean():.2%}\")\n",
    "    \n",
    "    # Regime detection analysis\n",
    "    regime_counts = pd.Series(validation_results['regimes_detected']).value_counts()\n",
    "    print(f\"\\nüéØ Market Regime Detection:\")\n",
    "    for regime, count in regime_counts.items():\n",
    "        print(f\"   {regime}: {count}/{n_validation_episodes} episodes ({count/n_validation_episodes:.1%})\")\n",
    "    \n",
    "    # Statistical significance tests\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Test if rewards are significantly positive\n",
    "    t_stat, p_value = stats.ttest_1samp(val_rewards, 0)\n",
    "    print(f\"\\nüìä Statistical Analysis:\")\n",
    "    print(f\"   T-test (rewards > 0): t={t_stat:.3f}, p={p_value:.4f}\")\n",
    "    print(f\"   Significance: {'‚úÖ Significant' if p_value < 0.05 else '‚ùå Not significant'} (Œ±=0.05)\")\n",
    "    \n",
    "    # Risk-adjusted performance\n",
    "    information_ratio = np.mean(val_rewards) / np.std(val_rewards) if np.std(val_rewards) > 0 else 0\n",
    "    calmar_ratio = np.mean(val_rewards) / abs(np.mean(val_drawdowns)) if np.mean(val_drawdowns) != 0 else 0\n",
    "    \n",
    "    print(f\"\\nüíº Risk-Adjusted Performance:\")\n",
    "    print(f\"   Information Ratio: {information_ratio:.3f}\")\n",
    "    print(f\"   Calmar Ratio: {calmar_ratio:.3f}\")\n",
    "    \n",
    "    # Save validation results\n",
    "    validation_summary = {\n",
    "        'validation_episodes': n_validation_episodes,\n",
    "        'validation_data_size': len(val_data),\n",
    "        'metrics': {\n",
    "            'mean_reward': float(np.mean(val_rewards)),\n",
    "            'std_reward': float(np.std(val_rewards)),\n",
    "            'mean_sharpe': float(np.mean(val_sharpes)),\n",
    "            'std_sharpe': float(np.std(val_sharpes)),\n",
    "            'mean_portfolio': float(np.mean(val_portfolios)),\n",
    "            'std_portfolio': float(np.std(val_portfolios)),\n",
    "            'mean_drawdown': float(np.mean(val_drawdowns)),\n",
    "            'consistency': float(1 - np.std(val_rewards)/np.mean(val_rewards)),\n",
    "            'win_rate': float((val_rewards > 0).mean()),\n",
    "            'information_ratio': float(information_ratio),\n",
    "            'calmar_ratio': float(calmar_ratio)\n",
    "        },\n",
    "        'statistical_tests': {\n",
    "            't_statistic': float(t_stat),\n",
    "            'p_value': float(p_value),\n",
    "            'significant': bool(p_value < 0.05)\n",
    "        },\n",
    "        'regime_analysis': dict(regime_counts),\n",
    "        'detailed_results': validation_results\n",
    "    }\n",
    "    \n",
    "    validation_file = os.path.join(strategic_save_dir, 'logs', 'strategic_validation_results.json')\n",
    "    with open(validation_file, 'w') as f:\n",
    "        json.dump(validation_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüíæ Validation results saved to: {validation_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot run strategic validation - no trained model or data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategic_export_section"
   },
   "source": [
    "## üì¶ Export Strategic Models and Production Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strategic_export"
   },
   "outputs": [],
   "source": [
    "# Comprehensive strategic model export\n",
    "if len(strategic_trainer.episode_rewards) > 0:\n",
    "    print(\"üì¶ Exporting Strategic MAPPO Models...\")\n",
    "    \n",
    "    # Strategic agent names and roles\n",
    "    strategic_agents = {\n",
    "        'strategic_agent': 'Long-term market strategy and position management',\n",
    "        'portfolio_manager': 'Portfolio optimization and risk management',\n",
    "        'regime_detector': 'Market regime identification and adaptation'\n",
    "    }\n",
    "    \n",
    "    # Export individual strategic agent models\n",
    "    for i, (agent_name, description) in enumerate(strategic_agents.items()):\n",
    "        # Export actor network\n",
    "        actor_path = os.path.join(strategic_save_dir, 'models', f'{agent_name}_actor.pth')\n",
    "        torch.save(strategic_trainer.actors[i].state_dict(), actor_path)\n",
    "        \n",
    "        # Export critic network\n",
    "        critic_path = os.path.join(strategic_save_dir, 'models', f'{agent_name}_critic.pth')\n",
    "        torch.save(strategic_trainer.critics[i].state_dict(), critic_path)\n",
    "        \n",
    "        # Export optimizers\n",
    "        actor_opt_path = os.path.join(strategic_save_dir, 'models', f'{agent_name}_actor_optimizer.pth')\n",
    "        torch.save(strategic_trainer.actor_optimizers[i].state_dict(), actor_opt_path)\n",
    "        \n",
    "        critic_opt_path = os.path.join(strategic_save_dir, 'models', f'{agent_name}_critic_optimizer.pth')\n",
    "        torch.save(strategic_trainer.critic_optimizers[i].state_dict(), critic_opt_path)\n",
    "        \n",
    "        print(f\"   ‚úÖ {agent_name} exported (Actor + Critic + Optimizers)\")\n",
    "    \n",
    "    # Export complete model architecture for reconstruction\n",
    "    model_architecture = {\n",
    "        'trainer_class': 'StrategicMAPPOTrainer',\n",
    "        'network_architecture': {\n",
    "            'actor': {\n",
    "                'class': 'StrategicActorNetwork',\n",
    "                'hidden_dim': 512,\n",
    "                'layers': ['Linear', 'ReLU', 'BatchNorm1d', 'Dropout', 'Linear', 'ReLU', 'BatchNorm1d', 'Dropout', 'Linear', 'ReLU', 'Linear', 'Softmax']\n",
    "            },\n",
    "            'critic': {\n",
    "                'class': 'StrategicCriticNetwork',\n",
    "                'hidden_dim': 512,\n",
    "                'layers': ['Linear', 'ReLU', 'BatchNorm1d', 'Dropout', 'Linear', 'ReLU', 'BatchNorm1d', 'Dropout', 'Linear', 'ReLU', 'Linear']\n",
    "            }\n",
    "        },\n",
    "        'training_parameters': {\n",
    "            'lr_actor': 1e-4,\n",
    "            'lr_critic': 3e-4,\n",
    "            'gamma': 0.995,\n",
    "            'eps_clip': 0.1,\n",
    "            'k_epochs': 8,\n",
    "            'gae_lambda': 0.98\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Enhanced configuration with strategic parameters\n",
    "    strategic_config = {\n",
    "        'model_info': {\n",
    "            'model_type': 'StrategicMAPPO',\n",
    "            'version': '1.0',\n",
    "            'timestamp': timestamp,\n",
    "            'training_episodes': len(strategic_trainer.episode_rewards),\n",
    "            'data_timeframe': '30min',\n",
    "            'market': 'NQ Futures',\n",
    "            'training_duration_minutes': total_training_time / 60 if 'total_training_time' in locals() else 0\n",
    "        },\n",
    "        'model_architecture': model_architecture,\n",
    "        'agent_configuration': {\n",
    "            'state_dim': strategic_trainer.state_dim,\n",
    "            'action_dim': strategic_trainer.action_dim,\n",
    "            'n_agents': strategic_trainer.n_agents,\n",
    "            'agent_roles': strategic_agents\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'best_reward': strategic_trainer.training_stats['best_reward'],\n",
    "            'best_sharpe': strategic_trainer.training_stats['best_sharpe'],\n",
    "            'avg_reward_50': strategic_trainer.training_stats['avg_reward_50'],\n",
    "            'final_portfolio_value': final_portfolio_value if 'final_portfolio_value' in locals() else 0\n",
    "        },\n",
    "        'action_mapping': {\n",
    "            0: 'HOLD',\n",
    "            1: 'BUY_CONSERVATIVE',\n",
    "            2: 'BUY_AGGRESSIVE', \n",
    "            3: 'SELL_CONSERVATIVE',\n",
    "            4: 'SELL_AGGRESSIVE',\n",
    "            5: 'REDUCE_RISK',\n",
    "            6: 'INCREASE_RISK'\n",
    "        },\n",
    "        'feature_description': {\n",
    "            'state_features': [\n",
    "                'price_change_1d', 'price_change_3d', 'price_change_7d',\n",
    "                'volatility_short', 'volatility_long', 'volume_ratio',\n",
    "                'volume_trend', 'rsi', 'bollinger_position', 'macd_signal',\n",
    "                'higher_highs', 'lower_lows', 'mmd_proxy'\n",
    "            ],\n",
    "            'lookback_periods': {\n",
    "                'strategic_analysis': 48,  # 24 hours of 30min data\n",
    "                'volatility_short': 16,    # 8 hours\n",
    "                'volatility_long': 32,     # 16 hours\n",
    "                'rsi_period': 14,\n",
    "                'bollinger_period': 20,\n",
    "                'macd_fast': 12,\n",
    "                'macd_slow': 26,\n",
    "                'macd_signal': 9\n",
    "            }\n",
    "        },\n",
    "        'deployment_info': {\n",
    "            'device': str(strategic_trainer.device),\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'recommended_batch_size': strategic_batch_size if 'strategic_batch_size' in locals() else 16,\n",
    "            'memory_requirements_mb': total_params * 4 / 1e6 if 'total_params' in locals() else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(strategic_save_dir, 'strategic_model_config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(strategic_config, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üìã Strategic configuration saved: {config_path}\")\n",
    "    \n",
    "    # Create comprehensive README for strategic models\n",
    "    readme_content = f\"\"\"\n",
    "# Strategic MAPPO Training Export - GrandModel\n",
    "\n",
    "## üéØ Overview\n",
    "**Training Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Model Type**: Strategic Multi-Agent Proximal Policy Optimization\n",
    "**Market**: NQ Futures (30-minute bars)\n",
    "**Training Episodes**: {len(strategic_trainer.episode_rewards)}\n",
    "**Best Reward**: {strategic_trainer.training_stats['best_reward']:.3f}\n",
    "**Best Sharpe Ratio**: {strategic_trainer.training_stats['best_sharpe']:.3f}\n",
    "\n",
    "## ü§ñ Agent Architecture\n",
    "\n",
    "### Strategic Agents:\n",
    "1. **Strategic Agent**: Long-term market strategy and position management\n",
    "2. **Portfolio Manager**: Portfolio optimization and risk management  \n",
    "3. **Regime Detector**: Market regime identification and adaptation\n",
    "\n",
    "### Network Architecture:\n",
    "- **State Dimension**: {strategic_trainer.state_dim} features\n",
    "- **Action Dimension**: {strategic_trainer.action_dim} actions\n",
    "- **Actor Networks**: 512 hidden units with BatchNorm and Dropout\n",
    "- **Critic Networks**: 512 hidden units with BatchNorm and Dropout\n",
    "\n",
    "## üìä Performance Metrics\n",
    "- **Best Episode Reward**: {strategic_trainer.training_stats['best_reward']:.3f}\n",
    "- **Best Sharpe Ratio**: {strategic_trainer.training_stats['best_sharpe']:.3f}\n",
    "- **Average Reward (50 episodes)**: {strategic_trainer.training_stats['avg_reward_50']:.3f}\n",
    "- **Current Market Regime**: {strategic_trainer.training_stats['current_regime']}\n",
    "\n",
    "## üìÅ Files Included\n",
    "\n",
    "### Models:\n",
    "- `best_strategic_model.pth`: Best overall performance checkpoint\n",
    "- `best_sharpe_strategic_model.pth`: Best Sharpe ratio checkpoint\n",
    "- `final_strategic_model.pth`: Final training checkpoint\n",
    "- `*_actor.pth`: Individual agent actor networks\n",
    "- `*_critic.pth`: Individual agent critic networks\n",
    "- `*_optimizer.pth`: Optimizer states for continued training\n",
    "\n",
    "### Configuration:\n",
    "- `strategic_model_config.json`: Complete model configuration\n",
    "- `comprehensive_statistics.json`: Training performance metrics\n",
    "- `strategic_validation_results.json`: Validation and backtesting results\n",
    "\n",
    "### Analysis:\n",
    "- `plots/`: Training progress and performance visualizations\n",
    "- `logs/`: Detailed training logs and performance history\n",
    "- `checkpoints/`: Periodic training checkpoints\n",
    "\n",
    "## üöÄ Deployment Guide\n",
    "\n",
    "### Loading Models:\n",
    "```python\n",
    "from colab.trainers.strategic_mappo_trainer import StrategicMAPPOTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = StrategicMAPPOTrainer(\n",
    "    state_dim={strategic_trainer.state_dim},\n",
    "    action_dim={strategic_trainer.action_dim},\n",
    "    n_agents={strategic_trainer.n_agents}\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "trainer.load_checkpoint('best_strategic_model.pth')\n",
    "```\n",
    "\n",
    "### Action Mapping:\n",
    "- `0`: HOLD - Maintain current positions\n",
    "- `1`: BUY_CONSERVATIVE - Small long position increase\n",
    "- `2`: BUY_AGGRESSIVE - Large long position increase\n",
    "- `3`: SELL_CONSERVATIVE - Small position reduction\n",
    "- `4`: SELL_AGGRESSIVE - Large position reduction\n",
    "- `5`: REDUCE_RISK - Lower portfolio risk exposure\n",
    "- `6`: INCREASE_RISK - Higher portfolio risk exposure\n",
    "\n",
    "### Market Regimes Detected:\n",
    "- `BULL_TREND`: Strong upward trend\n",
    "- `BEAR_TREND`: Strong downward trend\n",
    "- `VOLATILE`: High volatility environment\n",
    "- `CONSOLIDATION`: Low volatility, sideways movement\n",
    "- `CRISIS`: Extreme market stress\n",
    "- `NORMAL`: Standard market conditions\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "1. **Risk Management**: These models are trained for strategic decisions and should be combined with appropriate risk management systems.\n",
    "\n",
    "2. **Market Conditions**: Performance may vary in different market conditions. Monitor regime detection output.\n",
    "\n",
    "3. **Integration**: Combine with tactical models for complete trading system.\n",
    "\n",
    "4. **Validation**: Always validate on recent data before live deployment.\n",
    "\n",
    "## üìû Support\n",
    "For questions about model deployment or integration, refer to the GrandModel documentation.\n",
    "\n",
    "---\n",
    "*Generated by Strategic MAPPO Training System - GrandModel*\n",
    "\"\"\"\n",
    "    \n",
    "    readme_path = os.path.join(strategic_save_dir, 'README.md')\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"üìñ Comprehensive README created: {readme_path}\")\n",
    "    \n",
    "    # Create deployment script\n",
    "    deployment_script = f\"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Strategic MAPPO Model Deployment Script\n",
    "Auto-generated deployment helper for GrandModel Strategic agents\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "class StrategicModelDeployment:\n",
    "    def __init__(self, model_path='best_strategic_model.pth'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_path = model_path\n",
    "        self.trainer = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        from colab.trainers.strategic_mappo_trainer import StrategicMAPPOTrainer\n",
    "        \n",
    "        self.trainer = StrategicMAPPOTrainer(\n",
    "            state_dim={strategic_trainer.state_dim},\n",
    "            action_dim={strategic_trainer.action_dim},\n",
    "            n_agents={strategic_trainer.n_agents},\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        \n",
    "        self.trainer.load_checkpoint(self.model_path)\n",
    "        print(f\"Strategic model loaded from {{self.model_path}}\")\n",
    "    \n",
    "    def predict(self, market_data, deterministic=True):\n",
    "        \"\"\"\n",
    "        Make strategic predictions on market data\n",
    "        \n",
    "        Args:\n",
    "            market_data: DataFrame with OHLCV data (30min bars)\n",
    "            deterministic: Use deterministic actions for production\n",
    "        \n",
    "        Returns:\n",
    "            dict: Strategic actions and analysis\n",
    "        \"\"\"\n",
    "        # Prepare strategic features\n",
    "        states = []\n",
    "        for agent_idx in range(self.trainer.n_agents):\n",
    "            features = self.trainer.calculate_strategic_features(market_data)\n",
    "            states.append(features)\n",
    "        \n",
    "        # Get strategic actions\n",
    "        actions, log_probs, values = self.trainer.get_action(states, deterministic=deterministic)\n",
    "        \n",
    "        # Detect market regime\n",
    "        regime = self.trainer.detect_market_regime(market_data)\n",
    "        \n",
    "        return {{\n",
    "            'strategic_agent_action': actions[0],\n",
    "            'portfolio_manager_action': actions[1],\n",
    "            'regime_detector_action': actions[2],\n",
    "            'market_regime': regime,\n",
    "            'confidence_values': values,\n",
    "            'action_probabilities': log_probs\n",
    "        }}\n",
    "    \n",
    "    def get_action_description(self, action):\n",
    "        action_map = {{\n",
    "            0: 'HOLD',\n",
    "            1: 'BUY_CONSERVATIVE',\n",
    "            2: 'BUY_AGGRESSIVE',\n",
    "            3: 'SELL_CONSERVATIVE', \n",
    "            4: 'SELL_AGGRESSIVE',\n",
    "            5: 'REDUCE_RISK',\n",
    "            6: 'INCREASE_RISK'\n",
    "        }}\n",
    "        return action_map.get(action, 'UNKNOWN')\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Initialize deployment\n",
    "    deployment = StrategicModelDeployment()\n",
    "    \n",
    "    # Load sample data (replace with your data source)\n",
    "    # data = pd.read_csv('your_30min_data.csv')\n",
    "    # predictions = deployment.predict(data)\n",
    "    # print(f\"Strategic recommendation: {{deployment.get_action_description(predictions['strategic_agent_action'])}}\")\n",
    "    \n",
    "    print(\"Strategic MAPPO deployment ready!\")\n",
    "\"\"\"\n",
    "    \n",
    "    deployment_script_path = os.path.join(strategic_save_dir, 'deploy_strategic_model.py')\n",
    "    with open(deployment_script_path, 'w') as f:\n",
    "        f.write(deployment_script)\n",
    "    \n",
    "    print(f\"üöÄ Deployment script created: {deployment_script_path}\")\n",
    "    \n",
    "    # Copy to Google Drive if available\n",
    "    drive_path = '/content/drive/MyDrive/GrandModel_Strategic_Exports'\n",
    "    if os.path.exists('/content/drive'):\n",
    "        try:\n",
    "            os.makedirs(drive_path, exist_ok=True)\n",
    "            import shutil\n",
    "            drive_export_path = os.path.join(drive_path, f'strategic_training_{timestamp}')\n",
    "            shutil.copytree(strategic_save_dir, drive_export_path)\n",
    "            print(f\"‚òÅÔ∏è Strategic models backed up to Google Drive: {drive_export_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not backup to Google Drive: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Strategic Export Completed Successfully!\")\n",
    "    print(f\"üìÅ Export Directory: {strategic_save_dir}\")\n",
    "    print(f\"üìä Models Ready for Production Deployment\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No strategic models to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategic_summary_section"
   },
   "source": [
    "## üìù Strategic Training Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "strategic_final_summary"
   },
   "outputs": [],
   "source": [
    "# Comprehensive strategic training summary\n",
    "if len(strategic_trainer.episode_rewards) > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"üéØ STRATEGIC MAPPO TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    final_stats = strategic_trainer.get_strategic_stats()\n",
    "    \n",
    "    print(f\"\\nüèÜ STRATEGIC PERFORMANCE HIGHLIGHTS:\")\n",
    "    print(f\"   ‚Ä¢ Training Episodes: {final_stats['episodes']}\")\n",
    "    print(f\"   ‚Ä¢ Total Training Steps: {final_stats['total_steps']:,}\")\n",
    "    print(f\"   ‚Ä¢ Best Episode Reward: {final_stats['best_reward']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Best Sharpe Ratio: {final_stats['best_sharpe']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Final Portfolio Value: ${final_stats['latest_portfolio_value']:,.0f}\")\n",
    "    print(f\"   ‚Ä¢ Current Market Regime: {final_stats['current_regime']}\")\n",
    "    \n",
    "    print(f\"\\nü§ñ STRATEGIC AGENT CAPABILITIES:\")\n",
    "    print(f\"   ‚Ä¢ Strategic Agent: Long-term market positioning\")\n",
    "    print(f\"   ‚Ä¢ Portfolio Manager: Risk-adjusted optimization\")\n",
    "    print(f\"   ‚Ä¢ Regime Detector: Market condition adaptation\")\n",
    "    \n",
    "    print(f\"\\nüìä TECHNICAL SPECIFICATIONS:\")\n",
    "    print(f\"   ‚Ä¢ State Dimension: {strategic_trainer.state_dim} features\")\n",
    "    print(f\"   ‚Ä¢ Action Dimension: {strategic_trainer.action_dim} strategic actions\")\n",
    "    print(f\"   ‚Ä¢ Network Architecture: Enhanced 512-unit networks\")\n",
    "    print(f\"   ‚Ä¢ Training Device: {strategic_trainer.device}\")\n",
    "    print(f\"   ‚Ä¢ Data Timeframe: 30-minute strategic bars\")\n",
    "    \n",
    "    print(f\"\\nüíæ EXPORT PACKAGE CONTENTS:\")\n",
    "    print(f\"   ‚Ä¢ Location: {strategic_save_dir}\")\n",
    "    print(f\"   ‚Ä¢ Best Models: Production-ready checkpoints\")\n",
    "    print(f\"   ‚Ä¢ Individual Agents: Separate actor/critic networks\")\n",
    "    print(f\"   ‚Ä¢ Configuration: Complete deployment settings\")\n",
    "    print(f\"   ‚Ä¢ Validation Results: Comprehensive backtesting\")\n",
    "    print(f\"   ‚Ä¢ Deployment Script: Ready-to-use production code\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_info = gpu_optimizer.monitor_memory()\n",
    "        print(f\"\\nüñ•Ô∏è RESOURCE UTILIZATION:\")\n",
    "        print(f\"   ‚Ä¢ GPU Memory Peak: {memory_info['gpu_memory_used_gb']:.1f} GB\")\n",
    "        print(f\"   ‚Ä¢ System Memory: {memory_info['system_memory_percent']:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Training Efficiency: Optimized for strategic timeframes\")\n",
    "    \n",
    "    print(f\"\\nüéØ STRATEGIC ADVANTAGES:\")\n",
    "    print(f\"   ‚Ä¢ Market Regime Awareness: Automatic adaptation\")\n",
    "    print(f\"   ‚Ä¢ Risk-Adjusted Returns: Sharpe ratio optimization\")\n",
    "    print(f\"   ‚Ä¢ Portfolio Management: Multi-agent coordination\")\n",
    "    print(f\"   ‚Ä¢ Long-term Perspective: 30-minute strategic horizon\")\n",
    "    print(f\"   ‚Ä¢ Production Ready: Comprehensive export package\")\n",
    "    \n",
    "    print(f\"\\nüöÄ NEXT STEPS - INTEGRATION ROADMAP:\")\n",
    "    print(f\"   1. ‚úÖ Strategic MAPPO Training: COMPLETED\")\n",
    "    print(f\"   2. üîÑ Tactical Integration: Combine with 5min tactical models\")\n",
    "    print(f\"   3. üß™ Cross-Timeframe Validation: Multi-horizon backtesting\")\n",
    "    print(f\"   4. üéØ Production Deployment: Live trading integration\")\n",
    "    print(f\"   5. üìä Performance Monitoring: Real-time model tracking\")\n",
    "    \n",
    "    print(f\"\\nüìö RECOMMENDED NEXT ACTIONS:\")\n",
    "    print(f\"   ‚Ä¢ Load tactical MAPPO models for comparison\")\n",
    "    print(f\"   ‚Ä¢ Run cross-validation on different market periods\")\n",
    "    print(f\"   ‚Ä¢ Implement hierarchical agent communication\")\n",
    "    print(f\"   ‚Ä¢ Deploy to GrandModel production infrastructure\")\n",
    "    print(f\"   ‚Ä¢ Set up continuous learning pipeline\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ STRATEGIC MAPPO TRAINING SUCCESSFULLY COMPLETED!\")\n",
    "    print(\"üéØ Ready for Multi-Agent Strategic Trading Deployment\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Strategic training was not completed successfully\")\n",
    "    print(\"Please review the training configuration and data loading steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strategic_final_section"
   },
   "source": [
    "## üèÅ Strategic Training Complete\n",
    "\n",
    "### ‚úÖ Mission Accomplished:\n",
    "- **Strategic MAPPO Agents**: Fully trained and validated\n",
    "- **Market Regime Detection**: Adaptive strategy implementation\n",
    "- **Portfolio Optimization**: Risk-adjusted return maximization\n",
    "- **Production Package**: Complete deployment solution\n",
    "\n",
    "### üéØ Strategic Capabilities Achieved:\n",
    "1. **Long-term Market Analysis**: 30-minute strategic decision making\n",
    "2. **Multi-Agent Coordination**: Strategic, Portfolio, and Regime agents\n",
    "3. **Risk Management**: Advanced drawdown control and Sharpe optimization\n",
    "4. **Market Adaptation**: Dynamic regime detection and strategy adjustment\n",
    "\n",
    "### üöÄ Integration Ready:\n",
    "- **Tactical Models**: Combine with 5-minute tactical agents\n",
    "- **Production Deployment**: Use exported models in GrandModel infrastructure\n",
    "- **Continuous Learning**: Framework for ongoing model improvement\n",
    "- **Performance Monitoring**: Comprehensive validation and backtesting\n",
    "\n",
    "### üìä Key Results:\n",
    "- **Training Efficiency**: GPU-optimized strategic learning\n",
    "- **Model Performance**: Validated strategic decision making\n",
    "- **Export Quality**: Production-ready model package\n",
    "- **Documentation**: Comprehensive deployment guides\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ The Strategic MAPPO training system is now complete and ready for integration into the GrandModel MARL trading architecture. The trained agents provide sophisticated long-term market analysis and strategic decision-making capabilities for professional trading applications.**\n",
    "\n",
    "**Next Phase**: Deploy strategic models alongside tactical models for complete multi-timeframe trading system.\n",
    "\n",
    "---\n",
    "\n",
    "*Strategic MAPPO Training System - GrandModel MARL Infrastructure*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",\n",
  "colab": {\n   "collapsed_sections": [],\n   "machine_shape": "hm",\n   "provenance": []\n  },\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n }\n,\n "nbformat": 4,\n "nbformat_minor": 4\n}