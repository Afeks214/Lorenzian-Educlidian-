# MARL Training Configuration
# This configuration defines all hyperparameters and settings for training the multi-agent system

# General Training Settings
training:
  # Total number of training episodes
  n_episodes: 10000
  
  # Episode length (number of steps)
  episode_length: 1000
  
  # Evaluation frequency
  eval_frequency: 100
  
  # Model checkpoint frequency
  save_frequency: 500
  
  # Early stopping patience
  early_stopping_patience: 1000
  
  # Device for training (cuda/cpu)
  device: "cuda"
  
  # Random seed for reproducibility
  seed: 42
  
  # Logging directory
  log_dir: "logs/marl_training"

# Environment Configuration
environment:
  # Initial trading capital
  initial_capital: 100000
  
  # Transaction cost (percentage)
  transaction_cost: 0.001
  
  # Maximum position size
  max_position_size: 1.0
  
  # Episode length
  episode_length: 1000
  
  # List of agents
  agents: ["regime", "structure", "tactical", "risk"]
  
  # Market simulation settings
  market_simulation:
    # Use historical data or synthetic
    use_historical: true
    
    # Synthetic data parameters
    synthetic:
      volatility: 0.02
      trend_strength: 0.001
      regime_switch_prob: 0.01

# MAPPO Algorithm Configuration
mappo:
  # Learning rate
  learning_rate: 3e-4
  
  # Learning rate schedule
  lr_schedule:
    enabled: true
    type: "cosine"  # linear, cosine, exponential
    min_lr: 1e-5
  
  # Discount factor
  gamma: 0.99
  
  # GAE lambda for advantage estimation
  gae_lambda: 0.95
  
  # Number of PPO epochs per update
  ppo_epochs: 10
  
  # PPO clip parameter
  clip_param: 0.2
  
  # Value loss coefficient
  value_loss_coef: 0.5
  
  # Entropy coefficient
  entropy_coef: 0.01
  
  # Maximum gradient norm for clipping
  max_grad_norm: 0.5
  
  # Batch size for updates
  batch_size: 64
  
  # Experience buffer size
  buffer_size: 10000
  
  # Use prioritized experience replay
  prioritized_replay: false

# Agent-Specific Configurations
agents:
  # Regime Agent Configuration
  regime:
    # Model architecture
    model:
      type: "transformer"
      hidden_dim: 128
      n_layers: 4
      n_heads: 8
      dropout: 0.1
      
    # Input configuration
    input:
      matrix_shape: [96, 12]  # Regime matrix dimensions
      
    # Output configuration
    output:
      n_regimes: 4  # Number of regime classes
      
    # Training specific
    training:
      weight_decay: 1e-4
      gradient_clip: 1.0
  
  # Structure Agent Configuration
  structure:
    # Model architecture
    model:
      type: "lstm_cnn"
      cnn_channels: [8, 64, 128]
      cnn_kernels: [3, 5, 7]
      lstm_hidden: 256
      lstm_layers: 2
      dropout: 0.2
      
    # Input configuration
    input:
      matrix_shape: [48, 8]  # Structure matrix dimensions
      
    # Output configuration
    output:
      direction_range: [-1, 1]
      size_range: [0, 1]
      confidence_range: [0, 1]
      
    # Training specific
    training:
      weight_decay: 1e-4
      gradient_clip: 1.0
  
  # Tactical Agent Configuration
  tactical:
    # Model architecture
    model:
      type: "attention"
      hidden_dim: 128
      n_heads: 8
      n_layers: 3
      dropout: 0.1
      use_positional_encoding: true
      
    # Input configuration
    input:
      matrix_shape: [60, 7]  # Tactical matrix dimensions
      
    # Output configuration
    output:
      n_actions: 5  # enter_long, enter_short, exit, hold, reduce
      
    # Training specific
    training:
      weight_decay: 1e-4
      gradient_clip: 1.0
  
  # Risk Agent Configuration
  risk:
    # Model architecture
    model:
      type: "dqn"
      hidden_dims: [256, 512, 256]
      dropout: 0.1
      use_dueling: true
      use_noisy_net: false
      
    # Input configuration
    input:
      matrix_features: 5100  # Flattened matrix size
      portfolio_features: 10
      
    # Output configuration
    output:
      n_actions: 4  # allow, modify, exit, block
      
    # Training specific
    training:
      weight_decay: 1e-4
      gradient_clip: 1.0
      target_update_freq: 100

# Reward System Configuration
rewards:
  # Agent-specific reward weights
  agent_weights:
    regime:
      accuracy: 0.4
      stability: 0.3
      shared: 0.3
    structure:
      directional: 0.4
      confidence: 0.2
      shared: 0.4
    tactical:
      timing: 0.5
      execution: 0.2
      shared: 0.3
    risk:
      compliance: 0.6
      protection: 0.2
      shared: 0.2
  
  # Global reward parameters
  shared_weight: 0.3
  risk_penalty_weight: 0.2
  cooperation_bonus: 0.1
  
  # Reward normalization
  normalization:
    enabled: true
    scale: 1.0
    clip: 10.0
  
  # Reward function specific settings
  reward_functions:
    sharpe:
      buffer_size: 100
      annualization_factor: 252
    risk_adjusted:
      risk_free_rate: 0.02
    execution:
      slippage_penalty: 10.0
      cost_penalty_scale: 0.001

# Data Pipeline Configuration
data:
  # Data directory
  data_path: "data/market_data"
  
  # Processed data output
  output_path: "data/processed"
  
  # Trading symbols
  symbols: ["EUR_USD", "GBP_USD", "USD_JPY"]
  
  # Date range for training data
  start_date: "2020-01-01"
  end_date: "2023-12-31"
  
  # Train/val/test split ratios
  val_split: 0.15
  test_split: 0.15
  
  # Data augmentation
  augmentation:
    enabled: true
    noise_injection: true
    noise_level: 0.01
    time_warping: true
    warp_factor: 0.1
    smote: false

# Training Optimization Configuration
optimization:
  # Hyperparameter optimization
  hyperopt:
    enabled: true
    n_trials: 100
    sampler: "tpe"  # tpe, random, grid
    
    # Parameters to optimize
    search_space:
      learning_rate:
        type: "loguniform"
        low: 1e-5
        high: 1e-2
      ppo_epochs:
        type: "int"
        low: 5
        high: 20
      clip_param:
        type: "uniform"
        low: 0.1
        high: 0.3
      entropy_coef:
        type: "loguniform"
        low: 1e-4
        high: 1e-1
  
  # Distributed training
  distributed:
    enabled: false
    backend: "nccl"
    world_size: 4
    
  # Mixed precision training
  mixed_precision:
    enabled: true
    opt_level: "O1"  # O0, O1, O2, O3

# Monitoring and Evaluation Configuration
monitoring:
  # MLflow tracking
  mlflow:
    enabled: true
    tracking_uri: "http://localhost:5000"
    experiment_name: "MARL_Trading"
    
  # Metrics to track
  metrics:
    - "episode_reward"
    - "sharpe_ratio"
    - "max_drawdown"
    - "win_rate"
    - "avg_trade_duration"
    - "total_trades"
    
  # Tensorboard logging
  tensorboard:
    enabled: true
    log_frequency: 10
    
  # Performance thresholds for alerts
  alerts:
    min_sharpe_ratio: 0.5
    max_drawdown: 0.20
    min_win_rate: 0.45

# Evaluation Configuration
evaluation:
  # Number of evaluation episodes
  n_episodes: 100
  
  # Evaluation metrics
  metrics:
    - "total_return"
    - "sharpe_ratio"
    - "sortino_ratio"
    - "max_drawdown"
    - "calmar_ratio"
    - "win_rate"
    - "profit_factor"
    
  # Backtesting settings
  backtesting:
    # Out-of-sample test period
    test_start: "2024-01-01"
    test_end: "2024-03-31"
    
    # Include transaction costs
    include_costs: true
    
    # Slippage model
    slippage:
      type: "percentage"
      value: 0.0005
    
    # Position limits
    position_limits:
      max_position: 1.0
      max_leverage: 1.0

# Production Deployment Configuration
deployment:
  # Model serving
  serving:
    framework: "torchserve"
    batch_size: 1
    max_batch_delay: 10  # milliseconds
    
  # Model versioning
  versioning:
    enabled: true
    registry: "mlflow"
    
  # A/B testing configuration
  ab_testing:
    enabled: false
    control_allocation: 0.8
    
  # Monitoring in production
  production_monitoring:
    performance_tracking: true
    drift_detection: true
    alert_thresholds:
      latency_ms: 10
      error_rate: 0.01

# Structure Embedder Specific Training Configuration
structure_embedder:
  # Model architecture parameters
  architecture:
    input_channels: 8
    output_dim: 64
    d_model: 128
    n_heads: 4
    n_layers: 3
    d_ff: 512
    dropout_rate: 0.2
    max_seq_len: 48
    
  # Training hyperparameters
  training:
    batch_size: 32
    learning_rate: 1e-4
    weight_decay: 1e-5
    warmup_steps: 1000
    max_epochs: 100
    gradient_clip_norm: 1.0
    scheduler: 'cosine_with_warmup'
    
    # Optimizer settings
    optimizer: 'adamw'
    adam_betas: [0.9, 0.999]
    adam_eps: 1e-8
    
  # Loss configuration
  loss:
    prediction_weight: 0.4
    uncertainty_weight: 0.3
    attention_weight: 0.2
    reconstruction_weight: 0.1
    
    # Uncertainty calibration
    uncertainty_target_factor: 1.0
    min_uncertainty: 1e-6
    
    # Attention diversity
    min_attention_entropy: 2.0
    
  # Multi-task training
  multi_task:
    enable_adversarial: false
    enable_contrastive: false
    
    structure_weight: 1.0
    adversarial_weight: 0.1
    contrastive_weight: 0.2
    calibration_weight: 0.3
    
    # Adversarial training parameters
    adversarial_epsilon: 0.01
    adversarial_alpha: 0.1
    adversarial_steps: 3
    
    # Contrastive learning
    contrastive_temperature: 0.1
    contrastive_margin: 1.0
    
    # Calibration
    calibration_bins: 10
    
  # Data augmentation
  augmentation:
    noise_std: 0.01
    dropout_prob: 0.1
    time_shift_range: [-2, 2]
    feature_dropout: 0.05
    mixup_alpha: 0.2
    
  # Validation and early stopping
  validation:
    eval_frequency: 100
    early_stopping_patience: 20
    best_metric: 'val_loss'
    val_split: 0.2
    
  # Monitoring
  monitoring:
    log_frequency: 50
    save_frequency: 1000
    plot_attention: true
    track_uncertainty: true