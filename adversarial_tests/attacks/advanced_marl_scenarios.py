#!/usr/bin/env python3
"""
ðŸš¨ AGENT GAMMA MISSION - ADVANCED MARL ATTACK SCENARIOS
Advanced MARL Attack Development: Coordinated Attack Scenarios

This module implements sophisticated attack scenarios combining multiple attack vectors
to create realistic, coordinated attacks against MARL systems:
- Bull trap coordination attacks
- Whipsaw multi-agent attacks
- Fake breakout with agent manipulation
- Coordinated correlation gaming

Key Attack Scenarios:
1. Bull Trap Coordination: Combine regime attacks with temporal attacks
2. Whipsaw Multi-Agent: Coordinate coordination attacks with policy attacks
3. Fake Breakout Manipulation: Mix temporal and regime attacks
4. Coordinated Correlation Gaming: Combine all attack types

MISSION OBJECTIVE: Achieve >80% attack success rate with coordinated scenarios
"""

import numpy as np
import torch
from typing import Dict, List, Tuple, Any, Optional
import logging
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import time
import asyncio

# Import individual attack modules
from .marl_coordination_attack import (
    MARLCoordinationAttacker, 
    CoordinationAttackResult, 
    AttackType as CoordAttackType
)
from .temporal_sequence_attack import (
    TemporalSequenceAttacker,
    TemporalAttackResult,
    TemporalAttackType
)
from .policy_gradient_attack import (
    PolicyGradientAttacker,
    PolicyGradientAttackResult,
    PolicyAttackType
)
from .regime_transition_attack import (
    RegimeTransitionAttacker,
    RegimeAttackResult,
    RegimeAttackType
)

# Scenario Result Tracking
@dataclass
class ScenarioAttackResult:
    """Results from a coordinated attack scenario."""
    scenario_name: str
    success: bool
    overall_confidence: float
    coordination_disruption: float
    temporal_disruption: float
    policy_disruption: float
    regime_disruption: float
    individual_attack_results: List[Any]
    scenario_effectiveness: float
    execution_time_ms: float
    attack_sequence: List[str]
    timestamp: datetime

class AttackScenario(Enum):
    """Types of coordinated attack scenarios."""
    BULL_TRAP_COORDINATION = "bull_trap_coordination"
    WHIPSAW_MULTI_AGENT = "whipsaw_multi_agent"
    FAKE_BREAKOUT_MANIPULATION = "fake_breakout_manipulation"
    COORDINATED_CORRELATION_GAMING = "coordinated_correlation_gaming"

class AdvancedMARLScenarioAttacker:
    """
    Advanced MARL Scenario Attack System.
    
    This system orchestrates coordinated attacks using multiple attack vectors
    to create sophisticated, realistic attack scenarios.
    """
    
    def __init__(self, device: str = 'cpu'):
        """
        Initialize the Advanced MARL Scenario Attacker.
        
        Args:
            device: Device for tensor operations ('cpu' or 'cuda')
        """
        self.device = device
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        # Initialize individual attackers
        self.coordination_attacker = MARLCoordinationAttacker(device)
        self.temporal_attacker = TemporalSequenceAttacker(device)
        self.policy_attacker = PolicyGradientAttacker(device)
        self.regime_attacker = RegimeTransitionAttacker(device)
        
        # Scenario attack history
        self.scenario_history = []
        self.success_rates = {scenario: 0.0 for scenario in AttackScenario}
        self.scenario_metrics = {
            'total_scenarios': 0,
            'successful_scenarios': 0,
            'avg_effectiveness': 0.0,
            'max_effectiveness': 0.0,
            'scenario_types_executed': set()
        }
        
        # Scenario parameters
        self.coordination_weight = 0.3
        self.temporal_weight = 0.25
        self.policy_weight = 0.25
        self.regime_weight = 0.2
        self.success_threshold = 0.7
        
        self.logger.info(f"AdvancedMARLScenarioAttacker initialized: device={device}")
    
    def execute_bull_trap_coordination_attack(
        self,
        market_data: np.ndarray,
        regime_indicators: Dict[str, Any],
        agent_predictions: List[Dict[str, Any]],
        shared_context: Dict[str, Any]
    ) -> ScenarioAttackResult:
        """
        ðŸŽ¯ SCENARIO 1: BULL TRAP COORDINATION ATTACK
        
        Coordinates multiple attacks to create a sophisticated bull trap:
        1. Regime attack: Create false bull signals
        2. Temporal attack: Inject bullish correlations
        3. Coordination attack: Disrupt consensus toward bullish bias
        4. Policy attack: Manipulate action selection toward buying
        
        Args:
            market_data: Market data matrix
            regime_indicators: Current regime indicators
            agent_predictions: Agent predictions to attack
            shared_context: Shared context for coordination
            
        Returns:
            ScenarioAttackResult with coordinated attack outcome
        """
        start_time = time.time()
        scenario_name = "Bull Trap Coordination Attack"
        
        self.logger.info(f"ðŸŽ¯ Executing {scenario_name}")
        
        # Attack sequence
        attack_sequence = []
        individual_results = []
        
        # Step 1: Create false bull regime signals
        self.logger.info("Step 1: Creating false bull regime signals")
        regime_payload, regime_result = self.regime_attacker.generate_false_bull_signal_attack(
            market_data.copy(), regime_indicators.copy()
        )
        attack_sequence.append("regime_false_bull")
        individual_results.append(regime_result)
        
        # Step 2: Inject bullish temporal correlations
        self.logger.info("Step 2: Injecting bullish temporal correlations")
        temporal_payload, temporal_result = self.temporal_attacker.generate_correlation_poisoning_attack(
            market_data.copy(), 
            target_correlations={'momentum_reversal': 0.8, 'volatility_clustering': 0.9}
        )
        attack_sequence.append("temporal_bull_correlation")
        individual_results.append(temporal_result)
        
        # Step 3: Disrupt coordination toward bullish consensus
        self.logger.info("Step 3: Disrupting coordination toward bullish consensus")
        
        # Modify agent predictions to be excessively bullish
        bullish_predictions = []
        for pred in agent_predictions:
            bullish_pred = pred.copy()
            bullish_pred['action_probabilities'] = [0.85, 0.10, 0.05]  # Extreme buy bias
            bullish_pred['confidence'] = 0.95
            bullish_predictions.append(bullish_pred)
        
        coord_payload, coord_result = self.coordination_attacker.generate_consensus_disruption_attack(
            bullish_predictions, shared_context.copy()
        )
        attack_sequence.append("coordination_bull_consensus")
        individual_results.append(coord_result)
        
        # Step 4: Policy manipulation toward buying
        self.logger.info("Step 4: Manipulating policy toward buying")
        
        # Create mock policy network and state for demonstration
        mock_policy = self._create_mock_policy_network()
        mock_state = torch.randn(1, 13)
        target_action = torch.tensor([0])  # Buy action
        
        policy_payload, policy_result = self.policy_attacker.generate_fgsm_policy_attack(
            mock_policy, mock_state, target_action
        )
        attack_sequence.append("policy_buy_bias")
        individual_results.append(policy_result)
        
        # Calculate scenario effectiveness
        scenario_effectiveness = self._calculate_scenario_effectiveness(
            individual_results, "bull_trap"
        )
        
        # Calculate overall disruption scores
        coordination_disruption = coord_result.coordination_disruption_score
        temporal_disruption = temporal_result.temporal_disruption_score
        policy_disruption = policy_result.policy_disruption_score
        regime_disruption = regime_result.regime_disruption_score
        
        # Calculate overall confidence
        overall_confidence = (
            coordination_disruption * self.coordination_weight +
            temporal_disruption * self.temporal_weight +
            policy_disruption * self.policy_weight +
            regime_disruption * self.regime_weight
        )
        
        # Determine scenario success
        success = scenario_effectiveness > self.success_threshold
        
        # Create scenario result
        execution_time_ms = (time.time() - start_time) * 1000
        scenario_result = ScenarioAttackResult(
            scenario_name=scenario_name,
            success=success,
            overall_confidence=overall_confidence,
            coordination_disruption=coordination_disruption,
            temporal_disruption=temporal_disruption,
            policy_disruption=policy_disruption,
            regime_disruption=regime_disruption,
            individual_attack_results=individual_results,
            scenario_effectiveness=scenario_effectiveness,
            execution_time_ms=execution_time_ms,
            attack_sequence=attack_sequence,
            timestamp=datetime.now()
        )
        
        self._record_scenario_result(scenario_result, AttackScenario.BULL_TRAP_COORDINATION)
        
        self.logger.info(
            f"Bull trap coordination attack completed: "
            f"success={success}, effectiveness={scenario_effectiveness:.3f}, "
            f"time={execution_time_ms:.1f}ms"
        )
        
        return scenario_result
    
    def execute_whipsaw_multi_agent_attack(
        self,
        market_data: np.ndarray,
        regime_indicators: Dict[str, Any],
        agent_predictions: List[Dict[str, Any]],
        shared_context: Dict[str, Any]
    ) -> ScenarioAttackResult:
        """
        ðŸŽ¯ SCENARIO 2: WHIPSAW MULTI-AGENT ATTACK
        
        Creates rapid directional changes to confuse multiple agents:
        1. Temporal attack: Create whipsaw patterns
        2. Regime attack: Inject transition confusion
        3. Coordination attack: Disrupt agent timing
        4. Policy attack: Force boundary actions
        
        Args:
            market_data: Market data matrix
            regime_indicators: Current regime indicators
            agent_predictions: Agent predictions to attack
            shared_context: Shared context for coordination
            
        Returns:
            ScenarioAttackResult with coordinated attack outcome
        """
        start_time = time.time()
        scenario_name = "Whipsaw Multi-Agent Attack"
        
        self.logger.info(f"ðŸŽ¯ Executing {scenario_name}")
        
        # Attack sequence
        attack_sequence = []
        individual_results = []
        
        # Step 1: Create whipsaw temporal patterns
        self.logger.info("Step 1: Creating whipsaw temporal patterns")
        temporal_payload, temporal_result = self.temporal_attacker.generate_pattern_disruption_attack(
            market_data.copy(), 
            pattern_types=['trend', 'cycle'], 
            disruption_strength=0.8
        )
        attack_sequence.append("temporal_whipsaw")
        individual_results.append(temporal_result)
        
        # Step 2: Inject regime transition confusion
        self.logger.info("Step 2: Injecting regime transition confusion")
        regime_payload, regime_result = self.regime_attacker.generate_transition_confusion_attack(
            market_data.copy(), regime_indicators.copy()
        )
        attack_sequence.append("regime_confusion")
        individual_results.append(regime_result)
        
        # Step 3: Disrupt agent timing coordination
        self.logger.info("Step 3: Disrupting agent timing coordination")
        coord_payload, coord_result = self.coordination_attacker.generate_timing_attack(
            coordination_delay_ms=75.0, desync_pattern='random'
        )
        attack_sequence.append("coordination_timing")
        individual_results.append(coord_result)
        
        # Step 4: Force boundary actions
        self.logger.info("Step 4: Forcing boundary actions")
        
        # Create mock policy network and state
        mock_policy = self._create_mock_policy_network()
        mock_state = torch.randn(1, 13)
        
        policy_payload, policy_result = self.policy_attacker.generate_boundary_attack(
            mock_policy, mock_state, boundary_type='oscillating_boundary'
        )
        attack_sequence.append("policy_boundary")
        individual_results.append(policy_result)
        
        # Calculate scenario effectiveness
        scenario_effectiveness = self._calculate_scenario_effectiveness(
            individual_results, "whipsaw"
        )
        
        # Calculate overall disruption scores
        coordination_disruption = coord_result.coordination_disruption_score
        temporal_disruption = temporal_result.temporal_disruption_score
        policy_disruption = policy_result.policy_disruption_score
        regime_disruption = regime_result.regime_disruption_score
        
        # Calculate overall confidence
        overall_confidence = (
            coordination_disruption * self.coordination_weight +
            temporal_disruption * self.temporal_weight +
            policy_disruption * self.policy_weight +
            regime_disruption * self.regime_weight
        )
        
        # Determine scenario success
        success = scenario_effectiveness > self.success_threshold
        
        # Create scenario result
        execution_time_ms = (time.time() - start_time) * 1000
        scenario_result = ScenarioAttackResult(
            scenario_name=scenario_name,
            success=success,
            overall_confidence=overall_confidence,
            coordination_disruption=coordination_disruption,
            temporal_disruption=temporal_disruption,
            policy_disruption=policy_disruption,
            regime_disruption=regime_disruption,
            individual_attack_results=individual_results,
            scenario_effectiveness=scenario_effectiveness,
            execution_time_ms=execution_time_ms,
            attack_sequence=attack_sequence,
            timestamp=datetime.now()
        )
        
        self._record_scenario_result(scenario_result, AttackScenario.WHIPSAW_MULTI_AGENT)
        
        self.logger.info(
            f"Whipsaw multi-agent attack completed: "
            f"success={success}, effectiveness={scenario_effectiveness:.3f}, "
            f"time={execution_time_ms:.1f}ms"
        )
        
        return scenario_result
    
    def execute_fake_breakout_manipulation_attack(
        self,
        market_data: np.ndarray,
        regime_indicators: Dict[str, Any],
        agent_predictions: List[Dict[str, Any]],
        shared_context: Dict[str, Any]
    ) -> ScenarioAttackResult:
        """
        ðŸŽ¯ SCENARIO 3: FAKE BREAKOUT MANIPULATION ATTACK
        
        Creates false breakout signals to trap agents:
        1. Regime attack: Create false bull signals initially
        2. Temporal attack: Inject false momentum patterns
        3. Regime attack: Reverse to bear signals (trap execution)
        4. Coordination attack: Exploit gating network
        
        Args:
            market_data: Market data matrix
            regime_indicators: Current regime indicators
            agent_predictions: Agent predictions to attack
            shared_context: Shared context for coordination
            
        Returns:
            ScenarioAttackResult with coordinated attack outcome
        """
        start_time = time.time()
        scenario_name = "Fake Breakout Manipulation Attack"
        
        self.logger.info(f"ðŸŽ¯ Executing {scenario_name}")
        
        # Attack sequence
        attack_sequence = []
        individual_results = []
        
        # Step 1: Create initial false bull signals (setup phase)
        self.logger.info("Step 1: Creating initial false bull signals")
        regime_payload1, regime_result1 = self.regime_attacker.generate_false_bull_signal_attack(
            market_data.copy(), regime_indicators.copy(), signal_strength=0.7
        )
        attack_sequence.append("regime_false_bull_setup")
        individual_results.append(regime_result1)
        
        # Step 2: Inject false momentum patterns to support breakout
        self.logger.info("Step 2: Injecting false momentum patterns")
        temporal_payload, temporal_result = self.temporal_attacker.generate_correlation_poisoning_attack(
            market_data.copy(), 
            target_correlations={'momentum_reversal': 0.9, 'volume_price_divergence': -0.8}
        )
        attack_sequence.append("temporal_fake_momentum")
        individual_results.append(temporal_result)
        
        # Step 3: Reverse to bear signals (trap execution)
        self.logger.info("Step 3: Reversing to bear signals (trap execution)")
        regime_payload2, regime_result2 = self.regime_attacker.generate_false_bear_signal_attack(
            market_data.copy(), regime_indicators.copy(), signal_strength=0.9
        )
        attack_sequence.append("regime_false_bear_trap")
        individual_results.append(regime_result2)
        
        # Step 4: Exploit gating network vulnerability
        self.logger.info("Step 4: Exploiting gating network vulnerability")
        
        # Force regime agent dominance during confusion
        target_weights = np.array([0.1, 0.1, 0.8])  # Heavy regime bias
        
        coord_payload, coord_result = self.coordination_attacker.generate_gating_exploitation_attack(
            shared_context.copy(), target_weights
        )
        attack_sequence.append("coordination_gating_exploit")
        individual_results.append(coord_result)
        
        # Calculate scenario effectiveness (emphasize regime disruption for breakout)
        scenario_effectiveness = self._calculate_scenario_effectiveness(
            individual_results, "fake_breakout"
        )
        
        # Calculate overall disruption scores
        coordination_disruption = coord_result.coordination_disruption_score
        temporal_disruption = temporal_result.temporal_disruption_score
        policy_disruption = 0.0  # No policy attack in this scenario
        regime_disruption = (regime_result1.regime_disruption_score + regime_result2.regime_disruption_score) / 2
        
        # Calculate overall confidence (emphasize regime for breakout scenario)
        overall_confidence = (
            coordination_disruption * 0.3 +
            temporal_disruption * 0.2 +
            regime_disruption * 0.5  # Higher weight for regime in breakout
        )
        
        # Determine scenario success
        success = scenario_effectiveness > self.success_threshold
        
        # Create scenario result
        execution_time_ms = (time.time() - start_time) * 1000
        scenario_result = ScenarioAttackResult(
            scenario_name=scenario_name,
            success=success,
            overall_confidence=overall_confidence,
            coordination_disruption=coordination_disruption,
            temporal_disruption=temporal_disruption,
            policy_disruption=policy_disruption,
            regime_disruption=regime_disruption,
            individual_attack_results=individual_results,
            scenario_effectiveness=scenario_effectiveness,
            execution_time_ms=execution_time_ms,
            attack_sequence=attack_sequence,
            timestamp=datetime.now()
        )
        
        self._record_scenario_result(scenario_result, AttackScenario.FAKE_BREAKOUT_MANIPULATION)
        
        self.logger.info(
            f"Fake breakout manipulation attack completed: "
            f"success={success}, effectiveness={scenario_effectiveness:.3f}, "
            f"time={execution_time_ms:.1f}ms"
        )
        
        return scenario_result
    
    def execute_coordinated_correlation_gaming_attack(
        self,
        market_data: np.ndarray,
        regime_indicators: Dict[str, Any],
        agent_predictions: List[Dict[str, Any]],
        shared_context: Dict[str, Any]
    ) -> ScenarioAttackResult:
        """
        ðŸŽ¯ SCENARIO 4: COORDINATED CORRELATION GAMING ATTACK
        
        Combines all attack types for maximum disruption:
        1. Temporal attack: Correlation poisoning
        2. Regime attack: MMD poisoning
        3. Coordination attack: Communication jamming
        4. Policy attack: Gradient reversal
        5. Temporal attack: Memory exploitation
        
        Args:
            market_data: Market data matrix
            regime_indicators: Current regime indicators
            agent_predictions: Agent predictions to attack
            shared_context: Shared context for coordination
            
        Returns:
            ScenarioAttackResult with coordinated attack outcome
        """
        start_time = time.time()
        scenario_name = "Coordinated Correlation Gaming Attack"
        
        self.logger.info(f"ðŸŽ¯ Executing {scenario_name}")
        
        # Attack sequence
        attack_sequence = []
        individual_results = []
        
        # Step 1: Correlation poisoning
        self.logger.info("Step 1: Correlation poisoning")
        temporal_payload1, temporal_result1 = self.temporal_attacker.generate_correlation_poisoning_attack(
            market_data.copy(), 
            target_correlations={
                'momentum_reversal': -0.9,
                'volatility_clustering': 0.95,
                'volume_price_divergence': -0.8
            }
        )
        attack_sequence.append("temporal_correlation_poison")
        individual_results.append(temporal_result1)
        
        # Step 2: MMD poisoning
        self.logger.info("Step 2: MMD poisoning")
        regime_payload, regime_result = self.regime_attacker.generate_mmd_poisoning_attack(
            market_data.copy(), regime_indicators.copy(), target_mmd_score=0.95
        )
        attack_sequence.append("regime_mmd_poison")
        individual_results.append(regime_result)
        
        # Step 3: Communication jamming
        self.logger.info("Step 3: Communication jamming")
        coord_payload, coord_result = self.coordination_attacker.generate_communication_jamming_attack(
            shared_context.copy()
        )
        attack_sequence.append("coordination_comm_jam")
        individual_results.append(coord_result)
        
        # Step 4: Policy gradient reversal
        self.logger.info("Step 4: Policy gradient reversal")
        
        # Create mock policy network and loss function
        mock_policy = self._create_mock_policy_network()
        mock_state = torch.randn(1, 13)
        
        def mock_loss(output, state):
            return torch.mean(output ** 2)
        
        policy_payload, policy_result = self.policy_attacker.generate_gradient_reversal_attack(
            mock_policy, mock_state, mock_loss
        )
        attack_sequence.append("policy_gradient_reversal")
        individual_results.append(policy_result)
        
        # Step 5: Memory exploitation
        self.logger.info("Step 5: Memory exploitation")
        temporal_payload2, temporal_result2 = self.temporal_attacker.generate_memory_exploitation_attack(
            market_data.copy(), memory_length=10, exploitation_type='gradient_explosion'
        )
        attack_sequence.append("temporal_memory_exploit")
        individual_results.append(temporal_result2)
        
        # Calculate scenario effectiveness (all attacks weighted equally)
        scenario_effectiveness = self._calculate_scenario_effectiveness(
            individual_results, "coordinated_correlation"
        )
        
        # Calculate overall disruption scores
        coordination_disruption = coord_result.coordination_disruption_score
        temporal_disruption = (temporal_result1.temporal_disruption_score + temporal_result2.temporal_disruption_score) / 2
        policy_disruption = policy_result.policy_disruption_score
        regime_disruption = regime_result.regime_disruption_score
        
        # Calculate overall confidence (balanced across all attacks)
        overall_confidence = (
            coordination_disruption * 0.25 +
            temporal_disruption * 0.25 +
            policy_disruption * 0.25 +
            regime_disruption * 0.25
        )
        
        # Determine scenario success
        success = scenario_effectiveness > self.success_threshold
        
        # Create scenario result
        execution_time_ms = (time.time() - start_time) * 1000
        scenario_result = ScenarioAttackResult(
            scenario_name=scenario_name,
            success=success,
            overall_confidence=overall_confidence,
            coordination_disruption=coordination_disruption,
            temporal_disruption=temporal_disruption,
            policy_disruption=policy_disruption,
            regime_disruption=regime_disruption,
            individual_attack_results=individual_results,
            scenario_effectiveness=scenario_effectiveness,
            execution_time_ms=execution_time_ms,
            attack_sequence=attack_sequence,
            timestamp=datetime.now()
        )
        
        self._record_scenario_result(scenario_result, AttackScenario.COORDINATED_CORRELATION_GAMING)
        
        self.logger.info(
            f"Coordinated correlation gaming attack completed: "
            f"success={success}, effectiveness={scenario_effectiveness:.3f}, "
            f"time={execution_time_ms:.1f}ms"
        )
        
        return scenario_result
    
    def _create_mock_policy_network(self):
        """Create a mock policy network for testing."""
        import torch.nn as nn
        
        class MockPolicy(nn.Module):
            def __init__(self):
                super().__init__()
                self.network = nn.Sequential(
                    nn.Linear(13, 64),
                    nn.ReLU(),
                    nn.Linear(64, 3)
                )
            
            def forward(self, x):
                return self.network(x)
        
        return MockPolicy()
    
    def _calculate_scenario_effectiveness(
        self, 
        individual_results: List[Any], 
        scenario_type: str
    ) -> float:
        """Calculate overall scenario effectiveness."""
        if not individual_results:
            return 0.0
        
        # Extract success rates and confidence scores
        success_count = sum(1 for result in individual_results if result.success)
        success_rate = success_count / len(individual_results)
        
        # Calculate average confidence/disruption scores
        avg_confidence = 0.0
        count = 0
        
        for result in individual_results:
            if hasattr(result, 'coordination_disruption_score'):
                avg_confidence += result.coordination_disruption_score
                count += 1
            elif hasattr(result, 'temporal_disruption_score'):
                avg_confidence += result.temporal_disruption_score
                count += 1
            elif hasattr(result, 'policy_disruption_score'):
                avg_confidence += result.policy_disruption_score
                count += 1
            elif hasattr(result, 'regime_disruption_score'):
                avg_confidence += result.regime_disruption_score
                count += 1
        
        avg_confidence = avg_confidence / max(count, 1)
        
        # Scenario-specific effectiveness calculation
        if scenario_type == "bull_trap":
            # Bull trap effectiveness emphasizes regime and coordination
            effectiveness = (success_rate * 0.6) + (avg_confidence * 0.4)
        elif scenario_type == "whipsaw":
            # Whipsaw effectiveness emphasizes temporal and coordination
            effectiveness = (success_rate * 0.5) + (avg_confidence * 0.5)
        elif scenario_type == "fake_breakout":
            # Fake breakout effectiveness emphasizes regime changes
            effectiveness = (success_rate * 0.7) + (avg_confidence * 0.3)
        else:  # coordinated_correlation
            # Coordinated attack effectiveness balances all factors
            effectiveness = (success_rate * 0.5) + (avg_confidence * 0.5)
        
        return min(effectiveness, 1.0)
    
    def _record_scenario_result(self, result: ScenarioAttackResult, scenario_type: AttackScenario):
        """Record scenario result for analytics."""
        self.scenario_history.append(result)
        
        # Update metrics
        self.scenario_metrics['total_scenarios'] += 1
        if result.success:
            self.scenario_metrics['successful_scenarios'] += 1
        
        # Update success rates
        type_attempts = len([r for r in self.scenario_history if r.scenario_name == result.scenario_name])
        type_successes = len([r for r in self.scenario_history if r.scenario_name == result.scenario_name and r.success])
        self.success_rates[scenario_type] = type_successes / type_attempts
        
        # Update effectiveness metrics
        self.scenario_metrics['avg_effectiveness'] = np.mean([r.scenario_effectiveness for r in self.scenario_history])
        self.scenario_metrics['max_effectiveness'] = max(self.scenario_metrics['max_effectiveness'], result.scenario_effectiveness)
        self.scenario_metrics['scenario_types_executed'].add(scenario_type.value)
        
        # Keep history manageable
        if len(self.scenario_history) > 500:
            self.scenario_history = self.scenario_history[-250:]
    
    def get_scenario_analytics(self) -> Dict[str, Any]:
        """Get comprehensive scenario analytics."""
        if not self.scenario_history:
            return {'status': 'no_scenarios_executed'}
        
        recent_scenarios = self.scenario_history[-50:]  # Last 50 scenarios
        
        return {
            'total_scenarios': len(self.scenario_history),
            'recent_scenarios': len(recent_scenarios),
            'overall_success_rate': self.scenario_metrics['successful_scenarios'] / self.scenario_metrics['total_scenarios'],
            'success_rates_by_scenario': {scenario.value: rate for scenario, rate in self.success_rates.items()},
            'scenario_metrics': self.scenario_metrics.copy(),
            'recent_performance': {
                'avg_effectiveness': np.mean([r.scenario_effectiveness for r in recent_scenarios]),
                'max_effectiveness': max([r.scenario_effectiveness for r in recent_scenarios]),
                'avg_execution_time_ms': np.mean([r.execution_time_ms for r in recent_scenarios]),
                'success_rate': len([r for r in recent_scenarios if r.success]) / len(recent_scenarios)
            },
            'scenario_type_distribution': {
                scenario.value: len([r for r in recent_scenarios if scenario.value in r.scenario_name])
                for scenario in AttackScenario
            },
            'individual_attack_performance': {
                'coordination_avg': np.mean([r.coordination_disruption for r in recent_scenarios]),
                'temporal_avg': np.mean([r.temporal_disruption for r in recent_scenarios]),
                'policy_avg': np.mean([r.policy_disruption for r in recent_scenarios]),
                'regime_avg': np.mean([r.regime_disruption for r in recent_scenarios])
            }
        }

# Example usage and testing functions
def run_advanced_scenario_demo():
    """Demonstrate advanced MARL scenario attack capabilities."""
    print("ðŸš¨" * 50)
    print("AGENT GAMMA MISSION - ADVANCED MARL SCENARIO ATTACK DEMO")
    print("ðŸš¨" * 50)
    
    attacker = AdvancedMARLScenarioAttacker()
    
    # Generate mock data
    sequence_length = 48
    n_features = 13
    market_data = np.random.randn(sequence_length, n_features)
    market_data = np.cumsum(market_data, axis=0)
    
    regime_indicators = {
        'market_regime': 'sideways',
        'regime_confidence': 0.6,
        'volatility_regime': 'medium',
        'momentum_regime': 'weak',
        'mmd_score': 0.3
    }
    
    agent_predictions = [
        {'agent_name': 'MLMI', 'action_probabilities': [0.4, 0.3, 0.3], 'confidence': 0.7},
        {'agent_name': 'NWRQK', 'action_probabilities': [0.3, 0.4, 0.3], 'confidence': 0.6},
        {'agent_name': 'Regime', 'action_probabilities': [0.3, 0.3, 0.4], 'confidence': 0.8}
    ]
    
    shared_context = {
        'volatility_30': 0.02,
        'volume_ratio': 1.5,
        'momentum_20': 0.01,
        'momentum_50': 0.005,
        'mmd_score': 0.1,
        'price_trend': 0.008
    }
    
    print("\nðŸŽ¯ SCENARIO 1: BULL TRAP COORDINATION")
    result = attacker.execute_bull_trap_coordination_attack(
        market_data.copy(), regime_indicators.copy(), 
        agent_predictions.copy(), shared_context.copy()
    )
    print(f"Success: {result.success}, Effectiveness: {result.scenario_effectiveness:.3f}")
    
    print("\nðŸŽ¯ SCENARIO 2: WHIPSAW MULTI-AGENT")
    result = attacker.execute_whipsaw_multi_agent_attack(
        market_data.copy(), regime_indicators.copy(), 
        agent_predictions.copy(), shared_context.copy()
    )
    print(f"Success: {result.success}, Effectiveness: {result.scenario_effectiveness:.3f}")
    
    print("\nðŸŽ¯ SCENARIO 3: FAKE BREAKOUT MANIPULATION")
    result = attacker.execute_fake_breakout_manipulation_attack(
        market_data.copy(), regime_indicators.copy(), 
        agent_predictions.copy(), shared_context.copy()
    )
    print(f"Success: {result.success}, Effectiveness: {result.scenario_effectiveness:.3f}")
    
    print("\nðŸŽ¯ SCENARIO 4: COORDINATED CORRELATION GAMING")
    result = attacker.execute_coordinated_correlation_gaming_attack(
        market_data.copy(), regime_indicators.copy(), 
        agent_predictions.copy(), shared_context.copy()
    )
    print(f"Success: {result.success}, Effectiveness: {result.scenario_effectiveness:.3f}")
    
    print("\nðŸ“Š SCENARIO ANALYTICS")
    analytics = attacker.get_scenario_analytics()
    print(f"Overall Success Rate: {analytics['overall_success_rate']:.2%}")
    print(f"Average Effectiveness: {analytics['scenario_metrics']['avg_effectiveness']:.3f}")
    print(f"Scenario Types Executed: {list(analytics['scenario_metrics']['scenario_types_executed'])}")
    
    return attacker

if __name__ == "__main__":
    run_advanced_scenario_demo()